





































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































[{"categories":["BUG"],"contents":"介绍cnvkit的使用方法和原理。示例是：使用WGS测序数据从fastq到绝对拷贝数结果的例子。\n CNVkit解决的问题范畴：\n  fastq，BAM文件，片段文件等的绝对拷贝数提取\n  配对/部分对照/无对照样本的拷贝数变异提取\n  通过housekeeping gene的mRNA表达水平推测粗略的拷贝数\n   \n 0. 安装 多种安装方式：\n conda安装  conda install cnvkit pip安装  pip install cnvkit 源码安装  git clone https://github.com/etal/cnvkit cd cnvkit/ pip install -e . cnvkit的子命令的参数也通过-h来查询：\ncnvkit.py -h cnvkit.py target -h 注意使用2pip3或源码安装时，要额外安装R和R包DNAcopy。\nconda install r-base conda install bioconda::bioconductor-dnacopy 1. 参考基因组下载   去UCSC Genome Bioinformatics下载对应物种的FASTA格式的参考基因组。【必须】\n  基因的注释数据库，RefSeq或者Ensembl上下载，BED文件或\u0026quot;RefFlat\u0026quot;格式。【可选】\n  如果UCSC上没有你需要的物种类型也没关系，cnvkit只关注提供的参考基因组的FASTA格式。以上两种文件都必须是解压文件。\n如果target格式如下：\nchr1 1508981 1509154 chr1 2407978 2408183 chr1 2409866 2410095 需要额外的refFlat.txt文件。\n如果格式如下：\nchr1 1508981 1509154 SSU72 chr1 2407978 2408183 PLCH2 chr1 2409866 2410095 PLCH2 则不需要refFlat.txt文件。\n如果是全基因组测序数据，那么不需要target的BED文件，但是可能仍然需要refFlat.txt文件。\n2. BAM文件的准备 匹配测序read到参考基因组上，使用各种比对方法得到BAM文件。\n使用以BWA-MEM为例的aligner，并选择标记reads的次要比对位置的选项，使用SAMBLASTER、SAMBAMBA或Picard工具中的MarkDuplicates脚本标记PCR复制，以便CNVkit在计算读深度时跳过这些reads。一般情况下还会继续对最终的BAM文件使用samtools或SAMBAMBA进行index，CNVkit包含这一步。\nBAM文件必须是sorted的。CNVkit将检查前几个读取是否按位置顺序排序，如果不是，将报错。但是，CNVkit可能不会注意到文件后面的读取是未排序的;它只会忽略无序读取，并且在此点之后覆盖范围将为零。为了安全起见，请正确地对BAM文件进行排序。\n 如果已经预先构建了BAM索引文件.bai，注意保证它的时间timestamp晚于BAM文件，当.bai的时间早于.bam文件，CNVkit会自动对BAM文件索引，特别注意在下载BAM文件和对应bai文件时，由于bai文件很小，会优先被下载，导致其时间早于BAM文件，从而CNVkit会认为索引文件已过期，并重新构建。为避免这种情况发生，在所有文件完成下载后，使用unix命令touch来更新文件时间。\n 3. 构建正常参考组 假设BAM文件是肿瘤样本（种系疾病样本可以代替肿瘤样本）和正常样本，CNVkit使用bait的BED文件（可以由capture kit的供应商提供），参考基因组序列，测序可及区域（可选）以及 BAM 文件来：\n  多个正常样本获得的每个bin的拷贝数估计的参考池\n  接着，使用这个参考池对所有的肿瘤样本进行估计（来自同一个测序平台和同样的文库准备的方法）。\n  以上步骤可以使用batch命令完成。假设所有正常样本的后缀都是Normal.bam，所有肿瘤样本的后缀都是Tumor.bam：\ncnvkit.py batch *Tumor.bam --normal *Normal.bam \\  --targets my_baits.bed --fasta hg19.fasta \\  --access data/access-5kb-mappable.hg19.bed \\  --output-reference my_reference.cnn --output-dir example/ 其他参数的使用参考cnvkit.py batch -h。\n如果没有正常样本做参考，可以使用--normal/-n不指定任何BAM，来假设所有的bin都是equal coverage：\ncnvkit.py batch *Tumor.bam -n -t my_baits.bed -f hg19.fasta \\  --access data/access-5kb-mappable.hg19.bed \\  --output-reference my_flat_reference.cnn -d example2/ 以及，无论有没有正常样本都应该使用参考基因组序列FASTA文件运行此命令，以提取 GC 和 RepeatMasker 信息以进行偏差校正，这样 CNVkit 即使没有配对的正常样本也能够优化拷贝率估计值。\n如果target缺少基因名称，可以使用 \u0026ndash;annotate 参数在此处添加它们：\ncnvkit.py batch *Tumor.bam -n *Normal.bam -t my_baits.bed -f hg19.fasta \\  --annotate refFlat.txt --access data/access-5kb-mappable.hg19.bed \\  --output-reference my_flat_reference.cnn -d example3/ 命令和参数 batch 针对1个或多个BAM文件运行CNVkit pipeline：\n# From baits and tumor/normal BAMs cnvkit.py batch *Tumor.bam --normal *Normal.bam \\  --targets my_baits.bed --annotate refFlat.txt \\  --fasta hg19.fasta --access data/access-5kb-mappable.hg19.bed \\  --output-reference my_reference.cnn --output-dir results/ \\  --diagram --scatter 针对构建的参考池运行其他的样本：\n# Reusing a reference for additional samples cnvkit.py batch *Tumor.bam -r Reference.cnn -d results/ 使用target和antitarget构建新的参考（不分析）：\n# Reusing targets and antitargets to build a new reference, but no analysis cnvkit.py batch -n *Normal.bam --output-reference new_reference.cnn \\  -t my_targets.bed -a my_antitargets.bed \\  -f hg19.fasta -g data/access-5kb-mappable.hg19.bed  -p：并行处理每个BAM 文件。  cnvkit.py batch *.bam -r my_reference.cnn -p 8  \u0026ndash;method：  针对全基因组测序数据，--method设置为wgs，针对targeted amplicon sequencing，--method设置为amplicon，默认情况下是hybrid。\ntarget 准备捕获区域（baited region）的BED文件：\ncnvkit.py target my_baits.bed --annotate refFlat.txt --split -o my_targets.bed BED文件是target capture kit中捕获（baited）的基因组区域，这些区域（通常是外显子）可能大小不相等，因此--split选项会划分较大的区域，以便划分后的平均 bin 大小接近--average-size，如果三个参数中的任何一个--split, --annotate, --short-names设置了数值，新的target BED文件就会构建，否则，只使用给定的BED文件。\n没有capture regions BED文件怎么办？\n如果没有capture regions BED文件，但是知道使用的捕获外显子的商用试剂盒，可以在该数据仓库中尝试寻找需要的文件：Astra-Zeneca’s reference data repository，或者去试剂盒的公司官网等其他途径获取文件。如果都找不到，可以尝试使用script guess_baits.py，来根据使用的参考基因组已知的外显子区域来推测。\n bin size和分辨率 如果需要更高的分辨率，可以对选择更小的target以及antitarget的平均大小。\n人类基因组中外显子的平均长度约200bp，targed bin size默认是267bp，一个bin中包含的read太少会产生噪音较大的拷贝数信号，通过指定最小的bin大小来确保通过分割更大的外显子产生的噪音，不会大于平均长度下产生的噪音。\n如果把target bin的平均大小设置为100bp，比如：将target bin的平均大小设置为100bp，分辨率更高，并生成两倍的target bin。但是每个bin中的read数量也会减少一半，增加了bin水平上覆盖范围的方差或“噪音”（抽象意义上的噪声更大），大量的噪声bin使得可视化变得困难，并且由于噪声可能不会正态分布，尤其是存在很多0个read的bin的情况下，分割算法可能在低覆盖的样品上存在较少准确的结果。在该工具的实践中，好的结果伴随平均每个bin的reads数是200-300个read，因此建议将总体on-target的测序覆盖深度——对于100bp的读长至少要有200x～300x，以此证明把平均target bin的大小降低到了100bp。\n针对hybrid capture，如果目标不是均匀密度具体参考官方文档的描述，这里不做赘述。\n标定target区域 如果BED文件没有标定每个区域对应的基因名字，--annotate参数可以加上或者替代原有标签，基因注释的数据库比如RefSeq或Ensembl，在可以在UCSC上找到flat格式的数据，比如：refFlat.txt\n如果区域标签是组合的，组合了基因名和数据库的accession号，使用竖线隔开|，比如：(e.g. “ref|BRAF,mRNA|AB529216,ens|ENST00000496384”)，--short-names参数可以根据分割符号进行拆分，然后选择单个标注，该标注涵盖了覆盖该标注的最大连续区域数，并将其应用为这些区域的新标签【可能是合并同样标记的区域的意思，此处存疑】。（直接应用refFlat注释更简单）\n目标区域不一定是基因，但是为了方便起见，CNVKit的文档和源代码通常是指与“gene”相同标签的连续目标区域。\naccess 从给定的参考基因组计算染色体中可及序列的坐标（sequence-accessible coordinates），输出为一个BED文件。\ncnvkit.py access hg19.fa -x excludes.bed -o access-excludes.hg19.bed cnvkit.py access mm10.fasta -s 10000 -o access-10kb.mm10.bed 很多完整的测序基因组，包括人类基因组都包含大片段的未知区域（比如着丝粒、端粒和高度重复的区域等等），参考基因组中这些区域都是“N”标记的，这些区域无法匹配，CNVkit在计算antitarget bin location时会避开这些区域。access针对给定参考基因组，基于masked-out序列计算可获取序列的位置，把大范围的“N”当作不可获取的区域。\n其他已知的不可比对的，variable或者poorly测序的区域，可以通过-x/--exclude参数来排除这些区域，这个选项可以多次使用来排除多个BED文件中列出的不同的区域，比如，低比对度（poor mappability）的区域可以通过其他方式预先计算出来，比如可通过UCSC FTP Server获取。\n如果基因组中有多个小的需要排除或不可及的区域，那么小的可靠度低的antitarget bin会被挤到剩余的可及区域中，-s选项忽略原本会被排除的短区域，从而允许较大的antitarget bin来覆盖他们。\nUCSC reference human genome build hg19的access文件，包含一些需排除的已知低比对度（low-mappability）区域，在CNVkit目录下data/access-5kb-mappable.hg19.bed。\nantitarget 给定“target”的BED文件，列出了用来靶向重测序数据的染色体坐标，得到off-target的\u0026quot;antitarget\u0026quot;区域的BED文件。\ncnvkit.py antitarget my_targets.bed -g data/access-5kb-mappable.hg19.bed -o my_antitargets.bed 通过短读测序不能比对到特定基因组的区域，可以通过使用-g或--access参数来pass可及区域的位置，从而避免计算这些antitarget的区域。\nCNVkit使用的默认off-target bin的大小很谨慎，根据经验，这样会包含比平均的on-target bin更多的read，不过cnvkit鼓励使用者评测CNVkit得到的覆盖度计算，并设定合适的经过计算得到的off-target bin size来最大程度的获取拷贝数信息。\noff-target bin size 一个合适的off-target bin大小可以计算为：平均目标区域大小和目标区域中测序read的富集的乘积，这样大致和匹配到目标区域和非目标区域的read数量平均值相同，大致与目标富集的水平成正比。\n初步的覆盖度信息可以通过Picard suite的脚本CalculateHsMetrics计算。也可以通过CNVkit的coverage来获得。\n覆盖度coverage 通过BAM的读深来计算覆盖度。\n使用-count参数计算interval中read的起点位置的数量并归一化interval size从而得到覆盖度。\ncnvkit.py coverage Sample.bam Tiled.bed -o Sample.targetcoverage.cnn cnvkit.py coverage Sample.bam Background.bed -o Sample.antitargetcoverage.cnn  interval文件  对于外显子组测序，必须使用interval文件，而且需要对两端进行扩展，GATK推荐两端各扩展100bp，可以减少脱靶，且进行并行处理可以提高分析速度，BQSR必须限制区域以消除脱靶，因为内含子区域是无信息的。\nautobin 快速估计BAM文件的read count或深度来评估合理的on或off-target bin size，如果给定了很多BAM文件，使用平均大小。\n生成target和antitarget BED文件（如果相反），输出估计的平均读深和推荐的bin size表格。\ncnvkit.py autobin *.bam -t my_targets.bed -g access.hg19.bed cnvkit.py autobin *.bam -m amplicon -t my_targets.bed cnvkit.py autobin *.bam -m wgs -b 50000 -g access.hg19.bed --annotate refFlat.txt BAM的inde文件.bai可以快速判断文件中的read总数，并随机对target区域进行采样-t用来估计on-target的平均读深，这种方法比使用coverage命令更快。\nreference 对于包含正常样本的给定文件或文件夹构建拷贝数参考池，如果给定了-f参数（参考基因组），同样会计算每个区域的GC含量和repeat-masked proportion。参考池可以对0，1或多个对照样本构建。\n参考应该对每个target capture panel构建，使用BED文件中列出来的baited region的基因组坐标，理想情况下，对照样本或“正常”样本应该都属于同一类样本，比如都是新鲜组织提取的或者都是FFPE提取的，并且和实验组比如肿瘤样本使用同样的文库构建的方法（library preparation protocol）。\n对于target amplicon或全基因组测序，antitargetBED和.cnn文件可以省略，需要使用这两个文件时，要确保文件名后缀是相同的：.targetcoverage.cnn和.antitargetcoverage.cnn。\n 配对或pooled正常样本  通过coverage得到的*.targetcoverage.cnn和*.antitargetcoverage.cnn：\ncnvkit.py reference *coverage.cnn -f ucsc.hg19.fa -o Reference.cnn 分析在同一个平台得到的测序结果，即使配对的测序了normal-tumor的样本，仍推荐把所有的正常样本放进参考池中，该工具的benchmark发现使用参考池的性能稍优于分别分析配对的样本，另外，即使一个cohort中测序的匹配的normal样本仍会存在不同的拷贝数bias。（Plagnol st al. 2012和Backenroth et al. 2014），在chort中重复使用一个参考池提供了一致性，能帮助诊断此类的问题。\n关于样本选择：    可以使用cnvkit.py metrics *.cnr -s *.cns来查看样本噪音是否非常大。\n  目标区域覆盖度最小大约为10倍左右，CNVkit通常可以准确地检测较大的拷贝数变异，但可能会产生更多虚假分段，而且在此覆盖度以下往往难以推断出更小的拷贝数变异或者亚克隆拷贝数变异。这里的低最覆盖度远低于通常用于单核苷酸变异（SNV）检测的最低覆盖度阈值，尤其是对于可能存在明显正常细胞污染和亚克隆肿瘤细胞群体的肿瘤样本的靶向测序。因此，通过其他质控检查的正常样本可能可以用于构建CNVkit参考，前提是它和要调用的其他样本在同一平台上进行了测序。\n  如果没有正常样本，也支持使用一部分肿瘤样本构建参考池，可以使用scatter命令对于原始的.cnn coverage文件来帮助选择样本，选择拷贝数变异相对少的和非经常性的变异的样本作为参考（不会在不同样本中频繁观察到）。  没有对照样本 可以从target和和antitarget区间文件中为每个探针创建一个中性拷贝数（即log2 0.0）的“flat”参考。如果提供了参考基因组，这仍会计算每个区域的GC含量。\ncnvkit.py reference -o FlatReference.cnn -f ucsc.hg19.fa -t targets.bed -a antitargets.bed 用于构建flat reference可能的用法：\n  从一个或少数几个肿瘤样本中提取拷贝数信息，当没有合适的参考样本或正常样本集时。拷贝数调用可能不够准确，但大范围的拷贝数变异仍然可见。\n  创建一个“dummy（虚拟）”参考，作为batch命令的输入，用于处理一组正常样本。然后，从生成的*.targetcoverage.cnn和*.antitargetcoverage.cnn文件中创建一个“真实”的参考，并使用此更新的参考重新运行一组肿瘤样本的批处理。\n  用来评估给定的配对或参考池是否适合分析，重复使用CNVkit来比较相同的样本情况下，使用原始的文件做参考和使用flat reference做参考得到的拷贝数变异的差别。\n  CNVkit构建对照组信号的工作原理 CNVkit从对照样本中提取可用信号的方法：\n首先，对每个输入样本进行中位数居中处理（Median-centered），然后在每个正常样本上分别执行读深度偏差校正（与fix命令中使用的相同方法）。\n\u0026#34;Median-centered\u0026#34; 意味着对数据进行中位数居中处理。这个过程包括找到数据集的中位数（即将数据按大小排列，找到位于中间的值），然后将数据中的每个值减去这个中位数，以使得数据的中位数变成零。这样做可以减少数据中的整体偏差，使得数据更加集中在零值周围，有助于后续分析更准确地反映数据的变化特征。 然后将样本的中位数居中、偏差校正的log2读深度值进行组合，计算每个样本在所有样本中的加权平均值（Tukey的双加权位置）和扩展度（Tukey的双加权中间方差），在每个靶向和非靶向基因组区段中的值。（有关这些统计方法的背景，请参阅Lax (1985)和Randal (2008)。）为了调整对于较少样本估计参数的较低统计可靠性，计算这些值时，数据集中包含一个等同于中性拷贝数的“伪计数”样本。\n这些值以“reference.cnn”文件的形式保存，作为“log2”和“spread”列，表示预期读深度和此估计的可靠性。\n如果提供了参考基因组的FASTA文件，对于每个基因组bin，计算并存储GC的比例（子序列中“G”和“C”字符占所有“A”、“T”、“G”和“C”字符的比例，忽略“N”和任何其他模糊字符）和重复掩码值\u0026quot;repeat-masked\u0026quot;（序列中小写的非“N”字符的比例）作为“reference.cnn”文件的“gc”和“rmask”列。为了提高效率，samtools FASTA索引文件（.fai）用于定位FASTA文件中的binned序列区域。如果使用\u0026ndash;no-gc或\u0026ndash;no-rmask选项跳过GC或RepeatMasker偏差校正，则输出文件中将省略这些列；如果两者都被跳过，则根本不检查基因组FASTA文件。\n最终得到的是一个参考拷贝数profile文件，可以用于校正其他单个样本。\n\u0026#34;Repeat-masked\u0026#34; 是指在基因组序列中屏蔽掉重复序列部分的过程。基因组中存在许多重复性序列，这些序列可能在不同区域重复出现，包括转座子、重复元件等。为了避免这些重复序列对分析造成影响，通常会对基因组序列进行“掩盖”（mask），即将这些重复序列的部分标记为无效或不参与分析的区域。这通常以特殊的字符标记或将其标记为小写字母来实现，在分析过程中会忽略这些标记的区域，从而更准确地分析非重复部分的基因组序列。 示例：对WGS测序数据使用cnvkit提取CNA  bam文件的准备  bwa mem -K 10000000 -t 50 /home/data/sde/cj/ecDNA_calling/data/data_repo/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa SRR8236745_1.fastq SRR8236745_2.fastq | samtools view -Shu - | samtools sort -m 4G -@4 -o ./out_raw/SRR8236745.cs.bam -; } 2\u0026gt;./out_raw/SRR8236745_aln_stage.stderr 使用bwa mem命令进行基因组序列比对。-K 10000000设置最大处理区间长度为10000000（1000万），-t 50指定了使用的线程数为50。输入的参数包括参考基因组序列文件/data_repo/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa以及两个FASTQ格式的测序数据文件SRR8236745_1.fastq和SRR8236745_2.fastq。该命令将比对的结果通过管道传递给下一个命令。\n使用samtools view命令将bwa mem比对得到的SAM格式的数据转换为BAM格式，选项-Shu -表示输入来自标准输入，然后使用samtools sort命令对BAM文件进行排序。选项-m 4G指定排序时使用的内存大小为4G，-@4指定使用4个线程，并将排序后的结果输出到./out_raw/SRR8236745.cs.bam文件中。\n执行过程中的错误信息通过2\u0026gt;重定向到./out_raw/SRR8236745_aln_stage.stderr文件中。\n 使用cnvkik提取没有配对样本的肿瘤样本拷贝数变异  ref=/public/home/chengjing2/taozy/ecDNA/data_repo/GRCh37 tumor=/public/home/sample out=/public/home/cnvkit/result  cnvkit.py batch $tumor/tumor.bam \\ -m wgs \\ -n \\ -f $ref/hs37d5.fa \\ -d $out/ 注意，batch的命令实际上完成了以下所有的内容：\ncnvkit.py access hg19.fa -o access.hg19.bed cnvkit.py autobin *.bam -t baits.bed -g access.hg19.bed [--annotate refFlat.txt --short-names]  # For each sample... cnvkit.py coverage Sample.bam baits.target.bed -o Sample.targetcoverage.cnn cnvkit.py coverage Sample.bam baits.antitarget.bed -o Sample.antitargetcoverage.cnn  # With all normal samples... cnvkit.py reference *Normal.{,anti}targetcoverage.cnn --fasta hg19.fa -o my_reference.cnn  # For each tumor sample... cnvkit.py fix Sample.targetcoverage.cnn Sample.antitargetcoverage.cnn my_reference.cnn -o Sample.cnr cnvkit.py segment Sample.cnr -o Sample.cns  # Optionally, with --scatter and --diagram cnvkit.py scatter Sample.cnr -s Sample.cns -o Sample-scatter.pdf cnvkit.py diagram Sample.cnr -s Sample.cns -o Sample-diagram.pdf 输出文件夹中内容：\nhs37d5.antitarget.bed hs37d5.target.bed tumor.antitargetcoverage.cnn tumor.cnr tumor.bintest.cns tumor.cns tumor.call.cns tumor.targetcoverage.cnn reference.cnn 参考资料   cnvkit github page\n  cnvkit官方指南\n ","date":"January 4, 2024","image":null,"permalink":"/post/2024-01-04_cnvkit_usage/","title":"CNVKit使用方法【1】"},{"categories":["BUG"],"contents":"介绍cnvkit的使用方法和原理。\n fix 合并未校正的target和antitarget coverage tables（.cnn），并根据给定的参考来校正区域覆盖和GC含量的偏差。输出一个拷贝数比率的表格（.cnr）。\ncnvkit.py fix Sample.targetcoverage.cnn Sample.antitargetcoverage.cnn Reference.cnn -o Sample.cnr “观察到”的on-target和off-target读深度分别进行中位数居中和偏差校正，就像构建参考时一样。然后，从参考中得到相应的“预期”归一化的log2读深度值，并对每组区间进行相减。\n偏差校正使用参考.cnn文件中“gc”和“rmask”列的GC和RepeatMasker信息；如果这些信息缺失（即参考是在没有这些校正的情况下构建的），fix命令也会跳过这些校正（并发出警告）。如果构建了参考，但随后使用不同的偏差校正标志调用了fix命令，可能会在测试样本中过度或不足地校正偏差，因此，需要一致地使用选项\u0026ndash;no-gc、\u0026ndash;no-rmask和\u0026ndash;no-edge，或者完全不使用这些选项。\nCNVkit会过滤掉未达到某些预定义标准的bin：参考log2读深度低于阈值（默认为-5）或者参考中所有正常样本的读深度扩展（spread）超过阈值（默认为1.0）的bin。\n给剩余bin分配权重，考虑以下因素：\n  bin的大小；\n  bin在参考中log2值偏离0的程度；\n  bin在参考中的“扩展”（spread）程度。\n  后两者仅在至少使用一个正常/对照样本构建参考时适用。\n最后，修正后的on-target和off-target bin级别的拷贝比率以及相关权重被串联、排序，并写入一个.cnr文件。\nsegment 从给定的coverage表格中推断离散的拷贝数segment：\ncnvkit.py segment Sample.cnr -o Sample.cns segmentation对每个染色体臂单独运行，可以使用-p来并行（除了HMM方法），类似于batch。segment或断点的显著性阈值通过\u0026ndash;threshold/-t选项传递给底层方法，通常是p值或q值的cutoff，或者底层方法用于调整灵敏度的任何参数。\n segmentation方法  以下分割算法可以通过-m选项指定：\n  cbs：默认值，循环二分法分割（CBS）。在中等大小的target panel和外显子组中，该方法在作者的benchmark测试中表现最佳。需要R包DNAcopy。\n  haar：HaarSeg的纯Python实现，是基于小波的方法，非常快速，在small panel上表现合理，但在大型数据集上容易过度分割。\n  小波分析，这是一种信号处理技术，可以将信号分解成不同频率范围的组成部分。小波分析对于检测信号中的突变或突发性变化（例如，拷贝数变异）以及信号的局部特征非常有效。在基因组分析中，基于小波的方法可以用于检测基因组中的拷贝数变异或其他结构变化，通过对信号进行多尺度分解，捕获不同频率范围内的变化模式，以便更好地识别基因组的特定区域或变异模式。   hmm（实验性）：适用于大多数样本的3状态隐马尔可夫模型。比CBS更快，比Haar更慢但更准确。需要Python包pomegranate，以及下面两种方法。\n  hmm-tumor（实验性）：适用于优质肿瘤样本更细的分割，5状态隐马尔可夫模型。该方法能够检测到更大规模的，更小幅度的拷贝数扩增，或者在大规模hemizygous loss中检测到局部深度缺失。训练该模型比简单的hmm方法需要更多CPU。【怎么判断肿瘤样本的优质程度】\n  hmm-germline（实验性）：一个具有固定删除、中性和扩增状态的3状态隐马尔可夫模型，对应于绝对拷贝数为1、2和3。适用于生殖细胞样本和大多数二倍体基因组的单细胞测序样本。\n  none：简单地计算每个染色体臂的加权平均log2值。用于测试、调试，或作为其他方法基准测试的基线。\n  hemizygous loss 第一个方法cbs在内部使用了R，如果是conda安装的，依赖会自动安装，源码安装的需要另外安装R和R包依赖（即DNAcopy）。如果在非默认位置安装了R包，可以使用\u0026ndash;rscript-path选项指定Rscript可执行文件的位置。HMM方法hmm、hmm-tumor和hmm-germline是在CNVkit v.0.9.2中暂时引入的，可能会在新版本中进行更改。它们依赖于Python包pomegranate，可以通过conda和pip获取。\nhaar和none方法除了基本的CNVkit安装外，没有任何额外的依赖。\nbin filtering  权重为0的bin在segmentation之前会被分割，额外的筛选条件：\n  \u0026ndash;drop-low-coverage：去除读深为0的或接近0的bin，在肿瘤样本中使用。\n  \u0026ndash;drop-outliers：考虑到局部变异，去除和rolling平均值太远的log2值的区间，默认情况下使用。\n  SNP allele frequencies  如果使用\u0026ndash;vcf选项提供了VCF文件（以及相关选项 -i、-n、-z 和 \u0026ndash;min-variant-depth，其工作方式与其他命令相同），则在分段log2比率后，将从VCF中加载的SNP等位基因频率进行每个基于log2比率的segmentation的第二次分割。\n另请参考(获取拷贝数扩增和删除章节)[https://cnvkit.readthedocs.io/en/stable/calling.html]来获取有关如何解释和后处理生成的片段的建议。\ncall  基于给定的segmented log2 ratio估计.cns，获取每个segment的绝对整数拷贝数：   对于每个拷贝数状态，使用阈值log2值的列表（-m threshold）进行判定。或者，通过重新缩放（根据已知的肿瘤细胞比例和正常倍性），然后简单地四舍五入到最接近的整数拷贝数（-m clonal）。  cnvkit.py call Sample.cns -o Sample.call.cns cnvkit.py call Sample.cns -y -m threshold -t=-1.1,-0.4,0.3,0.7 -o Sample.call.cns cnvkit.py call Sample.cns -y -m clonal --purity 0.65 -o Sample.call.cns cnvkit.py call Sample.cns -y -v Sample.vcf -m clonal --purity 0.7 -o Sample.call.cns 输出是另一个.cns文件，其中包含一个额外的“cn”列，列出了每个片段的绝对整数拷贝数。这个.cns文件仍然与其他接受.cns文件的CNVkit命令兼容，并且可以使用scatter、heatmap和diagram命令以相同的方式绘制。要以其他格式（如BED或VCF）获取这些拷贝数值，参考export命令。\n对于具有SNV的VCF文件（-v/\u0026ndash;vcf），会提取肿瘤样本中SNP的b-等位基因频率，并对每个片段进行平均化：\ncnvkit.py call Sample.cns -y -v Sample.vcf -o Sample.call.cns 分段的b-等位基因频率也用于计算主要和次要等位特异性的整数拷贝数（见下文）。\n另外，选项 `-m none`` 执行重新缩放、重新居中，并从VCF中提取b-等位基因频率（如果需要），但不添加“cn”列或等位拷贝数：\ncnvkit.py call Sample.cns -v Sample.vcf --purity 0.8 -m none -o Sample.call.cns Transformations  如果在分析的肿瘤样本中存在已知水平的正常细胞DNA污染（请参阅肿瘤异质性页面），您可以选择重新缩放您的 .cnr 或 .cns 文件中的log2拷贝比率估计，以消除这种污染的影响，使文件中的结果log2比率值与完全纯净的肿瘤样本观察到的值相匹配。\n通过\u0026ndash;purity选项，log2比率重新缩放为在完全纯净、未污染样本中观察到的值。.cns输入文件中观察到的log2比率被视为某一部分肿瘤细胞（由\u0026ndash;purity指定），可能具有改变的拷贝数，以及剩余的具有中性拷贝数的正常细胞（\u0026ndash;ploidy指定常染色体；默认情况下，常染色体是二倍体，单倍体Y或X/Y取决于参考性别）。该结果被重新排列以找到仅肿瘤细胞的绝对拷贝数，四舍五入到最近的整数。\n不同性染色体（X和Y染色体）的理论值和观察值存在不同，因此如果在构建参考时使用了 -y / \u0026ndash;male-reference / \u0026ndash;haploid-x-reference 选项，则在此处重要的是指定同样的选项。如果已知样本性别，则可以指定样本性别，否则将从染色体X的平均log2比率中猜测样本性别。（另请参阅：染色体性别）。\n当使用 -v/\u0026ndash;vcf 选项提供了同一肿瘤样本（可选包含匹配的正常）的SNV调用的VCF文件时，落在每个片段内的杂合性、非体细胞SNV的b-等位基因频率（BAFs）被mirrored、平均，并列在输出的 .cns 文件中作为额外的“baf”列（使用与export nexus-ogt相同的逻辑）。如果指定了 \u0026ndash;purity，则还将重新缩放BAF值。\ncall命令还可以选择重新居中log2值，尽管当归一化到参考并纠正偏差时，.cnr文件通常不需要这样做，因为fix命令会自动将其进行中位数居中。但是，如果分析的基因组高度非等倍性，并且包含广泛的不平衡拷贝数删除或扩增，那么默认的中位数居中可能会使拷贝数中性区域略高于或略低于期望的log2值0。针对这种情况，可以使用 \u0026ndash;center 选项指定替代的居中方法：\ncnvkit.py call -m none Sample.cns --center mode Calling methods  经过上面的矫正后，threshold和clonal方法对每个segment计算绝对整数拷贝数值。\nclonal方法通过给定\u0026ndash;ploidy把log2值转换成绝对值，随后简单的把绝对拷贝数数值四舍五入到最邻近的整数上，该方法对种系(germline)样本、纯度很高的肿瘤样本（比如细胞系等等），或者当肿瘤占比是已知的并且通过\u0026ndash;purity指定的情况下是合理的。\nthreshold方法对每个整数拷贝数状态应用固定的log2比率cutoff值，这种方法可以作为指定和矫正肿瘤细胞占比或纯度的替代方法，一旦指定了\u0026ndash;purity，在应用拷贝数cutoff之前仍然会对log2值进行重新的缩放。\n默认的阈值对于至少30%纯度的肿瘤样本是“安全”的。+0.2和-0.25的内部截断值足够敏感，可以在纯度（或亚克隆细胞比例）仅为30%的二倍体肿瘤中检测到单拷贝扩增或删除。但-1.1和+0.7的外部截断值假设100%的纯度，因此更极端的拷贝数，即纯合性缺失（0拷贝）或多拷贝扩增（4+拷贝），只有在有强有力的证据支持时才会被分配给拷贝数变异。对于生殖细胞样本，下面显示的-t值（或-m clonal）可能会产生更精确的结果。\n下表中阈值按顺序映射到整数拷贝数：\n \n 对于已知倍性的homogeneous样本，可以通过：对感兴趣的整数拷贝数值进行对数转换，加上0.5（用于四舍五入），然后除以倍性来计算cutoff。对于二倍体基因组：\n\u0026gt; log2( (0:4 + .5) / 2) [1] -2.0000000 -0.4150375 0.3219281 0.8073549 1.1699250 对于任意纯度和倍性：\n\u0026gt; purity = 0.6 \u0026gt; ploidy = 4 \u0026gt; log2( (1 - purity) + purity * (0:6 + .5) / ploidy ) [1] -1.0740006 -0.6780719 -0.3677318 -0.1124747 0.1043367 0.2927817 0.4594316 等位频率和计数  如果使用 -v/\u0026ndash;vcf 选项提供了VCF文件，则针对VCF中包含SNV的每个片段，计算该片段内的平均b-等位频率（BAF），并将其输出到“baf”列。然后，从总拷贝数和BAF推断出等位特异性的整数拷贝数值，并输出到“cn1”和“cn2”列。此计算使用与PSCBS相同的方法：总拷贝数乘以BAF，四舍五入至最近的整数。\n当一个片段的“cn1”和“cn2”字段具有不同值时，会显示等位失衡情况，包括拷贝数中性的等位基因丢失（LOH）。\n片段过滤（自版本0.8.0起）  最后，可以根据多个标准对片段进行过滤，这些标准可以组合使用：\n  整数拷贝数（cn），合并相邻的相同调用值。\n  仅保留高水平的扩增（5个或更多拷贝）和纯合性缺失（0拷贝）（ampdel）。\n  置信区间与零重叠（ci）。\n  平均标准误差（sem），参数估计的置信区间，其行为类似。\n  在每种情况下，根据给定标准具有相同值的相邻片段将被合并在一起，并且列值将被适当地重新计算。即使总拷贝数相同，不同染色体上的片段或具有不同等位特异性拷贝数值的片段也不会被合并。\n针对WGS和target amplicon capture CNVkit最初是基于hybrid capture测序数据设计的，即存在off-target read并提升拷贝数估计。不过，CNVkit仍可用于WGS和targeted amplicon sequencing（TAS）数据。\nWGS CNVkit把WGS数据，当作给定参考基因组区域为“target”，没有\u0026quot;antitarget\u0026quot;区域的数据，对应命令为“batch \u0026ndash;method wgs”，因为输入没有包含有用的per-target gene label，需要给定gene annotation数据集并在输出中使用标记的基因。\ncnvkit.py batch Sample1.bam Sample2.bam -n Control1.bam Control2.bam \\  -m wgs -f hg19.fasta --annotate refFlat.txt 为了快速或提升WGS分析的准确性，可以尝试以下方法：\n  不必分析整个基因组，可以使用“target” BED文件将分析限制在基因区域。例如，您可以从UCSC基因组浏览器获取这样的BED文件。\n  增加“target”的平均bin大小（\u0026ndash;target-avg-size），例如，至少1000个碱基适用于30倍覆盖度，或者对于覆盖度较低的测序，按比例增加更多。【cnvkit是否对浅测序有推荐的碱基数目】\n  指定较小的p值阈值（segment -t）。对于CBS方法，1e-6可能效果不错。或者，尝试hmm分割方法。\n  在batch、coverage和segment命令中使用 -p/\u0026ndash;processes 选项，以确保利用所有可用的CPU。\n  确保您使用的是CNVkit的最新版本。每个版本都包含一些性能改进。\n  关闭reference和fix命令中的“edge”偏差校正（\u0026ndash;no-edge）。\n  batch -m wgs选项自动执行了所有这些操作，除了第一个。\nTarget amplicon sequencing (TAS) 当目标捕获方法是amplicon sequencing时，不会对off-target区域进行测序。虽然这相对于杂交捕获限制了测序数据中可用的拷贝数信息，但CNVkit可以仅使用on-target覆盖度并从分析中排除所有off-target区域来分析TAS数据。\nbatch -m amplicon选项使用给定的target推断覆盖度，忽略off-target区域：\ncnvkit.py batch -m amplicon -t targets.bed *.bam 等同于：\ncnvkit.py target targets.bed --split -o targets.split.bed # Create a blank file to substitute for antitargets touch MT # For each sample cnvkit.py coverage Sample.bam targets.split.bed -p 0 -o Sample.targetcoverage.cnn cnvkit.py reference *.targetcoverage.cnn --no-edge -o ref-tas.cnn cnvkit.py fix Sample.targetcoverage.cnn MT ref-tas.cnn --no-edge 这种方法不收集on-target区域之间的任何拷贝数信息，因此只有在确实使用了target amplicon sequencing protocol 准备样本时才应使用，且该方法不尝试在基因水平进一步对每个引物进行规范化，可能会在CNVkit的未来版本中解决。\n不要在使用此方法测序的样本的BAM文件中标记重复项。  Picard MarkDuplicates、samtools rmdup等工具旨在标记可能的PCR重复（最初用于WGS数据集，但对于杂交捕获也很有用）。诸如GATK和CNVkit之类的变异调用器会在其内部计算中忽略这些读取，认为这些读取是非独立的测量。（这个SeqAnswers主题提供了详细背景信息）。  在目标引物测序中，实际上所有放大的读取都是设计上的PCR重复。通过标记和因此省略这些读取，剩余的覆盖度将会很低，就像没有进行放大一样。 肿瘤样本 CNVkit广泛应用于实体瘤样本的target panel测序，或全外显子组测序protocol，有几种选项和方法可支持这种情况的使用：\n  有非成对的肿瘤样本，或者没有在相同平台上测序的正常样本，参考“reference”命令提到的方法。\n  使用\u0026quot;\u0026ndash;drop-low-coverage\u0026quot;选项忽略log2归一化覆盖值低于-15的区间。几乎所有肿瘤样本，甚至是癌细胞系，都不是完全同质的。即使在最大的肿瘤细胞克隆群体的同源性缺失区域，也会从未缺失的污染正常细胞中获得一些测序读数。因此，极低的log2拷贝比值值并不表示同源性缺失，而是失败的测序或匹配，无论该位点的拷贝数状态如何都不是有效信息。该选项在批处理命令中适用于segmentation；也可在segment、metrics、segmetrics、genemetrics和肿瘤异质性命令中使用。\n  为什么是-15？用于零覆盖bin的空log2值是-20（大约是平均bin覆盖度的百万分之一），通过归一化到参考值可以引入的最大正偏移是5（对于覆盖度为平均覆盖度的1/32的区间；低于此值的区间会被参考所屏蔽）。在.cnr文件中，任何log2值低于-15的区间可能基于对应于零覆盖（可能是无法匹配）的虚拟值，而不是真实观察到的。\n batch命令不直接输出整数拷贝数调用（参考肿瘤异质性）。而是使用call中的\u0026ndash;ploidy和\u0026ndash;purity选项来根据已知或估计的肿瘤细胞比例为每个样本单独计算拷贝数。还考虑在高度非整倍体样本中使用\u0026ndash;center median将真正中性区域的log2值向零点调整，因为初始值可能略有偏差。  如果有VCF格式的SNV调用，则在call和scatter命令中使用-v/\u0026ndash;vcf选项计算或绘制每个片段的b等位基因频率。这些值显示了等位基因失衡和杂合性失调（LOH），支持和扩展了推断的CNV。\n肿瘤异质性 DNA样本从实体瘤中提取很少是纯净的，即样本中通常存在基质或其他正常细胞以及不同亚克隆肿瘤细胞群，这可能会导致分段的log2比值拟合到绝对整数拷贝数过程中出现混乱。\nCNVkit提供了与现有工具和方法整合的多个点，以处理肿瘤异质性和正常细胞污染。\n估计肿瘤纯度和正常污染 通常可以使用以下一种或多种方法大致估计肿瘤纯度：\n  病理学家可以通过显微镜下检查固体肿瘤的样本来直观估计样本的纯度，计数基质细胞和肿瘤细胞。\n  如果认为肿瘤由体细胞突变驱动，例如黑色素瘤中的BRAF V600E突变，则假定该突变完全是克隆的，其等位基因频率表示肿瘤纯度。这可能会受到同一位点的拷贝数变化以及点突变是纯合性还是杂合性的影响，但同一样本中其他体细胞突变的频率可能会解决这个问题。\n  覆盖了种系杂合性SNP的较大规模hemizygous loss会改变样本中相同SNP的等位基因频率。例如，在50%纯度的肿瘤样本中，这些SNP的等位基因频率将从50%变为67%或33%，假设是二倍体样本（即来自正常样本的2/2拷贝中的1/2，来自肿瘤的0或1拷贝，取决于变异等位基因是丢失还是保留）。一般的计算比#1或#2更为复杂，类似地也可以用于拷贝数增加和纯合性缺失。\n  肿瘤样本中的拷贝数变化的log2比值对应于肿瘤细胞中的整数拷贝数，而这些log2值总体上将聚集在表明亚克隆种群的数值周围，每个亚克隆种群具有特定的多倍体和克隆性。例如，在纯度为50%的肿瘤样本中，单拷贝缺失会有中性位点（2/2正常拷贝，1/2肿瘤拷贝）3/4的覆盖度，因此log2值为log2(.75) = -0.415。这种计算也可以推广到其他拷贝数状态。\n  后三种方法的工具实现可直接用于DNA测序数据。\n从测序推断肿瘤纯度和亚克隆种群分数 目前，推断肿瘤种群结构不在CNVkit的范围内，但可以使用其他第三方工具，例如PureCN、THetA2、PyClone或BubbleTree来完成这项工作。每个工具都可用于估计样本中的肿瘤细胞含量和推断肿瘤亚克隆的整数拷贝数。PureCN可直接接受CNVKit的.cnn和.cnr文件作为输入，推荐使用。\n使用CNVkit与THetA2 CNVkit提供了将.cns片段导出为THetA2的输入格式，以及将THetA2的结果文件导入为CNVkit的分段.cns文件的命令。有关使用说明，参考theta和import-theta命令。\n在对样本运行CNVkit并联合对肿瘤和正常样本的SNV之后，可从.cns和.vcf文件生成THetA2的输入文件：\ncnvkit.py export theta Sample_T.cns reference.cnn -v Sample_Paired.vcf 这将生成三个输出文件：Sample_T.interval_count、Sample_T.tumor.snp_formatted.txt和Sample_T.normal.snp_formatted.txt。\n然后，运行THetA2（假设程序解压在/path/to/theta2/）：\n# 生成Sample_T.BEST.results： /path/to/theta2/bin/RunTHetASample_T.interval_count\\--TUMOR_FILESample_T.tumor.snp_formatted.txt\\--NORMAL_FILESample_T.normal.snp_formatted.txt\\--BAF--NUM_PROCESSES`nproc`--FORCE最后，将THetA2的结果导入回CNVkit的.cns格式，把原始分段（.cns）匹配到THetA2推测的绝对拷贝数数值上：\ncnvkit.py import-theta Sample_T.cns Sample_T.BEST.results THetA2会调整分段的log2值以符合每个检测到的亚克隆细胞群的推断细胞含量（cellularity）；如果检测到多个克隆性肿瘤细胞群，则可能会产生一个或两个代表亚克隆的.cns文件。THetA2还对每个表示CNA的分段执行一些显著性检验，因此THetA2得出的分段数可能比CNVkit最初发现的分段数要少。\n结果中的.cns文件的segment数值仍然经过log2转换，以方便使用CNVkit进行绘图等操作。这些文件也可以使用export命令轻松转换为其他格式。\n针对正常细胞污染情况调整拷贝数率和segment CNVkit的call命令使用肿瘤分数的估计值（任何来源的估计）来直接重新调整分段的log2比值以及存在的SNV b-等位基因频率，使其达到在完全纯净、无污染样本中的值。以下是一个例子，肿瘤纯度为60%，参考样本为男性：\ncnvkit.py call -m none Sample.cns --purity 0.6 -y -o Sample.call.cns call命令还可以将分段的log2比率估计转换为绝对整数拷贝数。如果肿瘤细胞比例已经明确，可以使用“-m clonal”方法将log2比率四舍五入至最接近的整数拷贝数。另一种方法是使用“-m threshold”方法来应用硬阈值。需要注意的是，针对纯度的重新缩放是可选的；不管怎样，除非使用“-m none”选项跳过，否则都会产生整数拷贝数。\ncnvkit.py call -m clonal Sample.cns -y --purity 0.65 -o Sample.call.cns # Or, if already rescaled cnvkit.py call -m clonal Sample.call.cns -y -o Sample.call.cns # With CNVkit\u0026#39;s default cutoffs cnvkit.py call -m threshold Sample.cns -y -o Sample.call.cns # Or, using a custom set of cutoffs cnvkit.py call -t=-1.1,-0.4,0.3,0.7 Sample.cns -y -o Sample.call.cns 导出 导出整数拷贝数为BED或VCF格式。\n“export bed”和“export vcf”命令会以标准的BED或VCF格式输出整数拷贝数调用。\ncnvkit.py export bed Sample.call.cns -y -o Sample.bed cnvkit.py export vcf Sample.call.cns -y -o Sample.vcf 如果是通过call命令生成的.call.cns文件，其中计算的整数拷贝数也会被导出。\ngermline分析 CNVkit可以用于constitutional sample（非肿瘤）样本的外显子测序一起使用，例如用于检测与遗传状况相关的生殖系拷贝数变异。然而，请注意，CNVkit在检测小于1Mbp的CNV时精度较低，通常只能检测跨越多个外显子或捕获区域的变异。当用于外显子或target panel数据集时，CNVkit将无法检测人群中更常见的小型CNV。\n要使用CNVkit检测constitutional样本中的中型到大型CNV或非平0衡结构变异，请按照以下步骤操作：\n  可以直接使用call命令，无需指定\u0026ndash;purity和\u0026ndash;ploidy值，因为默认值对于哺乳动物细胞是正确的（对于非二倍体物种，需要使用正确的\u0026ndash;ploidy）。默认的\u0026ndash;method threshold类似于\u0026ndash;method clonal，但对于获取单拷贝变化，有更小的阈值。默认的阈值允许CNV中的镶嵌现象（mosaicism），其对应的log2值比单拷贝CNV所指示的要小。（它们比通常认为的更常见。）\n  \u0026ndash;filter选项在调用中可用于减少返回的假阳性分段数量。要使用ci（推荐）或sem过滤器，首先将每个样本的分段.cns文件通过segmetrics处理，并使用\u0026ndash;ci选项，这将在.cns输出中添加上限和下限置信限，然后call \u0026ndash;filter ci可以使用这些置信限。\n  不应使用\u0026ndash;drop-low-coverage选项（参考肿瘤分析）；它通常会完全删除生殖系深度缺失，这是不可取的。\n  对于使用CNVkit与全基因组测序数据集，参考全基因组测序和靶向引物捕获。\n参考资料   cnvkit github page\n  cnvkit官方指南\n ","date":"January 3, 2024","image":null,"permalink":"/post/2024-01-03_cnvkit_usage2/","title":"CNVKit使用方法【2】"},{"categories":["BUG"],"contents":"解决使用cnvkit中出现的报错。\n 在服务器上使用conda下载了cnvkitconda install cnvkit，然而使用cnvkit.py时遇到如下报错：\ncnvkit.py Traceback (most recent call last):  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/bin/cnvkit.py\u0026#34;, line 10, in \u0026lt;module\u0026gt;  from cnvlib import commands  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/cnvlib/__init__.py\u0026#34;, line 1, in \u0026lt;module\u0026gt;  from skgenome.tabio import write  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/skgenome/__init__.py\u0026#34;, line 1, in \u0026lt;module\u0026gt;  from . import tabio  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/skgenome/tabio/__init__.py\u0026#34;, line 13, in \u0026lt;module\u0026gt;  from ..gary import GenomicArray as GA  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/skgenome/gary.py\u0026#34;, line 9, in \u0026lt;module\u0026gt;  from .intersect import by_ranges, into_ranges, iter_ranges, iter_slices  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/skgenome/intersect.py\u0026#34;, line 11, in \u0026lt;module\u0026gt;  from pandas import Int64Index ImportError: cannot import name \u0026#39;Int64Index\u0026#39; from \u0026#39;pandas\u0026#39; (/public/home/chengjing2/miniconda3/envs/ecDNA/lib/python3.10/site-packages/pandas/__init__.py) 尝试了其他方法提到的更新pandas版本，下载指定版本比如statsmodels==0.14.0，更新cnvkit的版本，在cnvkit.p中指定pandas等等都没有用，从cnvkit的github上下载后安装：\ngit clone https://github.com/etal/cnvkit cd cnvkit/ pip install -e . 重新键入cnvkit.py，报错已解决：\n(ecDNA) [chengjing2@hpc-login-gpu01 cnvkit]$ cnvkit.py usage: cnvkit.py [-h]  {batch,target,access,antitarget,autobin,coverage,reference,fix,segment,call,diagram,scatter,heatmap,breaks,genemetrics,gainloss,sex,gender,metrics,segmetrics,bintest,import-picard,import-seg,import-theta,import-rna,export,version}  ...  CNVkit, a command-line toolkit for copy number analysis.  positional arguments:  {batch,target,access,antitarget,autobin,coverage,reference,fix,segment,call,diagram,scatter,heatmap,breaks,genemetrics,gainloss,sex,gender,metrics,segmetrics,bintest,import-picard,import-seg,import-theta,import-rna,export,version}  Sub-commands (use with -h for more info)  batch Run the complete CNVkit pipeline on one or more BAM files.  target Transform bait intervals into targets more suitable for CNVkit.  access List the locations of accessible sequence regions in a FASTA file.  antitarget Derive off-target (\u0026#34;antitarget\u0026#34;) bins from target regions.  autobin Quickly calculate reasonable bin sizes from BAM read counts.  coverage Calculate coverage in the given regions from BAM read depths.  reference Compile a coverage reference from the given files (normal samples).  fix Combine target and antitarget coverages and correct for biases. Adjust raw coverage data according to the given reference, correct potential biases  and re-center.  segment Infer copy number segments from the given coverage table.  call Call copy number variants from segmented log2 ratios.  diagram Draw copy number (log2 coverages, segments) on chromosomes as a diagram. If both the raw probes and segments are given, show them side-by-side on  each chromosome (segments on the left side, probes on the right side).  scatter Plot probe log2 coverages and segmentation calls together.  heatmap Plot copy number for multiple samples as a heatmap.  breaks List the targeted genes in which a copy number breakpoint occurs.  genemetrics Identify targeted genes with copy number gain or loss.  sex Guess samples\u0026#39; sex from the relative coverage of chromosomes X and Y.  metrics Compute coverage deviations and other metrics for self-evaluation.  segmetrics Compute segment-level metrics from bin-level log2 ratios.  bintest Test for single-bin copy number alterations.  import-picard Convert Picard CalculateHsMetrics tabular output to CNVkit .cnn files. The input file is generated by the PER_TARGET_COVERAGE option in the  CalculateHsMetrics script in Picard tools. If \u0026#39;antitarget\u0026#39; is in the input filename, the generated output filename will have the suffix  \u0026#39;.antitargetcoverage.cnn\u0026#39;, otherwise \u0026#39;.targetcoverage.cnn\u0026#39;.  import-seg Convert a SEG file to CNVkit .cns files.  import-theta Convert THetA output to a BED-like, CNVkit-like tabular format. Equivalently, use the THetA results file to convert CNVkit .cns segments to integer  copy number calls.  import-rna Convert a cohort of per-gene log2 ratios to CNVkit .cnr format.  export Convert CNVkit output files to another format.  version Display this program\u0026#39;s version.  options:  -h, --help show this help message and exit  See the online manual for details: https://cnvkit.readthedocs.io 参考资料  cnvkit github page ","date":"January 2, 2024","image":null,"permalink":"/post/2024-01-02_cnvkit_install_error/","title":"CNVKit：ImportError: cannot import name 'Int64Index' from 'pandas'"},{"categories":["BUG"],"contents":"解决使用cnvkit运行中出现的报错。\n 我在使用CNVkit 0.9.11.dev0处理输入为bam文件时，遇到该报错\nCNVkit 0.9.11.dev0 Wrote /public/home/result/GRCh37_cnvkit_filtered_ref.target-tmp.bed with 565907 regions Wrote /public/home/result/GRCh37_cnvkit_filtered_ref.antitarget-tmp.bed with 0 regions Running 1 samples in serial Traceback (most recent call last):  File \u0026#34;/public/home/chengjing2/miniconda3/envs/ecDNA/bin/cnvkit.py\u0026#34;, line 8, in \u0026lt;module\u0026gt;  sys.exit(main())  File \u0026#34;/public/home/chengjing2/taozy/tools/cnvkit/cnvlib/cnvkit.py\u0026#34;, line 10, in main  args.func(args)  File \u0026#34;/public/home/chengjing2/taozy/tools/cnvkit/cnvlib/commands.py\u0026#34;, line 205, in _cmd_batch  pool.submit(  File \u0026#34;/public/home/chengjing2/taozy/tools/cnvkit/cnvlib/parallel.py\u0026#34;, line 18, in submit  return SerialFuture(func(*args)) TypeError: batch_run_sample() takes from 16 to 17 positional arguments but 18 were given 最后发现该报错是版本问题，不要使用0.9.11.dev0版本来安装，使用0.9.10版本即可。\nwget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.10.tar.gz tar -xvf v0.9.10.tar.gz cd cnvkit-0.9.10/ pip install -e . 使用源码安装注意同时在环境下安装r和r包DNAcopy。\n参考资料  cnvkit github page ","date":"January 2, 2024","image":null,"permalink":"/post/2024-01-02_cnvkit_error/","title":"TypeError: batch_run_sample() takes from 16 to 17 positional arguments but 18 were given"},{"categories":["可视化"],"contents":"使用R绘制类内组间比较热图，这里所使用的R包plotool我目前还没上传。\n 模仿\u0026quot;Pan-cancer whole-genome comparison of primary and metastatic solid tumours\u0026quot;这篇文章的figure1d绘制热图。\n \n 该图是：比较多种癌症类型内，原发瘤和转移瘤之间的染色体臂的倍性（经过标准化），矫正后p值小于0.01的标记为显著*。\n写了一个自用的R包plotool，里面的函数如下：\n \n 示例数据 构建示例数据，根据示例数据绘制示例图。\n# 设置种子以便复现结果 set.seed(123)  samples \u0026lt;- paste0(\u0026#34;Sample\u0026#34;, 1:200)  # 生成列名为 feature1 到 feature10 的多个特征列 num_features \u0026lt;- 20 feature_data \u0026lt;- matrix(rnorm(length(samples) * num_features), ncol = num_features) colnames(feature_data) \u0026lt;- paste0(\u0026#34;feature\u0026#34;, 1:num_features)   # 创建数据框 data \u0026lt;- data.frame(  sample = paste0(\u0026#34;Sample\u0026#34;, 1:200), # 生成样本名称  type = sample(c(paste0(\u0026#34;class\u0026#34;,LETTERS[1:10])), 200, replace = TRUE), # 生成type数据  pair = sample(c(\u0026#34;primary\u0026#34;, \u0026#34;metastasis\u0026#34;), 200, replace = TRUE), # 生成pair数据  feature_data )  types \u0026lt;- unique(data$type)  # get table pair.df \u0026lt;- plotool::get_pair_table(  data = data,  type = types,  feature = colnames(feature_data),  q_val_cutoff = 0.5,  cal_fun = \u0026#34;mean\u0026#34; ) %\u0026gt;%  dplyr::mutate(  feature = factor(  x = feature,  levels = paste0(\u0026#34;feature\u0026#34;, 1:num_features)  )  ) q_val_cutoff这里为了方便展示，把矫正后的p值q_val_cutoff参数设为了0.5。\n绘图：上面是primary，下面是metastatic。\n# plot out \u0026lt;- plotool::pair_feature_heatmap(  df = pair.df,  feature = colnames(feature_data),  title = \u0026#34;Type average-normalized feature value\u0026#34;,  title.position = \u0026#34;top\u0026#34;,  xlab = \u0026#34;Features\u0026#34;  ) out \n \n get_pair_table 获得显著性标记，计算每组每个特征在热图中展示的数值，生成用于绘图的数据，\n  data：必须包含：sample，type，pair（比如原发和转移，反应和不反应等分组标签）和各种feature数值的宽数据，列名要对应上\n  type：类别名，字符串，比如这里的class\n  featurea：特征名，字符串，比如这里的feature123\n  q_val_cutoff = NULL：当q值没有设置时，使用p值的cutoff作为筛选的条件来标注显著性\n  pvalue_cutoff = 0.05：默认p值的cutoff来标注显著性，小于该值标记显著\n  cal_fun = c(\u0026ldquo;mean\u0026rdquo;, \u0026ldquo;median\u0026rdquo;, \u0026ldquo;sum\u0026rdquo;)：计算展示数据的方式，分别对应：均值，中位数，总和\n  pair_feature_heatmap 用于绘制热图。\n  df：get_pair_table的输出作为该函数的输入\n  feature：特征名字符串\n  tile_colors = c(\u0026rsquo;#000080\u0026rsquo;,\u0026rsquo;#009ACD\u0026rsquo;,\u0026rsquo;#B2DFEE\u0026rsquo;,\u0026rsquo;#FFFFFF\u0026rsquo;,\u0026rsquo;#FF758C\u0026rsquo;,\u0026rsquo;#FF007F\u0026rsquo;,\u0026rsquo;#A8152E\u0026rsquo;)：颜色设置\n  breaks = c(seq(-1, -0.1, by = 0.1), seq(0.1, 1, by = 0.1))：对应颜色的数值范围划分\n  title = \u0026ldquo;Title\u0026rdquo;：图注的标题\n  title.position = \u0026ldquo;top\u0026rdquo;：图注的位置\n  xlab = \u0026ldquo;feature\u0026rdquo;：横坐标名\n  参考资料：   Pan-cancer whole-genome comparison of primary and metastatic solid tumours的github文档\n  R包plotool还未公开\n ","date":"December 29, 2023","image":null,"permalink":"/post/2023-12-29_plot_pairheatmap/","title":"使用R绘制类内组间比较热图"},{"categories":["R"],"contents":"最简单的介绍R包开发。\n 1. 创建包 library(devtools) create_package(\u0026#34;~/path/to/yourpackage\u0026#34;) 2. 构建函数 在R文件夹下创建R函数文件，比如my_function.R ，并定义函数内容，写函数注释，注意写上@export\nRCopy code #\u0026#39; My Function #\u0026#39; #\u0026#39; Description of my function. #\u0026#39; #\u0026#39; @param x Input parameter. #\u0026#39; @return Output value. #\u0026#39; @export # NAMESPACE export automatally #\u0026#39; @examples #\u0026#39; my_function(5) my_function \u0026lt;- function(x) {  # Function implementation  # ... } 3. DESCIPTION文件中使用依赖包 DESCIPTION文件中使用依赖包\nImports: \tdplyr 其中管道操作符需要额外导入。\n4. 自动生成文档 library(roxygen2) roxygen2::roxygenize() 5. 构建并安装包 devtools::build() devtools::install() 6. 测试包 library(YourPackageName) ?my_function 7. 内置数据集  数据导入  在包目录下，创建新的文件夹data，把数据文件放在该文件夹下，比如 my_data.csv，导入的数据要使用支持的格式，比如RData和csv等格式。\n在man文件夹下，有多个内置数据情况下，构建名为my_datasets.R文件  #\u0026#39; My Datasets #\u0026#39; #\u0026#39; Description of the datasets included in the package. #\u0026#39; #\u0026#39; @examples #\u0026#39; # Example usage of the datasets #\u0026#39; ...NULL #\u0026#39; @name swgs_segment.ace #\u0026#39; @title swgs_segment.ace #\u0026#39; @description Description of the first dataset load(\u0026#34;data/swgs_segment.ace.RData\u0026#34;) #\u0026#39; @name GRCh38_head.d1.vd1.fa.fai #\u0026#39; @title GRCh38_head.d1.vd1.fa.fai #\u0026#39; @description Description of the second dataset load(\u0026#34;data/GRCh38_head.d1.vd1.fa.fai.RData\u0026#34;) 重新构建和安装  roxygen2::roxygenize() devtools::document() devtools::install() 使用  library(YourPackageName) data(GRCh38_head.d1.vd1.fa.fai) data(swgs_segment.ace) 可以直接使用data()来查看所有的内置数据。?GRCh38_head.d1.vd1.fa.fai查看数据说明\n参考资料：   Pan-cancer whole-genome comparison of primary and metastatic solid tumours的github文档\n  R包plotool还未公开\n ","date":"December 28, 2023","image":null,"permalink":"/post/2023-12-28_rpackage_devolopment/","title":"R包开发——简单介绍"},{"categories":["生信工具"],"contents":"介绍BICseq2的用法。\n 实际上总共有三大步骤，针对无匹配样本的情况：\n  Get the uniquely mapped reads from the bam file (you may use the modified samtools as provided here).\n  Use BICseq2-norm to remove the biases in the data.\n  Use BICseq2-seg to detect CNVs based on the normalized data.\n  有匹配样本的情况：\n  Get the uniquely mapped reads from the case and the control genome bam files, respectively\n  Normalize the case and control genome individually using BICseq2-norm\n  Detect CNV in the case genome based on the normalized data of the case genome and the conrol genome.\n  1. 安装 # 创建一个新的环境 conda create -n BICseq2 conda activate BICseq2  # 下载 conda install -c bioconda bicseq2-norm conda install -c bioconda bicseq2-seg 安装成功后，直接输入NBICseq-norm.pl或NBICseq-seg.pl会出现使用说明。\n2. 准备seq文件 使用官方提供的修改后的samtools，下载：\nwget https://compbio.med.harvard.edu/BIC-seq/BICseq2/samtools-0.1.7a_getUnique-0.1.3.tar.gz gunzip samtools-0.1.7a_getUnique-0.1.3.tar.gz tar -xvf samtools-0.1.7a_getUnique-0.1.3.tar.gz 注意，如果已经安装过samtools，使用修改后的samtools要写明新工具的路径。\n创建一个存放结果的文件夹seq_file，运行时在该路径下运行：\ncd /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/seq_file /home/tzy/tools/samtools-0.1.7a_getUnique-0.1.3/samtools view -U BWA,ERR174341.hg19.1X,N,N /home/data/ERR174341.hg19.1.bam 基本语法：\nsamtools view [options] \u0026lt;in.bam\u0026gt;|\u0026lt;in.sam\u0026gt; [region1 [...]]  -U ：字符串，通常是\u0026lt;Aligner,OutputPrefix,ChromNameReport?,StrandReport?\u0026gt; or \u0026lt;Aligner,OutputPrefix,ChromNameReport?,StrandReport?minLen,maxLen\u0026gt; ，比如这里的BWA,ERR174341.hg19,N,N，aligner为BWA，输出的前缀是ERR174341.hg19.1X，染色体名和链不会报告。  输出文件：\nERR174341.hg19.1Xchr10.seq ERR174341.hg19.1Xchr11.seq ERR174341.hg19.1Xchr12.seq ... 3. 拆分染色体 该工具针对单条染色体处理，对下载的ucsc.hg19.fasta进行拆分，创建chromosome文件夹存储拆分后的fasta文件：\ncd /home/data/sde/tzy/NA12878/ucsc.hg19/chromosome faidx -x /home/data/sde/tzy/NA12878/ucsc.hg19/ucsc.hg19.fasta 4. 下载mappability文件 # download from https://compbio.med.harvard.edu/BIC-seq/ # Mappability files # Human hg19 100mer wget https://compbio.med.harvard.edu/BIC-seq/Mappability/hg19.CRG.100bp.tar.gz gunzip hg19.CRG.100bp.tar.gz tar -xvf hg19.CRG.100bp.tar 目录下文件：\nhg19.CRC.100mer.chr10.txt hg19.CRC.100mer.chr11.txt hg19.CRC.100mer.chr12.txt ... 5. 生成configfile 生成一个faname文件【这里我只分析1-22条染色体】：\nchr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr1 chr20 chr21 chr22 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 生成configFile，configFile.txt为输出的文件名：\nfile=/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname configFile=/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/configFile.txt  # 检查file文件是否存在 if [ ! -f \u0026#34;$file\u0026#34; ]; then  echo \u0026#34;File \u0026#39;$file\u0026#39; does not exist.\u0026#34;  exit 1 fi  # 生成config文件 echo -e \u0026#34;chromName\\tfaFile\\tMapFile\\treadPosFile\\tbinFileNorm\u0026#34; \u0026gt; \u0026#34;$configFile\u0026#34;  # 读取file文件中的每行，并生成config文件 while IFS= read -r prefix || [ -n \u0026#34;$prefix\u0026#34; ]; do  chrom=\u0026#34;$prefix\u0026#34;  faFile=\u0026#34;/home/data/sde/tzy/NA12878/ucsc.hg19/chromosome/${chrom}.fasta\u0026#34;  mapFile=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/hg19CRG.100bp/hg19.CRC.100mer.${chrom}.txt\u0026#34;  readPosFile=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/seq_file/ERR174341.hg19.1X${chrom}.seq\u0026#34;  binFileNorm=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/binFileNorm/${chrom}.norm.bin\u0026#34;   echo -e \u0026#34;${chrom}\\t${faFile}\\t${mapFile}\\t${readPosFile}\\t${binFileNorm}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$configFile\u0026#34; done \u0026lt; \u0026#34;$file\u0026#34;  echo \u0026#34;Config file generated: $configFile\u0026#34; 生成的configFile文件：\nchromName faFile MapFile readPosFile binFileNorm chr10 /home/data/sde/tzy/NA12878/ucsc.hg19/chromosome/chr10.fasta /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/hg19CRG.100bp/hg19.CRC.100mer.chr10.txt /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/seq_file/ERR174341.hg19.1Xchr10.seq /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/binFileNorm/chr10.norm.bin ... 官方文档对config文件的要求：\n \n 6. normalize start=$(date +%s) # 记录脚本开始时间  cd /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script  NBICseq-norm.pl -l 100 -s 100 --tmp $file/tmp configFile.txt BICseq2-norm.txt  end=$(date +%s) # 记录脚本结束时间 runtime=$((end - start)) # 计算脚本运行时间  echo \u0026#34;脚本运行时间为：$runtime 秒\u0026#34; 参数：\n -l=\u0026lt;int\u0026gt;: read length  -s=\u0026lt;int\u0026gt;: fragment size  -p=\u0026lt;float\u0026gt;: a subsample percentage: default 0.0002.  -b=\u0026lt;int\u0026gt;: bin the expected and observed as \u0026lt;int\u0026gt; bp bins; Default 100.  --gc_bin: if specified, report the GC-content in the bins  --NoMapBin: if specified, do NOT bin the reads according to the mappability  --bin_only: only bin the reads without normalization  --fig=\u0026lt;string\u0026gt;: plot the read count VS GC figure in the specified file (in pdf format)  --title=\u0026lt;string\u0026gt;: title of the figure  --tmp=\u0026lt;string\u0026gt;: the tmp directory; normalize的输出BICseq2-norm.txt：\n#FragmentLength 100 #ReadLen 100 #NumberOfExtendedBp 5 #Parameter estimates from the negative binomial model # 265 lines estimate std zvalue pvalue (Intercept) -4.08729958461365 0.0814275642617771 -50.1955280336472 0 G_Start1 -0.0786228026868843 0.0396487170352376 -1.98298478654451 0.0473691289124137 C_Start1 -0.102595852102073 0.0406397501756252 -2.52451975365752 0.0115856493521211 ... 7. 提取拷贝数 生成该步骤需要的config文件configFile2.txt，提取的是configFile.txt的第一列（染色体名称）和最后一列（normalize步骤生成的norm.bin文件），并使用分隔符\\t分开：\nawk -v OFS=\u0026#39;\\t\u0026#39; \u0026#39;{print $1, $NF}\u0026#39; configFile.txt \u0026gt; configFile2.txt  cd /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script NBICseq-seg.pl --bootstrap configFile2.txt CNV.txt 语法：\nBICseq2-seg.pl [options] \u0026lt;configFile\u0026gt; \u0026lt;output\u0026gt;  \u0026lt;configFile\u0026gt; stores the necessary information for BICseq2-seg to detect CNV \u0026lt;output\u0026gt; stores the final CNV detection results. 官方文档对config文件的要求：\n \n 附：第5步的复杂版 针对不同样本想要call的染色体不同的情况：根据hg19CRG.100bp目录下的文件名，来自动生成configfile：\n#!/bin/bash  # 设置file文件名和config文件名 # 删除之前存在的文件 if [ -f \u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname\u0026#34; ]; then  rm /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname fi  if [ -f \u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname.txt\u0026#34; ]; then  rm /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname.txt fi  # 提取染色体名 ls /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/hg19CRG.100bp | grep hg19.CRC.100mer* \u0026gt; /home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname.txt filepre=/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname.txt  # 提取每一行的 chr[num] while IFS= read -r line || [ -n \u0026#34;$line\u0026#34; ]; do # chr=$(echo \u0026#34;$line\u0026#34; | sed \u0026#39;s/.*\\.chr\\([0-9]*\\)\\.txt/\\1/\u0026#39;)  chr=$(echo \u0026#34;$line\u0026#34; | sed -n \u0026#39;s/.*\\.chr\\([1-9][0-9]*\\)\\.txt/\\1/p\u0026#39;)  echo \u0026#34;Extracted chr: $chr\u0026#34;  echo -e \u0026#34;chr$chr\u0026#34; \u0026gt;\u0026gt; \u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname\u0026#34; done \u0026lt; \u0026#34;$filepre\u0026#34;  file=/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/faname configFile=/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/configFile.txt  # 检查file文件是否存在 if [ ! -f \u0026#34;$file\u0026#34; ]; then  echo \u0026#34;File \u0026#39;$file\u0026#39; does not exist.\u0026#34;  exit 1 fi  # 生成config文件 echo -e \u0026#34;chromName\\tfaFile\\tMapFile\\treadPosFile\\tbinFileNorm\u0026#34; \u0026gt; \u0026#34;$configFile\u0026#34;  # 读取file文件中的每行，并生成config文件 while IFS= read -r prefix || [ -n \u0026#34;$prefix\u0026#34; ]; do  chrom=\u0026#34;$prefix\u0026#34;  faFile=\u0026#34;/home/data/sde/tzy/NA12878/ucsc.hg19/chromosome/${chrom}.fasta\u0026#34;  mapFile=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/script/hg19CRG.100bp/hg19.CRC.100mer.${chrom}.txt\u0026#34;  readPosFile=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/seq_file/ERR174341.hg19.1X${chrom}.seq\u0026#34;  binFileNorm=\u0026#34;/home/data/sde/tzy/NA12878/raw_data/ERR174341/BICseq2/binFileNorm/${chrom}.norm.bin\u0026#34;   echo -e \u0026#34;${chrom}\\t${faFile}\\t${mapFile}\\t${readPosFile}\\t${binFileNorm}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$configFile\u0026#34; done \u0026lt; \u0026#34;$file\u0026#34;  echo \u0026#34;Config file generated: $configFile\u0026#34; 这里hg19CRG.100bp目录下，我把以下几个文件单独放置了一个文件夹drop，因为我这里分析只需要1-22条染色体：\nhg19.CRC.100mer.chrM.txt hg19.CRC.100mer.chrX.txt hg19.CRC.100mer.chrY.txt 参考资料：  BICseq2官方文档 ","date":"December 8, 2023","image":null,"permalink":"/post/2023-12-08_%E6%B5%85%E6%B5%8B%E5%BA%8F%E6%8B%B7%E8%B4%9D%E6%95%B0%E5%8F%98%E5%BC%82%E5%B7%A5%E5%85%B7/","title":"使用BICseq2提取拷贝数变异"},{"categories":["生信工具"],"contents":"参考一篇做的比较全面的结构变异工具比较类文章中使用到的标准数据的获取和提取。\n 该研究中使用到的人类全基因组测序数据如下：\n \n 1. 原始数据下载 文章中使用到的数据分为4种：\n  NA12878：data1234都是短读数据，通过Illumina HiSeq得到的；PicBiodata123都是长读数据，通过PacBio RS得到的。\n  NA12878的父母本：为了探究结构变异检测孟德尔遗传错误，使用了NA12891父本，NA12892母本。\n  其他来源数据：HG00514，中国汉族人；HG002/NA24385德系犹太人之子。\n  模拟数据\n  以上三种类型的真实数据都是同时具有二代测序和三代测序结果的。\n https://www.coriell.org/1/NIGMS/Collections/NIST-Reference-Materials\n 1.0 SRA Toolkit下载安装 在官方文档中找到合适的版本：https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit，根据官方教程下载安装和使用：\nwget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.0.7/sratoolkit.3.0.7-ubuntu64.tar.gz tar -vxzf sratoolkit.3.0.7-ubuntu64 vi ~/.bashrc export PATH=$PATH:/home/tzy/tools/sratoolkit.3.0.7-ubuntu64/bin source ~/.bashrc 检查一下是否能找到which fastq-dump，并测试一下是否可用：\nfastq-dump --stdout -X 2 SRR390728 显示如下则可用：\nRead 2 spots for SRR390728 Written 2 spots for SRR390728 @SRR390728.1 1 length=72 CATTCTTCACGTAGTTCTCGAGCCTTGGTTTTCAGCGATGGAGAATGACTTTGACAAGCTGAGAGAAGNTNC +SRR390728.1 1 length=72 ;;;;;;;;;;;;;;;;;;;;;;;;;;;9;;665142;;;;;;;;;;;;;;;;;;;;;;;;;;;;;96\u0026amp;\u0026amp;\u0026amp;\u0026amp;( @SRR390728.2 2 length=72 AAGTAGGTCTCGTCTGTGTTTTCTACGAGCTTGTGTTCCAGCTGACCCACTCCCTGGGTGGGGGGACTGGGT +SRR390728.2 2 length=72 ;;;;;;;;;;;;;;;;;4;;;;3;393.1+4\u0026amp;\u0026amp;5\u0026amp;\u0026amp;;;;;;;;;;;;;;;;;;;;;;\u0026lt;9;\u0026lt;;;;;;464262 1.1 NA12878 data1 1.1.1 下载 在NCBI上搜索Accession number，使用ERP001775搜索得到32个不同的结果，ERP是study number，表示的是一个特定目的的研究课题，可以包含多个研究机构和研究类型等。以第一个检索结果和第二个检索结果对比，Accession: ERX150487和Accession: ERX150486，ERX表示一个实验记载的实验设计（Design），实验平台（Platform）和结果处理 （processing）三部分信息，一个实验信息可以同时包含多个结果集（run），以DRR，SRR，ERR 开头。NCBI编号含义可以查看这篇博客。这里我下载的是ERP001775的ERR174340数据：\n \n 需要使用SRA Toolkit来下载超过5G的数据，直接根据ERR号下载即可，默认的download limit is 20GB，设置到40G：\nprefetch ERR174340 --max-size 40G 下载后是一个名为ERR1743430的文件夹，里面是ERR174340.sra文件，需要转化为fastq文件，直接对文件夹转化即可：\nfasterq-dump ./ERR174340 得到如下两个文件：\nERR174341_1.fastq ERR174341_2.fastq 1.1.2 下载参考基因组 文章中使用的是GRCh37d5，和hg19的区别在于前者有额外的诱饵序列（contain 41.8Mb of extra decoy sequence comprising of 61 sequences）。另外如果使用的是DRAGEN可以下载Illumina DRAGEN Multigenome Reference — GRCh37d5，下载下载hg19的方法：\n访问ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/，登陆名和密码都默认：\nlocation: ftp.broadinstitute.org/bundle username: gsapubftp-anonymous password: 下载了这些\nhttps://blog-1310600458.cos.ap-shanghai.myqcloud.com/Macdo2023-12-02%2018.41.44.png\n下载的基因组需要使用bwa index来构建索引方便快速查找:\ngunzip ucsc.hg19.fasta.gz time bwa index -a bwtsw -p ucsc.hg19 -b 500000000 /home/data/sdb/tzy/NA12878/hg19/ucsc.hg19.fasta 新生成文件如下：\nucsc.hg19.ann ucsc.hg19.sa ucsc.hg19.bwt ucsc.hg19.amb ucsc.hg19.pac 不过我不准备使用hg19，准备用已经做好索引的gdc-hg38来继续分析。使用参考基因组做bwa的时候，注意cd到ref所在的位置：\npatha=/home/data/sdb/tzy/NA12878/raw_data/ERR174341 name=ERR174341 ref=/home/data/sdb/tzy/NA12878/ucsc.hg19 echo \u0026#34;start bwa in $name\u0026#34; cd $ref  bwa mem -M -t 20 -R \u0026#34;@RG\\tID:$name\\tSM:$name\\tLB:WGS\\tPL:Illumina\u0026#34; ucsc.hg19 ${patha}/${name}_fp_1.fq.gz ${patha}/${name}_fp_2.fq.gz | \\ samtools sort -@ 5 -m 1G -o ${patha}/$name.hg19.bam  end=`date +%s` time=`echo $start $end | awk \u0026#39;{print $2-$1}\u0026#39;` echo \u0026#34;over $time\u0026#34; 1.1.3 concatenate fastq 该文章的TableS3中，NA12878的Genome coverage为31X，我下载了ERR174340（NA12878的ERP001775的一个实验测序结果），无论是使用samtools depth还是使用samtools coverage检测平均深度，得到的都是结果都是11X左右，以人类基因组～3Gb的大小来看，原始fastq文件为30G左右，11X也是合理的，通过阅读文献 发现，这里应该是对fastq文件做了合并才达到更深的coverage，该文献中描述如下：\nH. sapiens - ERP001775 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR174/ERR174324/ERR174324_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR174/ERR174325/ERR174325_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR174/ERR174324/ERR174324_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR174/ERR174325/ERR174325_2.fastq.gz The first two files were concatenated and the last two files were concatenated to obtain a 28x coverage paired-end dataset. 如果需要深度测序的结果可以参考这种方式。这可能也是为什么在NCBI上搜索ERP001775，它的Sample Description写的是a sequence depth of more than 200-fold，描述的是多个实验测得的总深度。\n2. 参考结构变异下载 文章方法中使用到的是参考结构变异，综合了：\n  GRCh37在2016-05-15版本得到的NA12878变异数据\n  通过长读得到的NA12878的PacBio SV\n  2.1 DGV variant data 在DGV官网上下载，找到文章中提到的DGV variant 2016-5-15版本。【这里特别注意，如果直接点开网页版下载，可能会出现加载不全的情况，最好直接在服务器上下载，注意服务器科学上网】：\nwget http://dgv.tcag.ca/dgv/docs/GRCh37_hg19_variants_2016-05-15.txt 筛选出NA12878的部分：\ncat GRCh37_hg19_variants_2016-05-15.txt | grep NA12878 \u0026gt; GRCh37_hg19_variants_2016-05-15_NA12878.txt 2.2 PacBio SV 文章中PacBio SV的reference部分提供的是文献路径：\nPendleton M, Sebra R, Pang AW, Ummat A, Franzen O, Rausch T, et al. Assembly and diploid architecture of an individual human genome via single-molecule technologies. Nat Methods. 2015;12:780–6.\n下载Supplementary Table 5：Insertion and deletion SVs with phasing (XLSX 1079 kb)\n3. 模拟数据 3.1 varsim下载安装 文中使用varsim来模拟，官方网址：https://bioinform.github.io/varsim/\n安装：\ngit clone https://github.com/bioinform/varsim.git cd varsim ./build.sh 注意clone下来的工具版本比较老了，修改一下：\nvi ./build.sh 注释掉：\n#MAVEN_DIR=${OPT_DIR}/apache-maven-3.5.4 MAVEN_DIR=${OPT_DIR}/apache-maven-3.8.8 #wget --no-check-certificate -O- http://mirrors.sonic.net/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz | tar zxvf - wget --no-check-certificate -O- http://mirrors.sonic.net/apache/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz | tar zxvf - 注意3.9.5版本太新可能不兼容，注意一下。\n3.2 varsim模拟 文章附件中有详细的参数：\npython varsim.py --vc_in_vcf $HOME/tool/VarSim/All.vcf --sv_insert_seq $HOME/tool/VarSim/insert_seq.txt --sv_dgv $HOME/tool/VarSim/GRCh37_hg19_supportingvariants_2013-07-23.txt --reference hs37d5.fa --id Sim-A --read_length 125 --mean_fragment_size 500 --sd_fragment_size 100 --vc_num_snp 2700000 --vc_num_ins 270000 --vc_num_del 270000 --sv_num_ins 3000 --sv_num_del 4000 --sv_num_dup 2000 --sv_num_inv 500 --sv_percent_novel 0.2 --vc_percent_novel 0.01 --vc_min_length_lim 0 --vc_max_length_lim 29 --sv_min_length_lim 30 --sv_max_length_lim 1000000 --nlanes 1 --total_coverage 30 --simulator_executable $HOME/tool/art_bin_ChocolateCherryCake/art_illumina --out_dir out --log_dir log --simulator art --profile_1 $HOME/tool/art_bin_ChocolateCherryCake/Illumina_profiles/HiSeq2500L1 25R1.txt --profile_2 $HOME/tool/art_bin_ChocolateCherryCake/Illumina_profiles/HiSeq2500L1 25R2.txt ↩ 参考资料：  Comprehensive evaluation of structural variation detection algorithms for whole genome sequencing ","date":"December 6, 2023","image":null,"permalink":"/post/2023-11-14_na12878_all/","title":"NA12878多种来源数据下载[参考Kamatani 2019 Genome Biology]"},{"categories":["生信工具"],"contents":"使用LiftOver转换不同版本参考基因组坐标。\n 应用场景举例：当下载的数据在处理时所用的参考基因组为hg19，而自己需要的是hg38为参考基因组的数据，一方面从原始数据重新获取结果费时费力，另一方面，原始数据可能不可得。\n1. 在网页中使用 可以使用网页版的LiftOver，网页版注意数据格式为chrom:start-end，例如：\nchr1:7570073-7571526 LiftOver网址：https://genome.ucsc.edu/cgi-bin/hgLiftOver，需要选择需要新旧转化的参考基因组。\n \n 如果使用R处理了数据，导出时注意去除引号，导出为txt时可以参考以下代码：\nNA12878cn \u0026lt;- read.csv2(\u0026#34;~/mmc4.txt\u0026#34;, sep = \u0026#34;\\t\u0026#34;) NA12878cn$Chromosome \u0026lt;- paste0(\u0026#34;chr\u0026#34;,NA12878cn$Chromosome) NA12878cn$start \u0026lt;- paste0(NA12878cn$Chromosome,sep=\u0026#34;:\u0026#34;,NA12878cn$Start.position) NA12878cn$start \u0026lt;- paste0(NA12878cn$start,sep=\u0026#34;-\u0026#34;,NA12878cn$End.position) write.table(  NA12878cn$start, # object  \u0026#34;~/NA12878cn.txt\u0026#34;, # path to save  sep = \u0026#34;\\t\u0026#34;,  row.names = FALSE,  quote = FALSE) 2. 在R中使用 2.1 使用R包liftOver  安装  if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly = TRUE))  install.packages(\u0026#34;BiocManager\u0026#34;)  BiocManager::install(\u0026#34;liftOver\u0026#34;)  下载参考基因组  从哪个基因组转出就到哪个目录下找chain文件，http://hgdownload.soe.ucsc.edu/downloads.html#liftover\n例如：我需要把坐标从hg19的转化成hg38，下载文件：\nwget --timestamping \u0026#39;ftp://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz\u0026#39; -O hg19ToHg38.over.chain.gz gunzip hg19ToHg38.over.chain.gz  坐标转换  library(liftOver) NA12878cn \u0026lt;- read.csv2(\u0026#34;~/mmc4.txt\u0026#34;, sep = \u0026#34;\\t\u0026#34;) chain \u0026lt;- import.chain(\u0026#39;~/hg19ToHg38.over.chain\u0026#39;) # From data.frame to GRanges hg38cn \u0026lt;- as(NA12878cn, \u0026#34;GRanges\u0026#34;) hg19.gr \u0026lt;- liftOver(hg38cn, chain) out \u0026lt;- invertStrand(hg19.gr) %\u0026gt;% as.data.frame() 读取的数据NA12878cn需要转化为GRanges格式。\n完成。\n参考资料：  R 学习笔记：转换不同版本的基因组坐标 ","date":"November 14, 2023","image":null,"permalink":"/post/2023-11-14_hg38tohg19/","title":"使用LiftOver转换不同版本参考基因组坐标"},{"categories":["生信工具"],"contents":"介绍动态UI的使用方法。\n 基本语法 以选择框为例，介绍基本语法。\n  主要在serve中写入三个部分：reactive， observeEvent，renderXXX（不同的组件函数不同，注意查找对应组件在ui和serve中的函数）\n  reactive\n  注意响应式在被其他的部分引用时，要加上()，比如响应式x：x \u0026lt;- reactive(input$number)，out \u0026lt;- renderText({x()})\n observeEvent  用于在 Shiny 应用中响应特定输入的事件。它的主要作用是监视和响应用户界面中的交互动作，当指定的输入发生变化时，可以自动触发自定义的操作。常用于按钮点击、选择框的选择、输入框的输入等等。通过它，您可以动态更新 Shiny 应用的界面和功能，实现与用户的互动。\n该函数通常由两个主要参数组成：\n  eventExpr: 这是要监视的事件表达式。这是必选项。通常，这是一个 Shiny 输入元素的响应式对象，如输入框的值、按钮的点击事件等。当 eventExpr 的值发生变化时，observeEvent 中定义的操作将被触发。可以引用reactive对象。\n  handlerExpr: 这是要执行的操作。当 eventExpr 的值发生变化时，handlerExpr 中定义的操作将被执行。\n   renderXXX  这部分根据输入的改变，决定输出是什么。\n在server中，input中的内容可以任意被引用，凡是有id的组件都可以用这个格式获取内容：input$id。\n文本 ui \u0026lt;- fluidPage(  textOutput(outputId = \u0026#34;text\u0026#34;) ) server \u0026lt;- function(input, output, session) {  output$text \u0026lt;- renderText({  \u0026#34;Hello friend!\u0026#34;  }) } 表格 注意到其中在ui中应用到，server中对应到renderDataTable，\nlibrary(shiny) library(shinydashboard) library(shinyWidgets)  # 创建一个简单的数据框 data \u0026lt;- data.frame(  Name = c(\u0026#34;John\u0026#34;, \u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;David\u0026#34;),  Age = c(25, 30, 35, 28),  Score = c(95, 88, 92, 89) )  ui \u0026lt;- fluidPage(  dataTableOutput(\u0026#34;my_table\u0026#34;) )  server \u0026lt;- function(input, output, session) {  # 使用renderDataTable来渲染数据表  output$my_table \u0026lt;- renderDataTable(  data,  options = list(  pageLength = 10,  searching = FALSE,  dom = \u0026#39;lrtip\u0026#39;  ),   ) }  shinyApp(ui, server) 这里需要注意一些参数：\n searching = FALSE，设置图标下方的搜索框不显示   \n  dom = \u0026rsquo;lrtip\u0026rsquo;，其中  l - length changing input control f - filtering input t - The table! i - Table information summary p - pagination control r - processing display element 如果只想要表格本身，可以设置为dom = 't'。\n多选项框内容 满足条件如下：\n  能够下载展示出来的表格（最好是展示一部分，但是下载全部的表格）：通过设置按钮进行指定内容下载\n  多个选择框选择后输出的结果是交集关系：获取所有的选择条件后，对所有的选择条件进行合并\n  可以自由的选择，而不是必须在所有的选择框中进行选择：通过设置条件选项，当条件为空时，展示空表格或全部表格。\n  以下用到字符串转化为变量：\n# 创建一个名为 \u0026#34;my_variable\u0026#34; 的字符串 my_variable \u0026lt;- 42  variable_name \u0026lt;- \u0026#34;my_variable\u0026#34;  # 将字符串转换为变量名 eval(parse(text = variable_name)) 举例：\n \n 以下是示例代码，通过直接运行可以直观的感受一下：\nlibrary(shiny) library(shinydashboard) library(shinyWidgets) library(tidyverse)  # 创建一个简单的数据框 data \u0026lt;- data.frame(  Name = c(\u0026#34;John\u0026#34;, \u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;David\u0026#34;),  Age = c(25, 30, 35, 28),  Score = c(95, 88, 92, 89) ) empty_data \u0026lt;- data.frame(matrix(\u0026#34;\u0026#34;, ncol = length(data))) %\u0026gt;% .[-1,] # 创建空数据框，和目标数据拥有同样的列名，方便多条件选择后组合数据 colnames(empty_data) \u0026lt;- colnames(data)  ui \u0026lt;- fluidPage(  sidebarLayout(  sidebarPanel(  width = 3,  shinyWidgets::virtualSelectInput(  inputId = \u0026#34;name\u0026#34;,  label = \u0026#34;Name :\u0026#34;,  choices = list(  \u0026#34;All\u0026#34; = data$Name  ),  selectize = FALSE,  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  ),   shinyWidgets::virtualSelectInput(  inputId = \u0026#34;age\u0026#34;,  label = \u0026#34;Age :\u0026#34;,  choices = list(  \u0026#34;All\u0026#34; = data$Age  ),  selectize = FALSE,  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  ),   shinyWidgets::virtualSelectInput(  inputId = \u0026#34;score\u0026#34;,  label = \u0026#34;Score :\u0026#34;,  choices = list(  \u0026#34;All\u0026#34; = data$Score  ),  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  )),  mainPanel(  width = 9,  dataTableOutput(\u0026#34;my_table\u0026#34;)) ))    decide_select \u0026lt;- function(choice, col){  if (!is.null(choice) \u0026amp;\u0026amp; length(choice) \u0026gt; 0 \u0026amp;\u0026amp; choice != \u0026#34;\u0026#34;) {  # choice.all \u0026lt;- paste0(choice[choice != \u0026#34;\u0026#34;] %\u0026gt;% as.character(), collapse = \u0026#34;,\u0026#34;)  choice.all \u0026lt;- paste0(choice[choice != \u0026#34;\u0026#34;] %\u0026gt;% as.character(), collapse = \u0026#34;\u0026#39;, \u0026#39;\u0026#34;)  choice.all \u0026lt;- paste0(\u0026#34;\u0026#39;\u0026#34;, choice.all, \u0026#34;\u0026#39;\u0026#34;)  quote \u0026lt;- paste0(col, \u0026#34; %in% c(\u0026#34;, substitute(choice.all), \u0026#34;)\u0026#34;)  quote  }else{  select_name \u0026lt;- NULL  select_name  } }  server \u0026lt;- function(input, output, session) {  dataset_selection \u0026lt;- reactive({  dt.name \u0026lt;- decide_select(  choice = input$name,  col = \u0026#34;Name\u0026#34;  )  # print(dt.name)   dt.score \u0026lt;- decide_select(  choice = input$score,  col = \u0026#34;Score\u0026#34;  )  # print(dt.score)   dt.age \u0026lt;- decide_select(  choice = input$age,  col = \u0026#34;Age\u0026#34;  )  # print(dt.age)   quote \u0026lt;- c(dt.name, dt.score, dt.age)  print(quote)  quote.all \u0026lt;- paste(quote[quote != \u0026#34;\u0026#34;], sep = \u0026#34; \u0026amp; \u0026#34;)  # print(quote.all)   if (!is.null(quote.all) \u0026amp;\u0026amp; length(quote.all) \u0026gt; 0 \u0026amp;\u0026amp; quote.all != \u0026#34;\u0026#34;){  # out \u0026lt;- sprintf(\u0026#34;dplyr::filter(data, %s)\u0026#34;, quote.all)  # cat(\u0026#34;last: \u0026#34;,out)  out \u0026lt;- dplyr::filter(data, eval(parse(text = quote.all)))  cat(quote.all)  out  }else {  empty_data # return a dataframe include nothing  } })   output$my_table \u0026lt;- renderDataTable(  dataset_selection(),  options = list(  pageLength = 10,  dom = \u0026#39;frtip\u0026#39;,  scrollX = TRUE  )  ) }  shinyApp(ui, server) 如果是要使用mysql语句进行多条件选择，reactive的内容可以通过以下代码来完成：\nselect_name\u0026lt;-paste0(\u0026#34;\u0026#39;\u0026#34;,input$name,\u0026#34;\u0026#39;\u0026#34;,collapse=\u0026#34;, \u0026#34;)# 选择框里的选项，是根据列的内容选行 query.name\u0026lt;-sprintf(\u0026#34;`name` IN (%s)\u0026#34;,selected_site)# 填充进SQL条件中 cat(\u0026#34;Generated query:\u0026#34;,query.name,\u0026#34;\\n\u0026#34;)","date":"November 2, 2023","image":null,"permalink":"/post/2023-10-23_shiny3/","title":"shiny【3】——动态"},{"categories":["生信工具"],"contents":"介绍如何按照条件合并两列为一列。\n目标：合并var1和var2两列为var列，同时满足条件：\n  仅当var1为空，或var2为空时合并\n  当两列都有内容时使用or字符串连接两列内容\n  将合并内容生成新列名为”var“\n   \n data \u0026lt;- dplyr::mutate(  data.raw,  var = case_when(  var1 == \u0026#34;\u0026#34; ~ as.character(var2),  var2 == \u0026#34;\u0026#34; ~ as.character(var1),  TRUE ~ paste(var1, var2, sep = \u0026#34; or \u0026#34;)  )) 如何没有条件限制，可以直接使用tidyr::unite(Full_Name, First_Name, Last_Name, sep = \u0026quot; and \u0026ldquo;)来合并，full name是新定义的列。\n字符串中大小写不统一，想要将将字符串变量的首字母改为大写，其余字母改为小写可使用：\nstr_to_title(tolower(variable)) ","date":"October 31, 2023","image":null,"permalink":"/post/2023-10-23_r%E6%8C%89%E6%9D%A1%E4%BB%B6%E5%90%88%E5%B9%B6%E6%95%B0%E6%8D%AE%E5%88%97/","title":"R【合并列】——合并两列为一列"},{"categories":["生信工具"],"contents":"介绍如何使用shiny创建网页。\n ui构建HTML交互，server来render输出和对输入作出反馈。\n  新建一个shiny项目，在app.R中设定用户函数和开发者函数，并输出可视化结果\n  增加UI控制\n    fluidPage()：设定基本的可视化的页面的框架\n  selectInput()：输入控制，用于用户和app进行交互的\n  verbatimTextOutput()：输出控制，显示代码，告诉Shiny在哪里放置渲染输出\n  tableOutput()：输出控制，显示表格\n  增加习惯  我们将通过在服务器函数中定义输出来实现它们。\n render{Type}：renderPrint和renderTable，用来产生特定的输出（比如文字，表格，图片等等），renderPrint通常和verbatimTextOutput()配对使用，以显示带有固定宽度(逐字)文本的统计摘要，renderTable通常和tableOutput()配合使用，来展示表格中的输入数据。  根据我的使用习惯记录一下基本步骤。新建一个shiny项目，在app.R中写入各种参数。\n1. 选主题 在shiny扩展包中有很多主题可以选择，我选择的是shinydashboard，接下来的使用和修改也是在这个基础上进行的。主要是ui，serve和shinyAPP这三个部分。\n2. 搭框架 页面需要有基本的布局，shinydashboard主题中我主要使用到的有两块：\n  侧边栏\n  页面\n  其中页面中有两个重要的部分：\n  页面布局\n  各种类型的页面元素\n  2.1 侧边栏 shinydashboard中为例：\nlibrary(shinydashboard)  ui \u0026lt;- dashboardPage(  skin = \u0026#34;purple\u0026#34;, # 设置主题颜色   dashboardHeader(title = \u0026#34;database\u0026#34;), # 设置网页标题   dashboardSidebar(  sidebarMenu(  menuItem(\u0026#34;sidebar1\u0026#34;, tabName = \u0026#34;sidebar1\u0026#34;, icon = icon(\u0026#34;dashboard\u0026#34;))  )  ), # 设置主题侧边栏  # 设置网页内容  dashboardBody(  tags$head(tags$style(HTML(\u0026#39;  .main-header .logo {  font-family: \u0026#34;Georgia\u0026#34;, Times, \u0026#34;Times New Roman\u0026#34;, serif;  font-weight: bold;  font-size: 20px;  }  \u0026#39;))), # 更改网页标题的字体和大小   tabItems(  # 侧边栏页面内容  tabItems(  tabName = \u0026#34;sidebar1\u0026#34;,  column(  width = 8,  box(  title = \u0026#34;siderbar title\u0026#34;  )  )  )  )  ) )  server \u0026lt;- function(input, output, session) {  output$files \u0026lt;- renderTable(input$upload) }  shinyApp(ui, server)   dashboardHeader(title = \u0026ldquo;title name\u0026rdquo;) 设置标题。\n  dashboardSidebar是布局侧边栏，在其中使用menuItem设置具体的分页面。\n   dashboardSidebar(  sidebarMenu(  menuItem(\u0026#34;Page1\u0026#34;, tabName = \u0026#34;Page1\u0026#34;, icon = icon(\u0026#34;home\u0026#34;)),  menuItem(\u0026#34;Page2\u0026#34;, tabName = \u0026#34;Page2\u0026#34;, icon = icon(\u0026#34;fa-solid fa-database\u0026#34;)),  menuItem(\u0026#34;Page3\u0026#34;, tabName = \u0026#34;Page3\u0026#34;, icon = icon(\u0026#34;table\u0026#34;))  ))  dashboardBody可以指定每个分页面内容的具体布局和组分。  2.2 页面布局 页面中可以填充各种各样的元素，这些元素都要按照一定的布局去排布，布局有各种形式，下面逐一介绍。**将多个元素使用特定的函数组合成一个“单个元素”，该“单个元素”具有自己的属性和面板功能。**使用布局功能将面板和元素组织到一个布局中。添加元素作为布局函数的参数。\n不同的布局可以嵌套使用。\n2.2.0 直接布局  \n 代码规则：\nfluidPage(  fluidRow(  column(4,  ...  ),  column(8,  ...  )  ),  fluidRow(  column(6,  ...  ),  column(6,  ...  )  ) ) ...处放置各种组件。\n2.2.1 fluidRow布局  \n 一个网址可能不止一个页面，在当前主题（shinydashboard）中，使用dashboardPage替代fluidPage设置页面，tabItems负责设置每个子页面，单个页面上使用fluidRow对进行排布，分成上下布局板块。每个元素之间用,隔开，比如box(),infoBox()等等。\ncolumn函数是在UI中设置列，其中参数：\n  width：设置元素的宽度，取值范围在1～12之间。\n  offset：可以默认，也可以设置数值，从上一列的末尾偏移此列的列数。本质上是调整元素的位置。\n  2.2.2 flowLayout布局  \n 2.2.3 sidebarLayout布局  \n 2.2.4 splitLayout布局  \n 2.2.5 verticalLayout布局  \n 2.2.6 tabsetPanel布局  \n  图标 通常在该网页上搜索自己想要的图标：https://fontawesome.com/icons，在可使用icon的地方使用语法：icon = icon(\u0026quot;fa-solid fa-database\u0026quot;)导入图标。\n搜索框 shinyWidgets包提供了几个非常好看的搜索框。\ninstall.packages(\u0026#34;shinyWidgets\u0026#34;) 该包提供多个搜索组件：\n \n shinyWidgets::searchInput(  inputId = \u0026#34;id\u0026#34;,  label = \u0026#34;Enter your search :\u0026#34;,  placeholder = \u0026#34;This is a placeholder\u0026#34;,  btnSearch = icon(\u0026#34;search\u0026#34;),  btnReset = icon(\u0026#34;remove\u0026#34;),  width = \u0026#34;100%\u0026#34;  ) 修改标题字体 注意，字体样式的定义要放在ui的page函数里面，引用自定义的字体样式时，直接使用自定义的名即可，比如对Title设置字体加粗，定义的样式名为my-title，用法如下：\nui \u0026lt;- fluidPage(  tags$head(  tags$style(HTML(\u0026#39;  /* 自定义标题字体样式 */  .my-title {  font-family: \u0026#34;Helvetica\u0026#34;, sans-serif; /* 修改字体 */  font-size: 24px; /* 修改字体大小 */  font-weight: bold; /* 修改字体粗细 */  color: #ff0055; /* 修改字体颜色 */  }  \u0026#39;))  ),  title = tagList(  span(  class = \u0026#34;my-title\u0026#34;, # 应用自定义样式类  \u0026#34;Title\u0026#34;  )  ) ) 侧栏缩进伴随标题缩写 模仿左图中AdminLTE缩写成“ALT”：\n \n 加载包（有这么多包是因为忘记了在哪个包中定义的）：\nlibrary(shiny) library(shinyjqui) library(shinydashboard) library(shinyWidgets) library(shinydashboardPlus) library(shinyAce) library(styler) library(shinyEffects) 决定侧栏缩进后标题能够有缩写的是\u0026quot;logo-lg\u0026quot;这个样式，HTML行是额外对标题的字体进行设置，text一行表示侧栏缩进之后显示指定文字，还可以使用img=src(\u0026quot;/path_to_picture\u0026quot;)来指定显示图标。\ndashboardHeader(  title = tagList(  span(  class = \u0026#34;logo-lg\u0026#34;,  HTML(\u0026#39;\u0026lt;div style=\u0026#34;font-family: Arial; font-size: 18px;\u0026#34;\u0026gt;AdminLTE\u0026lt;/div\u0026gt;\u0026#39;)),  text = HTML(\u0026#39;\u0026lt;div style=\u0026#34;font-family: Arial; font-size: 14px;\u0026#34;\u0026gt;\u0026lt;b\u0026gt;ALT\u0026lt;/b\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;)  )) 层级 1. 整体层级 以下图为例，介绍页面的层级：\n \n 为了代码管理的方便，我把每个页面单独创建了一个文件，在app.R中在适当位置直接进行调用，比如该图片展示的页面：repository_tab就是其中一个页面，在app.R中调用：\nui \u0026lt;- dashboardPage(  dashboardHeader(), # 设置导航栏  dashboardSidebar(), # 设置侧栏  dashboardBody(  # 各种标题、正文等等的格式和字体的设置放在这里  tabItems(  first_tab,  repository_tab # 页面放置位置  )  ),  title = \u0026#34;shinyDashboardPlus\u0026#34;, # 调用的主题title  footer = dashboardFooter( # 页面脚注  left = \u0026#34;By XXX\u0026#34;,  right = \u0026#34;XXX Lab, 2023\u0026#34;  )  )  server \u0026lt;- function(input, output){}  shinyApp(ui, server) 2. 单个页面的层级 box里面可以纳入很多组件，比如想要构建左侧为筛选条件，右侧为筛选得到的数据页面：\n \n repository_tab.R的框架：\nrepository_tab \u0026lt;- tabItem(  tabName = \u0026#34;repository\u0026#34;,   column(  width = 12,  align = \u0026#34;left\u0026#34;,  HTML(\u0026#39;\u0026lt;div style=\u0026#34;font-family: Arial; font-size: 22px;color: #7d7d73;\u0026#34;\u0026gt;test Database\u0026lt;/div\u0026gt;\u0026#39;)  ),# test Database 的小标题   box( # box的设置  sidebarPanel(), # 放置box内左侧的选择框  mainPanel (  width = 8,  tabsetPanel(  tabPanel(),  tabPanel() # 小页面  ) # 放置box内右侧的数据框  )  ) 选择框 1. 树形选择框  \n library(shinyWidgets) treeInput(  inputId = \u0026#34;ID2\u0026#34;,  label = \u0026#34;Select cities:\u0026#34;,  choices = create_tree(cities), # 使用create_tree函数对cities数据构建树形数据  returnValue = \u0026#34;text\u0026#34;,  closeDepth = 1 ) cities数据结构如下，每列为大类，列的内容为小类：\ncities \u0026lt;- data.frame(  continent = c(\u0026#34;America\u0026#34;, \u0026#34;America\u0026#34;, \u0026#34;America\u0026#34;, \u0026#34;Africa\u0026#34;,  \u0026#34;Africa\u0026#34;, \u0026#34;Africa\u0026#34;, \u0026#34;Africa\u0026#34;, \u0026#34;Africa\u0026#34;,  \u0026#34;Europe\u0026#34;, \u0026#34;Europe\u0026#34;, \u0026#34;Europe\u0026#34;, \u0026#34;Antarctica\u0026#34;),  country = c(\u0026#34;Canada\u0026#34;, \u0026#34;Canada\u0026#34;, \u0026#34;USA\u0026#34;, \u0026#34;Tunisia\u0026#34;, \u0026#34;Tunisia\u0026#34;,  \u0026#34;Tunisia\u0026#34;, \u0026#34;Algeria\u0026#34;, \u0026#34;Algeria\u0026#34;, \u0026#34;Italy\u0026#34;, \u0026#34;Germany\u0026#34;, \u0026#34;Spain\u0026#34;, NA),  city = c(\u0026#34;Trois-Rivières\u0026#34;, \u0026#34;Québec\u0026#34;, \u0026#34;San Francisco\u0026#34;, \u0026#34;Tunis\u0026#34;,  \u0026#34;Monastir\u0026#34;, \u0026#34;Sousse\u0026#34;, \u0026#34;Alger\u0026#34;, \u0026#34;Oran\u0026#34;, \u0026#34;Rome\u0026#34;, \u0026#34;Berlin\u0026#34;, \u0026#34;Madrid\u0026#34;, NA),  stringsAsFactors = FALSE )  \n 2. 树形结构选择框2  \n 代码：\nshinyWidgets::virtualSelectInput(  inputId = \u0026#34;search1\u0026#34;,  label = \u0026#34;Select:\u0026#34;,  choices = list(  \u0026#34;Spring\u0026#34; = c(\u0026#34;March\u0026#34;, \u0026#34;April\u0026#34;, \u0026#34;May\u0026#34;),  \u0026#34;Summer\u0026#34; = c(\u0026#34;June\u0026#34;, \u0026#34;July\u0026#34;, \u0026#34;August\u0026#34;),  \u0026#34;Autumn\u0026#34; = c(\u0026#34;September\u0026#34;, \u0026#34;October\u0026#34;, \u0026#34;November\u0026#34;),  \u0026#34;Winter\u0026#34; = c(\u0026#34;December\u0026#34;, \u0026#34;January\u0026#34;, \u0026#34;February\u0026#34;)  ),  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  ) 特别注意，这里的变量如果有空格，比如变量cancertype中存在Non-Small Cell Lung Cancer，需要额外把它转化为HTML：\nchoices = list(  \u0026#34;All\u0026#34; = lapply(cancertype, HTML)  ) MySQL数据库接入 建立数据库连接，可以直接在app.R中读入数据，也可以单独创建一个用于读取数据的脚本data_select.R：\nlibrary(tidyverse) library(RMySQL) # 创建连接 db_password \u0026lt;- Sys.getenv(\u0026#34;MYSQL_PASSWORD\u0026#34;)  con \u0026lt;- dbConnect(MySQL(),  user=\u0026#34;root\u0026#34;,  password=db_password,  dbname=\u0026#34;testdb\u0026#34; )  query.experiment \u0026lt;- \u0026#34;SELECT DISTINCT `sequencing.platform` FROM example;\u0026#34; experiment \u0026lt;- dbGetQuery(con, query.experiment)$sequencing.platform 直接把experiment变量正常使用即可，比如多选框：\n \n shinyWidgets::virtualSelectInput(  inputId = \u0026#34;select_experiment\u0026#34;,  label = \u0026#34;Experimental Strategy :\u0026#34;,  choices = list(  \u0026#34;Spring\u0026#34; = experiment  ),  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  ) 在server中使用数据库的时候，可以在最后加上一行，来保障关闭网页数据库断连。\n # Close MySQL at last  on.exit({  dbDisconnect(con)  }) 小插件 增加空行：br()\n安装依赖 跟随《Mastering Shiny》书中安装依赖包：\ninstall.packages(c(  \u0026#34;gapminder\u0026#34;, \u0026#34;ggforce\u0026#34;, \u0026#34;gh\u0026#34;, \u0026#34;globals\u0026#34;, \u0026#34;openintro\u0026#34;, \u0026#34;profvis\u0026#34;,  \u0026#34;RSQLite\u0026#34;, \u0026#34;shiny\u0026#34;, \u0026#34;shinycssloaders\u0026#34;, \u0026#34;shinyFeedback\u0026#34;,  \u0026#34;shinythemes\u0026#34;, \u0026#34;testthat\u0026#34;, \u0026#34;thematic\u0026#34;, \u0026#34;tidyverse\u0026#34;, \u0026#34;vroom\u0026#34;,  \u0026#34;waiter\u0026#34;, \u0026#34;xml2\u0026#34;, \u0026#34;zeallot\u0026#34; )) 参考资料   shiny 速查表\n  shiny posit官方指南\n  shinydashboardPlus的每个函数\n  shinydashboardPlus网页demo\n ","date":"October 25, 2023","image":null,"permalink":"/post/2023-10-23_shiny1/","title":"shiny【1】——基本框架和步骤"},{"categories":["生信工具"],"contents":"介绍各种组件的使用方法。\n text文本 ui \u0026lt;- fluidPage( textOutput(\u0026ldquo;text\u0026rdquo;), ) server \u0026lt;- function(input, output, session) { output$text \u0026lt;- renderText({ \u0026ldquo;Hello friend!\u0026rdquo; }) }\n1. infobox  \n 语法：ui中写入infoBoxOutput，serve中写入renderInfoBox，renderInfoBox中可以调用变量。\nui \u0026lt;- dashboardPage(  dashboardBody(  tabItem(  tabName = \u0026#34;home\u0026#34;,  fluidRow(  infoBoxOutput(\u0026#34;cpuBox\u0026#34;, width = 3),  infoBoxOutput(\u0026#34;likeBox\u0026#34;, width = 3),  infoBoxOutput(\u0026#34;saleBox\u0026#34;, width = 3),  infoBoxOutput(\u0026#34;memberBox\u0026#34;, width = 3)  )  )  ) )  server = function(input, output) {  output$cpuBox \u0026lt;- renderInfoBox({  infoBox(  \u0026#34;CPU TRAFFIC\u0026#34;,  value = paste0(input$cpu, \u0026#34;%\u0026#34;),  # subtitle = \u0026#34;\u0026#34;,  icon = icon(\u0026#34;gear\u0026#34;),  color = \u0026#34;light-blue\u0026#34;  )  })  output$likeBox \u0026lt;- renderInfoBox({  infoBox(...)  })  output$saleBox \u0026lt;- renderInfoBox({  infoBox(...)  })  output$memberBox \u0026lt;- renderInfoBox({  infoBox(...)  })  } 2. 环形比例图  \n 语法：ui中写入plotlyOutput，serve中写入renderPlotly。\nlibrary(shiny) library(plotly)  # 定义UI ui \u0026lt;- fluidPage(  fluidRow(  box( # 这里的box是我自己的网页中设置了一个外面的box，在box中按照上图中的顺序对环图进行排列  # title = \u0026#34;Title\u0026#34;,  status = \u0026#34;warning\u0026#34;,  width = 12,  fluidRow(  column(6,plotlyOutput(\u0026#34;pie1\u0026#34;,height = 200)),  column(6,plotlyOutput(\u0026#34;pie2\u0026#34;,height = 200))  )  )  ) )  # 定义服务器逻辑 server \u0026lt;- function(input, output) {  # 创建交互式圆环图  output$pie1 \u0026lt;- renderPlotly({  plot_ly(data1, labels = ~label1, values = ~Freq, type = \u0026#39;pie\u0026#39;, hole = 0.6, textinfo = \u0026#34;none\u0026#34;) %\u0026gt;%  hide_legend() %\u0026gt;%  layout(title = \u0026#34;Donor Infor1\u0026#34;)  })   output$pie2 \u0026lt;- renderPlotly({  plot_ly(data2, labels = ~label2, values = ~Freq, type = \u0026#39;pie\u0026#39;, hole = 0.6, textinfo = \u0026#34;none\u0026#34;) %\u0026gt;%  hide_legend() %\u0026gt;%  layout(title = \u0026#34;Donor Infor2\u0026#34;)  }) }  # 运行Shiny应用 shinyApp(ui, server) 地图 该部分代码还需要进一步debug\nlibrary(shiny) library(leaflet) library(ip2location)  # 创建一个IP地址到地理位置的映射，这里使用ip2location包获取地理位置信息 ip_to_location \u0026lt;- function(ip) {  # 这里需要您自己获取IP地址的地理位置信息  # 在示例中，我们使用假数据来模拟地理位置信息  return(list(lat = 40.7128, lon = -74.0060)) # 纽约市的经纬度 }  ui \u0026lt;- fluidPage(  titlePanel(\u0026#34;IP地址地图展示\u0026#34;),  mainPanel(  leafletOutput(\u0026#34;ip_map\u0026#34;)  ) )  server \u0026lt;- function(input, output, session) {  # 创建一个空的leaflet地图  output$ip_map \u0026lt;- renderLeaflet({  leaflet() %\u0026gt;%  addTiles() # 添加底图  })   # 处理新的IP地址并在地图上标记位置  observe({  visitors_ip \u0026lt;- req(session$clientData$session$remote$address)  location \u0026lt;- ip_to_location(visitors_ip)   # 在地图上添加标记  leafletProxy(\u0026#34;ip_map\u0026#34;) %\u0026gt;%  addMarkers(  data = location,  lat = ~lat,  lon = ~lon,  popup = visitors_ip # 在标记上显示IP地址  )  }) }  shinyApp(ui, server) 把ip地址转化为经纬度 library(ip2location)  # 初始化IP2Location数据库文件 db \u0026lt;- IP2Location$new(database = \u0026#34;path_to_database_file.IP2LOCATION-LITE-DB1.IPV6.BIN\u0026#34;)  # 定义一个函数来获取IP地址的经纬度 get_lat_lon_from_ip \u0026lt;- function(ip) {  result \u0026lt;- db$find(ip)  if (!is.null(result)) {  lat \u0026lt;- result$latitude  lon \u0026lt;- result$longitude  return(list(lat = lat, lon = lon))  } else {  return(NULL)  } }  # 测试获取IP地址的经纬度 ip_address \u0026lt;- \u0026#34;8.8.8.8\u0026#34; # 举例：Google的DNS服务器IP地址 location \u0026lt;- get_lat_lon_from_ip(ip_address) if (!is.null(location)) {  cat(\u0026#34;IP地址:\u0026#34;, ip_address, \u0026#34; 经度:\u0026#34;, location$lon, \u0026#34; 纬度:\u0026#34;, location$lat, \u0026#34;\\n\u0026#34;) } else {  cat(\u0026#34;未找到IP地址的位置信息\\n\u0026#34;) } 在shiny中根据用户选择，取数据选多个列的方法 ui \u0026lt;- {  shinyWidgets::virtualSelectInput(  inputId = \u0026#34;select_site\u0026#34;,  label = \u0026#34;Tumor site :\u0026#34;,  choices = list(  \u0026#34;All\u0026#34; = lapply(sub(\u0026#34;_\u0026#34;,\u0026#34; \u0026#34;,tumorsite.pie$tumor.site), HTML)  ),  selectize = FALSE,  showValueAsTags = TRUE,  search = TRUE,  multiple = TRUE  ) }  dataset \u0026lt;- reactive({  selected_columns \u0026lt;- input$select_site  if (!is.null(selected_columns) \u0026amp;\u0026amp; length(selected_columns) \u0026gt; 0) {  selected_columns \u0026lt;- paste0(\u0026#34;`\u0026#34;, selected_columns, \u0026#34;`\u0026#34;, collapse = \u0026#34;, \u0026#34;)  query \u0026lt;- sprintf(\u0026#34;SELECT %s FROM cbioportal;\u0026#34;, selected_columns)  cat(\u0026#34;Generated query:\u0026#34;, query, \u0026#34;\\n\u0026#34;)  dbGetQuery(con, query) %\u0026gt;% as.data.frame()  } else {  data.frame() # 返回一个空数据框，以避免错误  }  })   observeEvent(dataset(), {  updateVirtualSelect(  inputId = \u0026#34;select_site\u0026#34;,  choices = dataset()$select_site  )  })  output$dynamic1 \u0026lt;- renderDataTable( # Clinical  dataset(),  options = list(  pageLength = 10,  scrollX = TRUE  )  ) 参考资料：  master shiny ","date":"October 25, 2023","image":null,"permalink":"/post/2023-10-23_shiny2/","title":"shiny【2】——组件"},{"categories":["生信工具"],"contents":"介绍如何使用MySQL创建和使用数据库。\n查找并访问数据库  找出服务器上当前存在哪些数据库：  SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | performance_schema | +--------------------+ 2 rows in set (0.02 sec)  访问数据库  USE information_schema USE, 就像QUIT, 不需要分号。且USE语句：它必须在一行中给出。\n创建、使用、填入表格到数据库  创建数据库  创建名为menagerie的数据库。和SQL 关键字不同，在 Unix 下，数据库名称和表名区分大小写。如果提示没有权限创建数据库，参考本文《权限》：登陆管理员账户，授予该用户对所有数据库的全部权限。\nCREATE DATABASE menagerie;  使用数据库  USE menagerie 或者在调用mysql时在命令行上选择数据库，在任何连接参数后指定其名称即可。\nmysql -h host -u user -p menagerie  刚创建的数据库是空的，数据库的结构应该是什么：需要哪些表以及每个表中应该包含哪些列，可以查看数据库的内容：  SHOW TABLES; 使用CREATE TABLE语句指定表的布局，创建名为pet的表，内容有：姓名、所有者、物种、性别、出生和死亡。\nCREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20), species VARCHAR(20), sex CHAR(1), birth DATE, death DATE); VARCHAR对于姓名，所有者和物种是一个不错的选择，列值的长度不同，这些列定义中的长度也不必全部相同，如果后续需要更长的字段，MySQL会提供一个ALTER TABLE的声明。\n创建表后，SHOW TABLES应该会产生一些输出：\nmysql\u0026gt; SHOW TABLES; +---------------------+ | Tables in menagerie | +---------------------+ | pet | +---------------------+ 验证表是否按照预期的方式创建，请使用以下DESCRIBE语句：\nmysql\u0026gt; DESCRIBE pet; +---------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+-------------+------+-----+---------+-------+ | name | varchar(20) | YES | | NULL | | | owner | varchar(20) | YES | | NULL | | | species | varchar(20) | YES | | NULL | | | sex | char(1) | YES | | NULL | | | birth | date | YES | | NULL | | | death | date | YES | | NULL | | +---------+-------------+------+-----+---------+-------+ 创建表格后需要填充它，假设宠物的记录如下，注意MySQL存储日期的格式是‘YYYY-MM-DD’。由于创建的是一个空的表格，可以通过创建一个text文件，每行包含每个宠物，把text文件内容导入表格中。text文件的分隔符是tab，缺失的数据使用NULL，在文本中可以表示为\\N。比如：\nWhistler Gwen bird \\N 1997-12-09 \\N 要将文本文件pet.txt加载到表pet中，使用以下语句：\nLOAD DATA LOCAL INFILE \u0026#39;/path/pet.txt\u0026#39; INTO TABLE pet; 可以在LOAD DARA中指定列之间的分隔符和每行的结尾标记，默认情况下分隔符是tab，使用换行符作为每行的结尾。\n设置数据库权限  设置数据库权限，注意设置完权限后需要使用FLUSH PRIVILEGES;来刷新权限。  在该数据库中创建的任何内容都可以被有权访问该数据库的其他人删除。因此，可能应该向 MySQL 管理员请求设置自己的数据库的权限。假设想调名为“your_database”的数据库，管理员需要执行这样的语句：\nGRANT ALL ON your_database.* TO \u0026#39;your_mysql_name\u0026#39;@\u0026#39;your_client_host\u0026#39;; your_mysql_name MySQL 用户名，your_client_host是自己连接到服务器的主机。这句表示该用户your_mysql_name对该数据库your_database有全部权限，包括以下所有：\n  SELECT：用户被允许查询（检索）数据库中的数据。\n  INSERT：用户被允许将新数据插入到数据库表中。\n  UPDATE：用户被允许更新数据库中的数据。\n  DELETE：用户被允许删除数据库中的数据。\n  CREATE：用户被允许创建新表或数据库。\n  ALTER：用户被允许修改数据库结构（例如，更改表的列）。\n  DROP：用户被允许删除表或数据库。\n  GRANT OPTION：用户被授予将他们自己的权限授予其他用户的权限。\n  比如只允许用户查询（检索）数据库中的数据：\nGRANT SELECT ON your_database.* TO \u0026#39;your_mysql_name\u0026#39;@\u0026#39;your_client_host\u0026#39;; 修改数据库 举例：发现某行记录中日期写错了，有两种办法处理，一个是对原始的txt文件进行修改，删除mysql中创建的表格，再重新把txt文件导入心表格中；另一个就是直接对记录进行修改。\n 修改原始txt文件pet.txt  DELETE FROM pet; LOAD DATA LOCAL INFILE \u0026#39;pet.txt\u0026#39; INTO TABLE pet;  使用UPDATE来修改表格pet中的具体数值  UPDATE pet SET birth = \u0026#39;1989-08-31\u0026#39; WHERE name = \u0026#39;Bowser\u0026#39;;  查看被更改后的数值  mysql\u0026gt; SELECT * FROM pet WHERE name = \u0026#39;Bowser\u0026#39;; +--------+-------+---------+------+------------+------------+ | name | owner | species | sex | birth | death | +--------+-------+---------+------+------------+------------+ | Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 | +--------+-------+---------+------+------------+------------+  使用INSERT添加新纪录  最简单的是提供完整一行，比如：\nINSERT INTO pet VALUES (\u0026#39;Puffball\u0026#39;,\u0026#39;Diane\u0026#39;,\u0026#39;hamster\u0026#39;,\u0026#39;f\u0026#39;,\u0026#39;1999-03-30\u0026#39;,NULL); insert的时候可以直接使用NULL来代替空值，而不需要使用\\N。\n检索数据库  从表中检索信息  使用SELECT从表中提取信息，一般形式为：\nSELECT what_to_select FROM which_table WHERE conditions_to_satisfy; what_to_select表示想看到的内容，可以是列的列表，或者*来指示“所有列”。\nwhat_table表示从中检索数据的表。\nWHERE子句是可选的，如果存在，condition_to_satisfy则指定行必须满足的一个或多个条件才有资格进行检索。\nLOAD DATA INFILE \u0026#39;/path/to/your/file.csv\u0026#39; INTO TABLE your_table FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\u0026#34;\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; (column1, column2, column3, ...); 权限设置  查询权限：  SHOWGRANTSFOR\u0026#39;taozy\u0026#39;@\u0026#39;%\u0026#39;;+-------------------------------------------------------+|Grantsfortaozy@%|+-------------------------------------------------------+|GRANTUSAGEON*.*TO`taozy`@`%`||GRANTALLPRIVILEGESON`mydatabase`.*TO`taozy`@`%`|+-------------------------------------------------------+2rowsinset(0.00sec) 使用具有管理员权限的 MySQL 用户（例如 \u0026ldquo;root\u0026rdquo;）登录到 MySQL。将用户 \u0026ldquo;taozy\u0026rdquo; 提升为管理员权限：授予用户 \u0026ldquo;taozy\u0026rdquo; 对所有数据库的全部权限。  GRANT ALL PRIVILEGES ON *.* TO \u0026#39;taozy\u0026#39;@\u0026#39;%\u0026#39;;  刷新权限，每次更改权限后都需要刷新才能生效。  FLUSH PRIVILEGES;  撤销权限  将用户 \u0026ldquo;taozy\u0026rdquo; 重新限制为只有普通用户权限，以保持安全性：这将撤销用户 \u0026ldquo;taozy\u0026rdquo; 对所有数据库的所有权限。\nREVOKE ALL PRIVILEGES ON *.* FROM \u0026#39;taozy\u0026#39;@\u0026#39;%\u0026#39;; 参考资料  创建和使用数据库官方文档 ","date":"October 24, 2023","image":null,"permalink":"/post/2023-10-23_mysql_3/","title":"MySQL【3】——创建和使用数据库"},{"categories":["生信工具"],"contents":"解决MySQL无法导入本地问文件的问题。\n报错 could not run statement: Loading local data is disabled; this must be enabled on both the client and server sides 解决  登陆MySQL查看当前状态，在命令行输入：  SHOW GLOBAL VARIABLES LIKE \u0026#39;local_infile\u0026#39;; OFF状态：\n+---------------+-------+ | Variable_name | Value | +---------------+-------+ | local_infile | OFF | +---------------+-------+ 1 row in set (0.00 sec) 启用全局本地数据加载功能，在命令行输入：  SET GLOBAL local_infile = \u0026#39;ON\u0026#39;; SET GLOBAL local_infile = 1; SET GLOBAL local_infile = true; 重新检查状态：  SHOW GLOBAL VARIABLES LIKE \u0026#39;local_infile\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | local_infile | ON | +---------------+-------+ 1 row in set (0.01 sec) 注意，如果是重新启动MySQLsystemctl restart mysql还是会变更为OFF，只是退出MySQLexit;再登录不会变更。\n参考资料  How should I enable LOAD DATA LOCAL INFILE in my.cnf in MySQL? ","date":"October 24, 2023","image":null,"permalink":"/post/2023-10-23_mysql_4/","title":"MySQL【4】——MySQL导入本地文件"},{"categories":["生信工具"],"contents":"介绍如何使用R操作MySQL数据库。\n1. 安装  安装并加载RMySQL包：  install.packages(\u0026#34;RMySQL\u0026#34;) library(RMySQL) 2. 创建连接  创建连接  con \u0026lt;- dbConnect(MySQL(),  user=\u0026#34;taozy\u0026#34;, # MySQL用户名  password=\u0026#34;xxx\u0026#34;, # MySQL密码  # host=\u0026#34;192.168.1.244\u0026#34;,  # port=3306,  dbname=\u0026#34;cnadb\u0026#34; # 数据库名称  ) 如果想要避免密码暴露，可以把密码写在~/.Renviron文件中:\n# mysql password MYSQL_PASSWORD=xxx 在R中赋值给变量db_password，再传给dbConnect函数中的password参数：\ndb_password \u0026lt;- Sys.getenv(\u0026#34;MYSQL_PASSWORD\u0026#34;)  断开连接：  dbDisconnect(con)  查看数据库信息  dbGetInfo(con)  列出数据库中的表格，我的数据库中目前没有任何表格。  dbListTables(con) 3. 导入数据并创建表格  表格为空，向表格中导入数据，会自动创建表格，不需要像在MySQL命令行中那样先创建表格，再导入数据。  dbWriteTable(  conn = con, # 连接对象  name = \u0026#34;example\u0026#34;, # 创建的表格名字  value = example, # R中的数据名，R对象即可  row.names = FALSE  # overwrite = FALSE,  # append = FALSE,  # field.types = NULL,  # temporary = FALSE  )  删除创建的表格  dbRemoveTable(con,\u0026#39;example\u0026#39;) 3. 查看表格内容 MySQL在R接口中的筛选语法：dbGetQuery(连接对象, SQL语句)，其中所有SQL语句都可以直接在mysql命令中运行。\n 查看example表格中的所有内容  dbGetQuery(con,\u0026#39;select * from example\u0026#39;) *：所有的列。\n在R中View查看：\nView(dbGetQuery(con,\u0026#39;select * from example\u0026#39;))  显示表格的前三行  dbGetQuery(con, \u0026#34;SELECT * FROM example limit 3\u0026#34;) 4. 按列筛选行  筛选表格的行：指定列的数值  举例：筛选条件为，对example表格中，根据sequencing.platform列选择数值为“Targeted”的行。\ndbGetQuery(con, \u0026#34;SELECT * FROM example WHERE `sequencing.platform` = \u0026#39;Targeted\u0026#39;\u0026#34;) 在MySQL中就是：\nSELECT * FROM example WHERE `sequencing.platform` = \u0026#39;Targeted\u0026#39;; 注意，这里的字符串比较通常不区分大小写，所以可以使用Targeted来查找，也可以使用TARGETED来查找。\n 筛选表格的行：年月的比较  选择pet表格中出生日期大于等于1998-1-1的行。\nSELECT * FROM pet WHERE birth \u0026gt;= \u0026#39;1998-1-1\u0026#39;; +----------+-------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +----------+-------+---------+------+------------+-------+ | Chirpy | Gwen | bird | f | 1998-09-11 | NULL | | Puffball | Diane | hamster | f | 1999-03-30 | NULL | +----------+-------+---------+------+------------+-------+  组合条件筛选：AND  举例：选择species为dog，且owner为harold：\nSELECT * FROM pet WHERE species = \u0026#39;dog\u0026#39; AND owner = \u0026#39;Harold\u0026#39;; +-------+--------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +-------+--------+---------+------+------------+-------+ | Buffy | Harold | dog | f | 1989-05-13 | NULL | +-------+--------+---------+------+------------+-------+  组合条件筛选：OR  举例：选择species为snake，或species为bird：\nSELECT * FROM pet WHERE species = \u0026#39;snake\u0026#39; OR species = \u0026#39;bird\u0026#39;; +----------+-------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +----------+-------+---------+------+------------+-------+ | Chirpy | Gwen | bird | f | 1998-09-11 | NULL | | Whistler | Gwen | bird | NULL | 1997-12-09 | NULL | | Slim | Benny | snake | m | 1996-04-29 | NULL | +----------+-------+---------+------+------------+-------+  组合条件筛选：OR AND 混用，AND的优先级比OR高。  SELECT * FROM pet WHERE (species = \u0026#39;cat\u0026#39; AND sex = \u0026#39;m\u0026#39;)  OR (species = \u0026#39;dog\u0026#39; AND sex = \u0026#39;f\u0026#39;); +-------+--------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +-------+--------+---------+------+------------+-------+ | Claws | Gwen | cat | m | 1994-03-17 | NULL | | Buffy | Harold | dog | f | 1989-05-13 | NULL | +-------+--------+---------+------+------------+-------+  组合条件筛选，使用IN来选择包含在指定变量中的内容  SELECT * FROM your_table WHERE column_name IN (\u0026#39;a1\u0026#39;, \u0026#39;a2\u0026#39;, \u0026#39;a3\u0026#39;); 特别注意：IN后面的格式一个都不能少，比如a1两边的''以及不同变量之间的,，以及所有变量外面的括号。\n5. 筛选列  根据列名  举例：筛选name和birth列\nmysql\u0026gt; SELECT name, birth FROM pet; +----------+------------+ | name | birth | +----------+------------+ | Fluffy | 1993-02-04 | | Claws | 1994-03-17 | | Buffy | 1989-05-13 | | Fang | 1990-08-27 | | Bowser | 1989-08-31 | | Chirpy | 1998-09-11 | | Whistler | 1997-12-09 | | Slim | 1996-04-29 | | Puffball | 1999-03-30 | +----------+------------+  根据列名筛选，并去除重复  选择pet表格中的owner列\nSELECT owner FROM pet; +--------+ | owner | +--------+ | Harold | | Gwen | | Harold | | Benny | | Diane | | Gwen | | Gwen | | Benny | | Diane | +--------+ 使用DISTINCT对owner列去除重复：\nSELECT DISTINCT owner FROM pet; +--------+ | owner | +--------+ | Benny | | Diane | | Gwen | | Harold | +--------+  组合对行列进行筛选  选择三列：name, species, birth，并筛选species为dog或cat的行。\nSELECT name, species, birth FROM pet  WHERE species = \u0026#39;dog\u0026#39; OR species = \u0026#39;cat\u0026#39;; +--------+---------+------------+ | name | species | birth | +--------+---------+------------+ | Fluffy | cat | 1993-02-04 | | Claws | cat | 1994-03-17 | | Buffy | dog | 1989-05-13 | | Fang | dog | 1990-08-27 | | Bowser | dog | 1989-08-31 | +--------+---------+------------+ 6. 排序  使用ORDER BY根据列值对行进行排序  SELECT name, birth FROM pet ORDER BY birth; +----------+------------+ | name | birth | +----------+------------+ | Buffy | 1989-05-13 | | Bowser | 1989-08-31 | | Fang | 1990-08-27 | | Fluffy | 1993-02-04 | | Claws | 1994-03-17 | | Slim | 1996-04-29 | | Whistler | 1997-12-09 | | Chirpy | 1998-09-11 | | Puffball | 1999-03-30 | +----------+------------+ 默认是升序排列，使用DESC降序排列。\nSELECT name, birth FROM pet ORDER BY birth DESC; +----------+------------+ | name | birth | +----------+------------+ | Puffball | 1999-03-30 | | Chirpy | 1998-09-11 | | Whistler | 1997-12-09 | | Slim | 1996-04-29 | | Claws | 1994-03-17 | | Fluffy | 1993-02-04 | | Fang | 1990-08-27 | | Bowser | 1989-08-31 | | Buffy | 1989-05-13 | +----------+------------+  同时根据多列对行进行降序  SELECT name, species, birth FROM pet  ORDER BY species, birth DESC; +----------+---------+------------+ | name | species | birth | +----------+---------+------------+ | Chirpy | bird | 1998-09-11 | | Whistler | bird | 1997-12-09 | | Claws | cat | 1994-03-17 | | Fluffy | cat | 1993-02-04 | | Fang | dog | 1990-08-27 | | Bowser | dog | 1989-08-31 | | Buffy | dog | 1989-05-13 | | Puffball | hamster | 1999-03-30 | | Slim | snake | 1996-04-29 | +----------+---------+------------+ 注意到，DESC只对邻近的1个列生效，并不对species这列造成影响。\n 对于字符串，可以使用ORDER BY BINARY col_name强制对大小写进行排序。  7. 日期计算 MySQL提供多个对日期的计算，比如计算年龄或者提取部分日期。\n 根据日期计算年龄  使用TIMESTAMPDIFF来计算，生成age列。\nSELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet; +----------+------------+------------+------+ | name | birth | CURDATE() | age | +----------+------------+------------+------+ | Fluffy | 1993-02-04 | 2003-08-19 | 10 | | Claws | 1994-03-17 | 2003-08-19 | 9 | | Buffy | 1989-05-13 | 2003-08-19 | 14 | | Fang | 1990-08-27 | 2003-08-19 | 12 | | Bowser | 1989-08-31 | 2003-08-19 | 13 | | Chirpy | 1998-09-11 | 2003-08-19 | 4 | | Whistler | 1997-12-09 | 2003-08-19 | 5 | | Slim | 1996-04-29 | 2003-08-19 | 7 | | Puffball | 1999-03-30 | 2003-08-19 | 4 | +----------+------------+------------+------+  对两列之间的数值进行运算，并把结果输入新的列  举例：death为NULL表示没有死亡，计算非NULL的死亡样本的存活时间。\nSELECT name, birth, death, TIMESTAMPDIFF(YEAR,birth,death) AS age FROM pet WHERE death IS NOT NULL ORDER BY age; +--------+------------+------------+------+ | name | birth | death | age | +--------+------------+------------+------+ | Bowser | 1989-08-31 | 1995-07-29 | 5 | +--------+------------+------------+------+  单独对日期提取出年、月、日信息  比如YEAR()，MONTH()，DAYOFMONTH()等等。举例：提取出月份信息。\nSELECT name, birth, MONTH(birth) FROM pet; +----------+------------+--------------+ | name | birth | MONTH(birth) | +----------+------------+--------------+ | Fluffy | 1993-02-04 | 2 | | Claws | 1994-03-17 | 3 | | Buffy | 1989-05-13 | 5 | | Fang | 1990-08-27 | 8 | | Bowser | 1989-08-31 | 8 | | Chirpy | 1998-09-11 | 9 | | Whistler | 1997-12-09 | 12 | | Slim | 1996-04-29 | 4 | | Puffball | 1999-03-30 | 3 | +----------+------------+--------------+ 8. 模式匹配 MySQL 提供标准 SQL 模式匹配以及基于扩展正则表达式的模式匹配形式，类似于 Unix 实用程序（例如 vi、grep和 sed）所使用的模式匹配。可以使用_来匹配任何单字符，用%来匹配任何数量的字符（包括零个字符）。在 MySQL 中，SQL 模式默认不区分大小写。不要使用 =或\u0026lt;\u0026gt;，改用LIKE或 NOT LIKE做比较运算符。\n 查找行，满足条件：name以b开头  SELECT * FROM pet WHERE name LIKE \u0026#39;b%\u0026#39;; +--------+--------+---------+------+------------+------------+ | name | owner | species | sex | birth | death | +--------+--------+---------+------+------------+------------+ | Buffy | Harold | dog | f | 1989-05-13 | NULL | | Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 | +--------+--------+---------+------+------------+------------+  查找行，满足条件：name以fy结尾  SELECT * FROM pet WHERE name LIKE \u0026#39;%fy\u0026#39;; +--------+--------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +--------+--------+---------+------+------------+-------+ | Fluffy | Harold | cat | f | 1993-02-04 | NULL | | Buffy | Harold | dog | f | 1989-05-13 | NULL | +--------+--------+---------+------+------------+-------+  查找行，满足条件：包含w  SELECT * FROM pet WHERE name LIKE \u0026#39;%w%\u0026#39;; +----------+-------+---------+------+------------+------------+ | name | owner | species | sex | birth | death | +----------+-------+---------+------+------------+------------+ | Claws | Gwen | cat | m | 1994-03-17 | NULL | | Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 | | Whistler | Gwen | bird | NULL | 1997-12-09 | NULL | +----------+-------+---------+------+------------+------------+  查找行，满足条件：name恰好包含五个字符，使用5个_  SELECT * FROM pet WHERE name LIKE \u0026#39;_____\u0026#39;; +-------+--------+---------+------+------------+-------+ | name | owner | species | sex | birth | death | +-------+--------+---------+------+------------+-------+ | Claws | Gwen | cat | m | 1994-03-17 | NULL | | Buffy | Harold | dog | f | 1989-05-13 | NULL | +-------+--------+---------+------+------------+-------+  MySQL 提供的另一种模式匹配：使用扩展正则表达式。使用函数 REGEXP_LIKE()（或 REGEXP或RLIKE 运算符）  举例：要查找以b开头的name，使用 ^来匹配name的开头：\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;^b\u0026#39;); 想要强制正则表达式比较区分大小写，可以使用区分大小写的规则，或使用关键字 BINARY使字符串之一成为二进制字符串，或指定c 匹配控制字符。以下查询中的每一个都仅匹配b名称开头的小写字母：\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;^b\u0026#39; COLLATE utf8mb4_0900_as_cs); SELECT * FROM pet WHERE REGEXP_LIKE(name, BINARY \u0026#39;^b\u0026#39;); SELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;^b\u0026#39;, \u0026#39;c\u0026#39;); 查询以fy结尾的name，使用$来匹配：\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;fy$\u0026#39;); 要查找包含w的name，使用以下查询：\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;w\u0026#39;); 要查找恰好包含五个字符的name，请使用 ^和$来匹配name的开头和结尾，以及中间的五个.\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;^.....$\u0026#39;); 或者使用：\nSELECT * FROM pet WHERE REGEXP_LIKE(name, \u0026#39;^.{5}$\u0026#39;); 9. 计算行列数 使用COUNT计算表格pet的行数：\nSELECT COUNT(*) FROM pet; +----------+ | COUNT(*) | +----------+ | 9 | +----------+ 使用SHOW COLUMNS计算表格的列数：\nSHOW COLUMNS FROM table_name; 每个人owner有多少宠物pet：\nSELECT owner, COUNT(*) FROM pet GROUP BY owner; 组合species和sex计数：\nSELECT species, sex, COUNT(*) FROM pet GROUP BY species, sex; 组合其他筛选使用：\nSELECT species, sex, COUNT(*) FROM pet WHERE species = \u0026#39;dog\u0026#39; OR species = \u0026#39;cat\u0026#39; GROUP BY species, sex; +---------+------+----------+ | species | sex | COUNT(*) | +---------+------+----------+ | cat | f | 1 | | cat | m | 1 | | dog | f | 1 | | dog | m | 2 | +---------+------+----------+ 10. 使用多个表格：多表查询 注意，多表查询需要有连接条件，否则会出现笛卡尔积的错误。\n 等值连接  举例：使用iGMDR数据库中内容：\ndrug_gene \u0026lt;- dbGetQuery(con,\u0026#39;select * from drug_gene\u0026#39;) gene_info \u0026lt;- dbGetQuery(con,\u0026#39;select * from gene_info\u0026#39;)  左：drug_gene, 右：gene_info\n multitable \u0026lt;- dbGetQuery(  con,  \u0026#39;SELECT *  FROM gene_info, drug_gene  WHERE gene_info.gene_id = drug_gene.gene_id AND gene_info.gene_id = 5789  LIMIT 3\u0026#39;  )  \n 非等值连接  multitable.unequal \u0026lt;- dbGetQuery(  con,  \u0026#39;SELECT *  FROM gene_info, drug_gene  WHERE gene_info.gene_id BETWEEN 5789 AND 5800  LIMIT 3\u0026#39; )   FROM连接两个表中的信息，两个表中的name\n  ON：根据名称值来匹配两个表\n  INNNER JOIN：两个表都满足ON中指定的条件时，才允许来自任一表格的行出现在结果中。在此示例中，ON 子句指定 pet 表中的名称列必须与事件表中的名称列匹配。如果某个名称出现在一个表中但没有出现在另一个表中，则该行不会出现在结果中。\n  由于名称列出现在两个表中，因此在引用该列时必须具体说明所指的是哪个表。 这是通过将表名添加到列名之前来完成的。\n  11. 关闭数据库连接 再R中用完MySQL数据库后应及时关闭数据库连接，比如：\nlibrary(RMySQL)  # 创建MySQL连接 con \u0026lt;- dbConnect(MySQL(), user = \u0026#34;your_username\u0026#34;, password = \u0026#34;your_password\u0026#34;, dbname = \u0026#34;your_database\u0026#34;)  # 这将关闭名为\u0026#39;con\u0026#39;的连接 dbDisconnect(con) 如果每次使用的时候都没有关闭，那么使用次数超过16次后（默认）则无法再打开，出现以下报错：\nError in .local(drv, ...) : Cannot allocate a new connection: 16 connections already opened 此时可以列出所有连接并关闭：\nlibrary(RMySQL)  # 列出所有的连接 open_connections \u0026lt;- dbListConnections(MySQL())  # 关闭所有连接 for (conn in open_connections) {  dbDisconnect(conn) }  # 验证所有连接已关闭 open_connections \u0026lt;- dbListConnections(MySQL()) if (length(open_connections) == 0) {  cat(\u0026#34;All database connections have been closed.\\n\u0026#34;) } else {  cat(\u0026#34;Some database connections were not closed.\\n\u0026#34;) } 统计某列中不重复数值 dbGetQuery(  con,  \u0026#34;SELECT COUNT(DISTINCT `Sample.ID`) FROM cbioportal;\u0026#34; ) %\u0026gt;% as.numeric() SQL语句中引入变量 使用sprintf和%s配套使用，在SQL语句中引用变量：\nlibrary(tidyverse) library(RMySQL) db_password \u0026lt;- Sys.getenv(\u0026#34;MYSQL_PASSWORD\u0026#34;) con \u0026lt;- dbConnect(MySQL(),  user=\u0026#34;root\u0026#34;,  password=XXX,  dbname=\u0026#34;XXX\u0026#34; ) term \u0026lt;- \u0026#34;column_name_of_dataframe\u0026#34;  # 使用参数化查询来引用R中的变量 query \u0026lt;- sprintf(\u0026#34;SELECT COUNT(DISTINCT `%s`) FROM cbioportal;\u0026#34;, term)  out \u0026lt;- dbGetQuery(con, query) %\u0026gt;% as.numeric() 根据查询结果生成新数据 -- 创建新表 CREATE TABLE new_table (  column1 datatype,  column2 datatype,  -- 添加更多列 );  -- 将查询结果插入到新表 INSERT INTO new_table SELECT column1, column2 FROM your_table WHERE condition; -- 可选的筛选条件 检索提速-添加索引  查看已经存在的索引  SHOW INDEX FROM table_name;  删除索引  DROP INDEX index_name ON table_name; 应用场景：我有上百万行的数据，想要按照某几列的内容检索，并且输出满足条件的行。此时通过设定这几列的索引，再通过分页的方法可以使得结果输出在1s以内。注意组合使用效果比较好。举例：\nCREATE INDEX index_name ON table_name (column1); # 多列(column1, column2, ...) SELECT * FROM cbioportal WHERE `sequencing.platform` IN (\u0026#39;Wes\u0026#39;) limit 1000,100; 参考资料  Retrieving Information from a Table ","date":"October 24, 2023","image":null,"permalink":"/post/2023-10-23_mysql_r/","title":"MySQL【5】——MySQL接入R语言"},{"categories":["生信工具"],"contents":"介绍非root用户如何安装MySQL。\n1. 下载 在该网址上选择对应的MySQL Community Server版本：https://dev.mysql.com/downloads/mysql/5.7.html，我这里选的是5.7.44\n \n wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.44-linux-glibc2.12-x86_64.tar.gz tar -zxvf mysql-5.7.44-linux-glibc2.12-x86_64.tar.gz mv mysql-5.7.44-linux-glibc2.12-x86_64 mysql # 更改名称 创建一个my.conf文件：\n[client] port=3336 socket=/your_path/mysql/mysql.sock  [mysqld] port=3336 basedir=/your_path/mysql datadir=/your_path/mysql/data pid-file=/your_path/mysql/mysql.pid socket=/your_path/mysql/mysql.sock log_error=/your_path/mysql/error.log server-id=100 在mysql目录下运行：\nbin/mysqld --defaults-file=/your_path/mysql/my.cnf --basedir=/your_path/mysql/ --datadir=/your_path/mysql/data/ --user=mysql --initialize 找error.log里面的初始密码：\ncat error.log  A temporary password is generated for root@localhost: YkGFpqMki7?\u0026lt; 启动：\nbin/mysqld_safe \\ --defaults-file=/your_path/mysql/my.cnf \\ --user=oper \u0026amp; 增加sock路径:\nbin/mysql -u root -p -S /your_path/mysql/mysql.sock 输入初始密码即可登陆。\n参考资料：  Linux非root用户安装及配置MySql[吊打某度95%以上的教程] ","date":"October 24, 2023","image":null,"permalink":"/post/2023-10-23_mysql_5_nonroot/","title":"MySQL【6】——非root安装MySQL"},{"categories":["生信工具"],"contents":"在ubuntu上安装MySQL。\n安装步骤  安装：  sudo apt-get install mysql-server mysql-client mysql-server：\n是数据库服务器软件包，用于存储和管理数据库的核心服务，负责接收来自客户端应用程序的数据库查询请求，并执行请求。\nmysql-client：\nMySQL 客户端软件包。它包含用于与 MySQL 服务器进行通信的命令行工具和客户端库。允许用户以交互方式或通过编程语言（如Python、Java、PHP等）脚本执行数据库查询。它们提供了管理和查询数据库的工具，允许用户执行各种数据库操作。\n ubuntu上安装MySQL没有显示要设置账户和密码，通过以下代码检查默认设置的密码：  sudo cat /etc/mysql/debian.cnf  \n  输入默认密码登陆  mysql -u debian-sys-maint -p  创建一个名为 \u0026ldquo;newuser\u0026rdquo; 的用户，密码为 \u0026ldquo;newpassword\u0026rdquo;：  CREATE USER \u0026#39;newuser\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;newpassword\u0026#39;; # mysql\u0026gt; CREATE USER \u0026#39;taozy\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;XXX\u0026#39;; Query OK, 0 rows affected (0.04 sec) \u0026rsquo;newuser\u0026rsquo;：新用户的用户名。\n\u0026rsquo;localhost\u0026rsquo;：这表示用户只能从本地主机连接到 MySQL。如果您希望用户可以从任何主机连接，请使用\u0026rsquo;%\u0026lsquo;代替\u0026rsquo;localhost\u0026rsquo;。\n\u0026rsquo;newpassword\u0026rsquo;：新用户的密码。\n 授予权限：使用 GRANT 语句来为新用户分配权限。例如，给用户 \u0026ldquo;newuser\u0026rdquo; 对 \u0026ldquo;mydatabase\u0026rdquo; 数据库的所有权限：  GRANT ALL PRIVILEGES ON mydatabase.* TO \u0026#39;newuser\u0026#39;@\u0026#39;localhost\u0026#39;; # GRANT ALL PRIVILEGES ON mydatabase.* TO \u0026#39;taozy\u0026#39;@\u0026#39;%\u0026#39;; mydatabase：要分配权限的数据库名称。使用 * 表示所有数据库。\n\u0026rsquo;newuser\u0026rsquo;@\u0026rsquo;localhost\u0026rsquo;：新用户的用户名和允许连接的主机。\n如果只需要授予特定权限（例如，SELECT、INSERT、UPDATE 等），而不是所有权限，可以相应地调整权限列表。\n 刷新权限：在执行权限更改后，刷新 MySQL 的权限表以使更改生效：  FLUSH PRIVILEGES;  退出 MySQL：  EXIT; 或者control+D来断开。\n 创建完新的用户和密码就可以使用新的来登陆了  mysql -u taozy -p 参考资料  官方文档 ","date":"October 23, 2023","image":null,"permalink":"/post/2023-10-23_mysql_1/","title":"MySQL【1】——在linux系统上安装MySQL"},{"categories":["生信工具"],"contents":"介绍MySQL基本语法。\n基础命令  HELP  mysql --help  启动MySQL服务器  sudo systemctl start mysql  重启  sudo systemctl restart mysql  登陆  mysql -h host -u user -p host是MySQL服务器主机名。如果在运行 MySQL 的同一台计算机上登录，则可以省略-h host\n MySQL版本和当前日期  mysql\u0026gt; SELECT VERSION(), CURRENT_DATE; 查询通常由 SQL 语句后跟分号组成。（在某些例外情况下，可以省略分号。QUIT前面提到的 就是其中之一。稍后我们将讨论其他情况。）\nmysql以表格形式（行和列）显示查询输出。第一行包含列的标签。以下行是查询结果。通常，列标签是从数据库表中获取的列的名称。如果您要检索表达式而不是表列的值（如刚刚所示的示例）， mysql将使用表达式本身来标记该列。\nmysql显示返回了多少行以及执行查询需要多长时间，这可以让您大致了解服务器性能。这些值不精确，因为它们代表挂钟时间（不是 CPU 或机器时间），并且受到服务器负载和网络延迟等因素的影响。\n 关键字可以以任何字母大小写输入。以下查询是等效的：  mysql\u0026gt; SELECT VERSION(), CURRENT_DATE; mysql\u0026gt; select version(), current_date; mysql\u0026gt; SeLeCt vErSiOn(), current_DATE;  MySQL作简单计算器:这是另一个查询。它演示了您可以使用 mysql作为一个简单的计算器：  mysql\u0026gt; SELECT SIN(PI()/4), (4+1)*5; +------------------+---------+ | SIN(PI()/4) | (4+1)*5 | +------------------+---------+ | 0.70710678118655 | 25 | +------------------+---------+ 1 row in set (0.02 sec)  可以在一行中输入多个语句。每个都以分号结束  mysql\u0026gt; SELECT VERSION(); SELECT NOW(); +-----------+ | VERSION() | +-----------+ | 8.0.13 | +-----------+ 1 row in set (0.00 sec)  +---------------------+ | NOW() | +---------------------+ | 2018-08-24 00:56:40 | +---------------------+ 1 row in set (0.00 sec) mysql通过查找终止分号来确定语句的结束位置，而不是通过查找输入行的末尾。（换句话说，mysql 接受自由格式输入：它收集输入行，但在看到分号之前不会执行它们。）\nmysql\u0026gt; SELECT  -\u0026gt; USER()  -\u0026gt; ,  -\u0026gt; CURRENT_DATE; +---------------+--------------+ | USER() | CURRENT_DATE | +---------------+--------------+ | jon@localhost | 2018-08-24 | +---------------+--------------+  中断命令  如果您决定不想执行正在输入的查询，请输入以下命令取消它 \\c：\nmysql\u0026gt; SELECT  -\u0026gt; USER()  -\u0026gt; \\c mysql\u0026gt; ","date":"October 23, 2023","image":null,"permalink":"/post/2023-10-23_mysql_2/","title":"MySQL【2】——常用命令"},{"categories":["生信工具"],"contents":"记录实验室服务器批量运行任务的脚本。\n介绍 批量对sort.bam计算平均测序深度。\n脚本 从创建template，到批量生成每个样本的深度计算shell脚本，到提交，打包成一个脚本。\nbampath=/home/data/sda/tzy/colon/# Path of sorted bam file 【required】 outpath=/home/data/sda/tzy/colon_swgs_depth# Path of script file【required】 cd$bampath### generate config file foriin*_sorted.bam;doecho${i%.*};done\u0026gt;$outpath/config【required】### generate template file echo\u0026#39;#!/bin/bash\u0026#39;\u0026gt;templatecat\u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;templatecd$bampathoutpath=/home/data/sda/tzy/colon_swgs_depthsamtoolsdepth-@120-a./\u0026lt;sample\u0026gt;.bam\u0026gt;$outpath/result/\u0026lt;sample\u0026gt;_depth.txtawk\u0026#39;{sum += $3} END {print \u0026#34;Average depth: \u0026#34;, sum/NR}\u0026#39;$outpath/result/\u0026lt;sample\u0026gt;_depth.txt\u0026gt;$outpath/result/\u0026lt;sample\u0026gt;_average_depth.txt”EOF### generate mapping file echo\u0026#39;\u0026lt;sample\u0026gt;,0\u0026#39;\u0026gt;mapping### batch generate shell script sync-qgen-ftemplate-sconfig-mmapping-d$outpath/colon_sh### change name from .pbs to .sh cd$path/colon_shforiin`ls./`;domv$i${i%%.*}.sh;done### batch run script chmod-R777$outpath/colon_shforiin`ls`;dosudoecho\u0026#34;nohup $outpath/colon_sh/$i \u0026amp;\u0026#34;|bash;done","date":"October 18, 2023","image":null,"permalink":"/post/2023-10-18_shell%E8%84%9A%E6%9C%AC/","title":"【shell脚本】——实验室服务器批量运行任务"},{"categories":["生信工具"],"contents":"维护。\n状况 在实验室服务器提交了任务，然而使用top命令查看时发现，只有3个在运行，其余任务全部在D（暂停）状态，任务跑的奇慢无比。\n查询 问了一下chatgpt：“My server has more than 1000 threads available, but when you run 100 scripts each only uses one thread, only a few scripts are running. why”\n给出如下原因：\n  资源限制:尽管有许多线程，但服务器的资源，如CPU、内存和I/O，可能会受到限制。如果服务器的资源耗尽，它可以限制并发脚本执行的数量。\n  线程池配置:如果使用线程池管理线程，则池的配置可能会限制并发线程的数量。检查线程池设置，包括最大线程数和队列大小，以确保它们足以满足您的需求。\n  任务调度:任务调度逻辑可能导致脚本排队并等待执行。糟糕的任务调度可能导致一些脚本排队等待，而另一些脚本获得优先级。\n  阻塞操作:一些脚本可能包含阻塞操作，例如等待外部资源(例如，数据库查询或网络请求)，这可能导致线程等待而不执行其他脚本。\n  并发控制:可能存在线程争用或死锁条件，导致线程无法继续。\n  检查资源配置 top 使用top、htop或atop等系统监控工具检查服务器的当前资源使用情况。这些工具提供关于CPU、内存和进程利用率的实时信息。比如使用top：\n \n 第一行是系统整体的统计信息，\ntop - 14:00:12 up 2 min, 5 users, load average: 82.69, 21.31, 7.24 top：当前时间。\nup：机器运行了多长时间。\nusers：当前登录用户数。\nload average：系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。这个数值高了也并不能直接代表这台机器的性能有问题，可能是因为正在进行CPU密集型的计算，也有可能是因为I/O问题导致运行队列堵了。所以，看到这个数值飙升的时候，还得具体问题具体分析。一个CPU在一个时间片里面只能运行一个进程，CPU核数的多少直接影响到这台机器在同时间能运行的进程数。所以一般来说Load Average的数值别超过这台机器的总核数，就基本没啥问题。\n第二行是进程统计，\nTasks: 1657 total, 2 running, 1530 sleeping, 3 stopped, 122 zombie Tasks：当前有多少进程。\nrunning：正在运行的进程数。\nsleeping：正在休眠的进程数。\nstopped：停止的进程数。\nzombie：僵尸进程数。\n第三行的统计，\n%Cpu(s): 5.1 us, 0.3 sy, 0.0 ni, 93.8 id, 0.7 wa, 0.0 hi, 0.1 si, 0.0 st us：用户空间占CPU的百分比（像shell程序、各种语言的编译器、各种应用、web服务器和各种桌面应用都算是运行在用户地址空间的进程）。\nsy：内核空间占CPU的百分比（所有进程要使用的系统资源都是由Linux内核处理的，对于操作系统的设计来说，消耗在内核态的时间应该是越少越好，在实践中有一类典型的情况会使sy变大，那就是大量的IO操作，因此在调查IO相关的问题时需要着重关注它）。\nni：用户进程空间改变过优先级（ni是nice的缩写，可以通过nice值调整进程用户态的优先级，这里显示的ni表示调整过nice值的进程消耗掉的CPU时间，如果系统中没有进程被调整过nice值，那么ni就显示为0）。\nid：空闲CPU占用率。\nwa：等待输入输出的CPU时间百分比（和CPU的处理速度相比，磁盘IO操作是非常慢的，有很多这样的操作，比如，CPU在启动一个磁盘读写操作后，需要等待磁盘读写操作的结果。在磁盘读写操作完成前，CPU只能处于空闲状态。Linux系统在计算系统平均负载时会把CPU等待IO操作的时间也计算进去，所以在我们看到系统平均负载过高时，可以通过wa来判断系统的性能瓶颈是不是过多的IO操作造成的）。\nhi：硬中断占用百分比【硬中断是硬盘、网卡等硬件设备发送给CPU的中断消息，当CPU收到中断消息后需要进行适当的处理(消耗CPU时间)】。\nsi：软中断占用百分比（软中断是由程序发出的中断，最终也会执行相应的处理程序，消耗CPU时间）。\nst：steal time。\n第四行：\nMiB Mem : 257538.2 total, 980.7 free, 25744.3 used, 230813.2 buff/cache total：物理内存总量。\nfree：空闲内存量。\nused：使用的内存量。\nbuffer/cache：用作内核缓存的内存量。\n第五行：\nMiB Swap: 2048.0 total, 2042.2 free, 5.8 used. 229947.6 avail Mem total：交换区内存总量。\nfree：空闲交换区总量。\nused：使用的交换区总量。\nbuffer/cache：缓冲的交换区总量。\n不过当内存的free变少的时候，其实我们并不需要太紧张。真正需要看的是Swap中的used信息。Swap分区是由硬盘提供的交换区，当物理内存不够用的时候，操作系统才会把暂时不用的数据放到Swap中。所以当这个数值变高的时候，说明内存是真的不够用了。\n进程信息：\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 7388 tzy 20 0 1410560 11132 8324 S 9.5 0.0 0:00.31 samtools PID 进程id\nUSER 进程所有者的用户名\nPR 优先级，数字越小表示任务的优先级越高\nNI nice值，负值表示高优先级，正值表示低优先级\nVIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES\nRES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA\nSHR 共享内存大小，单位kb\nS 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程\n%CPU 上次更新到现在的CPU时间占用百分比\n%MEM 进程使用的物理内存百分比\nTIME+ 进程使用的CPU时间总计，单位1/100秒\nCOMMAND 命令名/命令行\nhtop 第一行 (Tasks, thr, running)\n任务(tasks)是打开的进程总数的代表，但并不是每个打开的进程都在不断消耗CPU。 每个进程都处于几种状态\nR: Running：表示进程(process)正在使用CPU\nS: Sleeping: 通常进程在大多数时间都处于睡眠状态，并以固定的时间间隔执行小检查，或者等待用户输入后再返回运行状态。\nT/S: Traced/Stoped: 表示进程正在处于暂停的状态\nZ:Zombie or defunct:已完成执行但在进程表中仍具有条目的进程。\n· 为什么休眠任务这么多 在系统监视工具（如 top 或 htop）中，大多数任务处于休眠状态通常是正常现象。这是因为休眠状态表示这些进程当前没有活动，它们正在等待某些事件的发生，例如：\n等待 I/O 操作完成：大多数休眠的进程通常是等待某些 I/O 操作完成，比如文件读写、网络请求、数据库查询等。这是常见的情况，因为 I/O 操作通常比 CPU 计算慢得多，而进程在等待 I/O 完成时会被挂起。\n等待用户输入：如果进程需要用户输入或交互，它们可能会休眠，直到用户提供所需的信息。\n等待定时器或信号：一些进程可能会休眠，等待特定的时间间隔或信号触发，以执行下一步操作。\n休眠时期：某些守护进程或服务进程可能会在休眠时期，以降低系统资源消耗，等待需要执行的任务。\n等待资源可用性：如果进程需要特定的资源，例如内存、锁或其他资源，而这些资源当前不可用，它们可能会休眠，直到资源可用。\n在多任务操作系统中，休眠状态是一种有效的资源管理方式。它使得操作系统能够有效地分配资源给正在运行的进程，同时节省系统资源以便在需要时分配给其他进程。因此，许多休眠的进程通常不是问题，除非您注意到某些进程一直保持休眠状态，而且这可能与性能问题有关。在这种情况下，您应该检查这些特定进程以找出原因，并根据需要进行进一步的诊断和优化。\n· IO负载过高 使用iostat命令查看IO负载，发现用来存储数据的sda盘负载过高，将数据挪到其他盘中。\n \n CPU：all 表示统计信息为所有 CPU 的平均值。\n%user：显示在用户级别(application)运行使用CPU 总时间的百分比。\n%nice：显示在用户级别，用于nice操作，所占用CPU 总时间的百分比。\n%system：在核心级别(kernel)运行所使用CPU 总时间的百分比。\n%iowait：显示用于等待I/O操作占用CPU 总时间的百分比。\n%steal：管理程序(hypervisor)为另一个虚拟进程提供服务而等待虚拟 CPU 的百分比。\n%idle：显示 CPU 空闲时间占用CPU 总时间的百分比（同top命令第三行id）\n若 %iowait 的值过高，表示硬盘存在I/O瓶颈\n若 %idle 的值高但系统响应慢时，有可能是CPU 等待分配内存，此时应加大内存容量\n若 %idle 的值持续低于10，则系统的CPU 处理能力相对较低，表明系统中最需要解决的资源是CPU。\n使用sar命令监控CPU：\n$ sar -u 1 3 每秒钟一次总共三次  Linux 5.15.0-86-generic (asus-RS720-E10-RS12) 2023年10月18日 _x86_64_ (128 CPU)  16时17分50秒 CPU %user %nice %system %iowait %steal %idle 16时17分51秒 all 0.91 0.00 0.64 92.60 0.00 5.85 16时17分52秒 all 0.72 0.00 0.33 92.15 0.00 6.80 16时17分53秒 all 0.89 0.00 0.63 93.11 0.00 5.37 Average: all 0.84 0.00 0.53 92.62 0.00 6.01 要判断系统瓶颈问题，有时需几个 sar 命令选项结合起来\n  怀疑CPU存在瓶颈，可用 sar -u 和 sar -q 等来查看\n  怀疑内存存在瓶颈，可用 sar -B、sar -r 和 sar -W 等来查看\n  怀疑I/O存在瓶颈，可用 sar -b、sar -u 和 sar -d 等来查看\n  参考资料   TOP命令参数详解\n  基于sar分析磁盘IO性能\n ","date":"October 18, 2023","image":null,"permalink":"/post/2023-10-18_highio/","title":"【实验室服务器】——检查限制任务运行原因【top部分均来源于转载】"},{"categories":["生信工具"],"contents":"我的学习目的：模仿Picard写一个对BAM文件进行处理的生信工具。【思考了一下，感觉这个工具没必要写，就简单记录一下学的东西吧】\n介绍 由于我想要的功能目前的工具都无法做到，所以我想模仿Picard去写一个处理BAM文件的工具。\n为什么选择Java？\n因为Picard就是使用java写的，使用gradle来构建，使用HTSJDK Java库HTSJDK，以支持对SAM和VCF等高通量测序数据常用的文件格式的访问。\n开发步骤  准备  java-17已经安装好了，还需要下载gradle。下载来源：https://gradle.org/install/\nwget https://services.gradle.org/distributions/gradle-8.4-all.zip unzip gradle-8.4-all.zip  export PATH=$PATH:/home/tzy/tools/gradle-8.4/bin gradle -v  cd gradle-8.4 $ bin docs init.d lib LICENSE NOTICE README src 创建名为upbam的工具  mkdir upbam cd upbam gradle init --type java-application 选项如下：\nStarting a Gradle Daemon (subsequent builds will be faster)  Select build script DSL:  1: Kotlin  2: Groovy Enter selection (default: Kotlin) [1..2] 2  Select test framework:  1: JUnit 4  2: TestNG  3: Spock  4: JUnit Jupiter Enter selection (default: JUnit Jupiter) [1..4] 2  Project name (default: gradle-8.4): Upbam Source package (default: upbam): upbam Enter target version of Java (min. 7) (default: 17): 17 Generate build using new APIs and behavior (some features may change in the next minor release)? (default: no) [yes, no]   \u0026gt; Task :init To learn more about Gradle by exploring our Samples at https://docs.gradle.org/8.4/samples/sample_building_java_applications.html  BUILD SUCCESSFUL in 4m 2 actionable tasks: 2 executed 创建文件夹mkdir -p /home/tzy/tools/upbam/src/main/java，在java目录下写入测试函数HelloWorld.java，注意java文件名要和class的名称一样。  HelloWorld.java：\npublic class HelloWorld {  public static void main(String[] args) {  System.out.println(\u0026#34;Hello, World!\u0026#34;);  } } 在upbam文件夹下创建build.gradle文件  plugins {  id \u0026#39;java\u0026#39; }  repositories {  jcenter() }  dependencies {  implementation \u0026#39;com.google.guava:guava:30.1.1-jre\u0026#39; }  apply plugin: \u0026#39;application\u0026#39;   application {  mainClassName = \u0026#39;upbam.HelloWorld\u0026#39; }  java {  sourceCompatibility = \u0026#39;17\u0026#39;  targetCompatibility = \u0026#39;17\u0026#39; }  manifest {  description = \u0026#39;test\u0026#39; }  tasks.withType(Jar){  manifest {  attributes \u0026#39;Main-Class\u0026#39;: \u0026#39;HelloWorld\u0026#39;,  \u0026#39;Implementation-Title\u0026#39;: \u0026#39;upbam\u0026#39;,  \u0026#39;Multi-Release\u0026#39;: \u0026#39;true\u0026#39;  } } 使用gradle构建  gradle build 调用工具  cd /home/tzy/tools/upbam java -jar ./build/libs/upbam.jar 参考资料  DownsampleSam (Picard) ","date":"October 17, 2023","image":null,"permalink":"/post/2023-10-17_javahelloworld/","title":"生信工具开发——Java【1】helloworld"},{"categories":["生信工具"],"contents":"介绍一些基本概念。\n源文件，类，方法  源文件（source file），扩展名为.java，是带有类的定义。类用来表示程序的一个组件，程序或许只有一个类。类的内容必须在括号内。  类：\npublic class Dog {  }  类（class），类中有一个或多个方法。在Dog这个类中，bark方法带有如“汪汪”的指令。方法必须在类的内部声明。  public class Dog {  void bark () {   } } 在 Java 中，void 是一种返回类型（return type），用于表示一个方法不返回任何值。具体来说，当一个方法的返回类型被声明为 void 时，它表示该方法执行后不会返回任何数据或值给调用者。\n 方法  在方法的括号中编写方法应该执行的指令。方法代码是由一组语句(statement)组成的，可以把方法想象成一个函数或过程。\npublic class Dog (  volid bark () {  statement1;  statement2;  } ) 如何编写和执行Java程序 java中所有的东西都会属于某个类。建立源文件（扩展名为.java），然后将其编译为新的类文件（扩展名为.class），真正被执行的是类。\n不管程序有多大（不管有多少个类），一定会有一个main()来作程序的起点。\npublic class MyFirstApp {   public static void main (String[] args) {  System.out.printIn(\u0026#34;I Rule!\u0026#34;);  System.out.printIn(\u0026#34;The World\u0026#34;);  } } 将上述代码保存为MyFirstApp.java，编译javac MyFirstApp.java，运行\n%java MyFirstApp I Rule! The World JVM和JDK  java虚拟机（JVM）  Java虚拟机是Java的核心技术，它使Java程序能够跨平台运行，并提供了许多重要功能，如内存管理、多线程支持和安全性，以支持Java应用程序的执行。著名的JVM实现包括Oracle HotSpot、OpenJ9、GraalVM等。JVM的主要目标是实现Java的\u0026quot;一次编写，到处运行\u0026quot;（Write Once, Run Anywhere）的理念，使开发人员能够编写Java应用程序，而这些应用程序可以在不同操作系统上无需任何修改就能运行。\n java编译器（JDK）  Java编译器是一种用于将Java源代码文件（通常以.java为扩展名）转换为字节码文件（通常以.class为扩展名）的工具。它是Java编程语言中的一个关键组件，用于将高级的Java源代码翻译成可以在Java虚拟机（JVM）上运行的字节码。\n标准的Java编译器是 javac，它是Java Development Kit（JDK）的一部分，可以从Oracle或其他JDK提供商获得。编译Java代码的基本语法是在命令行中使用 javac 命令：\njavac YourJavaFile.java 面对对象的继承（inheritance） 举个例子：要求对正方形，圆形，三角形，和阿米巴虫，实现点击图像后旋转并播放对应音效。可以通过程序写4个类：\n \n 由于他们都具有rotate和playSound，这4个类可以同属于Shape类，提取出Shape类：\n \n 可以将4个形状以“继承”的关系连接到Shape这个类：\n \n 可以称为“Square继承自Shape”、“Circle继承自Shape”等等，rotate和playSound从次级的类中移开。Shape是下面4个类的父类，子类会自动获得父类的功能。\n增加条件：阿米巴虫的旋转方法是特殊的，通过该子类重新定义集成方法，以改变或延伸此方法的行为称为”覆盖“。\n \n 类和对象的异同 对象是靠类的模型塑造出来的。对象本身已知的事物被称为实例变量（instance variable），对象可以执行的动作称为方法。类会告诉虚拟机创建某种类型的对象。举例：通讯录里的每张卡片都有相同的空白字段标注姓名、电话、邮件，它们是实例变量，填入新的联络人就仿佛创建新的实例（对象）。卡片类上的方法就是对卡片做的事情，比如getTel(),changeAddress(),deleteCard()等等。每张卡片能够执行相同的动作，但是得到的结果是各自独立的。\n参考资料  Head First Java - Kathy Sierra \u0026amp; Bert Bates ","date":"October 17, 2023","image":null,"permalink":"/post/2023-10-17_java1/","title":"生信工具开发——Java【2】"},{"categories":["生信工具"],"contents":"我的学习目的：开发工具的过程中需要去对深测序和浅测序样本进行转换。\n介绍 通过阅读文献，看到了两种方法去对基因组数据降采样：\n  对reads数据进行降采样，通常输入是BAM或SAM文件，比如使用Picard这样的工具去做。\n  对mutation数据进行降采样，文献中没有给出方法[1]。\n  以及一个问题：为什么我们需要降采样？可能是：\n 这次使用深度测序，下次是否通过更浅的测序可以得到类似的结果，从而降低成本？  Picard下载 git clone https://github.com/broadinstitute/picard.git cd picard/ 使用gradle来build Picard：\n./gradlew shadowJar 在安装过程中，我最初使用的是java11，而Picard更新后要求使用java17：\nsudo apt-get install openjdk-17-jdk 调用：\njava -jar build/libs/picard.jar 使用Picard 使用Picard对SAM或BAM文件进行降采样，随机保留reads的子集，从相同的模板得到的reads同时被保留或者丢弃，来满足reads是从一定概率的输入模板中得到的，这个条件。该工具提供不同的降采样策略：\n  ConstantMemory：采用hash-project strategy来限制内存，由于降采样是随机的，所以真实保留比例是围绕给定限定比例变化的。由于内存固定，适合大的输入，由于随机性，在数量多的地方准确，少的地方准确度下降。\n  HighAccuracy：尽可能接近指定的比例，需要和传入reads流中模板名称输入成比例的内存，对于大的输入，需要很大的内存。\n  Chained：综合上述两个工具的优点。对于数据量比较大的情况（比如上百万的reads）很准确，对于小的输入，推荐HighAccuracy。\n  例子1: 保留2%的reads\njava -jar picard.jar DownsampleSam \\ I=input.bam \\ O=downsampled.bam \\ STRATEGY=Chained \\ P=0.02 \\ ACCURACY=0.0001 例子2: 默认参数保留2%的reads\njava -jar picard.jar DownsampleSam \\ I=input.bam \\ O=downsampled.bam \\ P=0.02 例子3: 精确度很高地默认参数保留0.001%的reads（需要很大的内存）\njava -jar picard.jar DownsampleSam \\ I=input.bam \\ O=downsampled.bam \\ STRATEGY=HighAccuracy \\ P=0.00001 \\ ACCURACY=0.0000001 由作者进行了比较，和samtools相比，Picard节约了80%的时间。\n我的任务：把一个平均深度为50X的pcawg样本降到2X。downsample.sh：\n#!/bin/bash # 记录开始时间 start_time=$(date +%s)  input=/home/data/sda/tzy/pcawg/73c4a73954614dda9be097dfb8305bd8.bam output=/home/data/sda/tzy/pcawg/downsamp_73c4a73954614dda9be097dfb8305bd8.bam  java -jar /home/tzy/tools/picard/build/libs/picard.jar DownsampleSam \\ I=$input \\ O=$output \\ P=0.04  # 记录结束时间 end_time=$(date +%s)  # 计算总时间 elapsed_time=$((end_time - start_time))  echo \u0026#34;进程执行时间为 $elapsed_time秒\u0026#34; run:\nnohup bash downsample.sh \u0026gt; nohup.downsample.out \u0026amp; 总时长花了2个小时左右。\n批量并行 任务目标：\n depth_level记录了降采样目标：0.5X, 1X, 2X, 5X, 10X  0.5 1 2 5 10  ERR174341.ave_depth.txt 记录的是samtools获得的深度，文件内容是  Average depth: 11.5458  想通过depth_level除以ERR174341.ave_depth.txt中的深度，来分别降采样，并以新的降采样深度命名。  data=\u0026#34;/home/data/sdb/tzy/NA12878/raw_data/ERR174341\u0026#34; depth=\u0026#34;/home/tzy/project/CNbenchmark/script/ERR174341/depth_level\u0026#34; average_file=\u0026#34;/home/data/sdb/tzy/NA12878/raw_data/ERR174341/ERR174341.ave_depth.txt\u0026#34;  export data export depth export average_file  # 提取 ERR174341.ave_depth.txt 文件中的数字部分 average=$(grep -oE \u0026#39;[0-9]+([.][0-9]+)?\u0026#39; \u0026#34;${average_file}\u0026#34;) export average  function parallel_down { start_time=$(date +%s) echo \u0026#34;This is $1\u0026#34;  number=$(echo \u0026#34;$1\u0026#34; | awk -v num=\u0026#34;${average}\u0026#34; \u0026#39;{ if (num != 0) printf \u0026#34;%.2f\\n\u0026#34;, $1/num }\u0026#39;) echo \u0026#34;$number\u0026#34;  java -jar /home/tzy/tools/picard/build/libs/picard.jar DownsampleSam \\ I=$data/ERR174341.hg19.bam \\ O=$data/ERR174341.hg19.$1.bam \\ P=${number}  # 记录结束时间 end_time=$(date +%s)  # 计算总时间 elapsed_time=$((end_time - start_time))  echo \u0026#34;进程执行时间为 $elapsed_time 秒\u0026#34; } export -f parallel_down  parallel --link parallel_down :::: ${depth} 最终并行生成以下文件，且不需要人为指定DownsampleSam中的P参数（P=目标深度/当前深度）\nERR174341.hg19.0.5.bam ERR174341.hg19.1.bam ERR174341.hg19.2.bam ERR174341.hg19.5.bam ERR174341.hg19.10.bam 参考资料   DownsampleSam (Picard)\n  [1] Gulhan, D.C., Lee, J.J.K., Melloni, G.E., Cortes-Ciriano, I. and Park, P.J., 2019. Detecting the mutational signature of homologous recombination deficiency in clinical samples. Nature genetics, 51(5), pp.912-919.\n  Picard和samtools比较\n  Picard下载\n ","date":"October 13, 2023","image":null,"permalink":"/post/2023-10-13_downsample/","title":"基因组数据降采样"},{"categories":["生信工具"],"contents":"NA12878样本常被用来作为benchmark的参考，介绍一下该样本基因组序列的下载和处理。\n1. 下载  下载NA12878的cram文件（该网站上只有cram文件，需要后续根据使用的参考基因组转成bam文件）：https://www.internationalgenome.org/data-portal/sample/NA12878  #!/bin/bash # all file download from https://www.internationalgenome.org/data-portal/sample/NA12878  ## [1] low coverage WGS data # cram and crai wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/CEU/NA12878/alignment/NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram  # fasta file # wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR622/SRR622461/SRR622461_2.fastq.gz # wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR622/SRR622461/SRR622461_1.fastq.gz  下载参考基因组（一定要下载对应的，否则转成的bam文件在后续运行时会出现文件不完整的报错）  wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/20150713_location_of_centromeres_and_other_regions.txt /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla-extra.fa /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.dict /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.alt /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.amb /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.ann /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.bwt /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.pac /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.sa /home/data/sda/tzy/NA12878/GRCh38_reference_genome wget -c https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/README.20150309.GRCh38_full_analysis_set_plus_decoy_hla /home/data/sda/tzy/NA12878/GRCh38_reference_genome  使用samtools检测文件完整性，如果文件完整则不输出内容：  \u0026gt; samtools quickcheck NA12878.bam NA12878.bam was missing EOF block when one should be present. 2. 转化 转成bam文件\n#!/bin/bash samtools view -@ 120 -T /home/data/sda/tzy/NA12878/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa -b /home/data/sda/tzy/NA12878/NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram -o /home/data/sda/tzy/NA12878/NA12878.bam 3. 计算深度 #!/bin/bash cd /home/data/sda/tzy/NA12878/ samtools depth -@ 120 -a /home/data/sda/tzy/NA12878/NA12878.bam \u0026gt; /home/data/sda/tzy/NA12878/depth.txt awk \u0026#39;{sum += $3} END {print \u0026#34;Average depth: \u0026#34;, sum/NR}\u0026#39; /home/data/sda/tzy/NA12878/depth.txt \u0026gt; /home/data/sda/tzy/NA12878/ave_depth.txt 注意这个深度的计算应该是针对sorted的bam文件。\n4. 生成bai文件 #!/bin/bash samtools index -@ 120 /home/data/sda/tzy/NA12878/NA12878.bam 5. 根据工具需要处理 freec需要每个染色体的fasta文件\nfor i in `cut -f1 GRCh38_full_analysis_set_plus_decoy_hla.fa.fai`;do samtools faidx GRCh38_full_analysis_set_plus_decoy_hla.fa $i \u0026gt; ./chr/$i.fa;done samtools faidx GRCh38_full_analysis_set_plus_decoy_hla.fa HLA-C\\*02:11 \u0026gt; chr/HLA-C*02:11.fa # 不知道什么情况，这个区域只能手动生成，总是识别不出来 6. 判断单端还是双端测序 返回0就是单端，1就是双端\nsamtools view -h NA12878.bam | head -n 100 | samtools view -c -f 1 参考资料  官方下载地址 ","date":"October 12, 2023","image":null,"permalink":"/post/2023-10-11_na12878/","title":"NA12878样本下载和处理"},{"categories":["生信工具"],"contents":" 打开ICGC的DATA REPOSITORIES筛选和查找自己需要的数据。比如，我想要下载PCAWG中icgc_specimen_id为SP117655的BAM文件，在数据库中进行检索是搜不到的，但是会出来对应的donor名，可以根据donor名再检索一次：   \n  题外话：如果想要确认Donor id，或者查看其他的id比如aliquot_id等等，可以在该页面点击DCC DATA RELEASES \u0026gt; PCAWG \u0026gt; data_releases \u0026gt; latest \u0026gt; pcawg_sample_sheet.v1.4.2016-09-14.tsv，或者选一个最新的样本表格下载。\n \n 后续如果不使用ICGC提供的vcf文件的话，在使用Mutect2提取vcf文件过程中，会遇到的一个问题：\nA USER ERROR has occurred: Bad input: Sample 80ae5df80bd3e08383ed80f48e4dab58 is not in BAM header: [1e2dcbcc-771c-43c5-8c8d-e0eb77cb3494, 8205d14c-6f7e-4592-b8e1-76e3bc5f9613] 80ae5df80bd3e08383ed80f48e4dab58是我们下载下来的文件的前缀，通过查询文件内容可知，这不是真正的样本名：\nsamtools view -h 8205d14c-6f7e-4592-b8e1-76e3bc5f9613.bam | head -n 90 | grep \u0026#39;@RG\u0026#39; @RG ID:CACR:NA096_1 PL:ILLUMINA CN:CACR PI:170 DT:2014-04-16T00:00:00+00:00 LB:WGS:CACR:WHC107SM:8205d14c-6f7e-4592-b8e1-76e3bc5f9613 PU:CACR:1_4 PG:fastqtobam @RG ID:CACR:NA096_2 PL:ILLUMINA CN:CACR PI:170 DT:2014-04-16T00:00:00+00:00 LB:WGS:CACR:WHC107SM:8205d14c-6f7e-4592-b8e1-76e3bc5f9613 PU:CACR:1_5 PG:fastqtobam @RG ID:CACR:NA096_3 PL:ILLUMINA CN:CACR PI:170 DT:2014-04-16T00:00:00+00:00 LB:WGS:CACR:WHC107SM:8205d14c-6f7e-4592-b8e1-76e3bc5f9613 PU:CACR:1_6 PG:fastqtobam 需要把样本名进行更改\nmv 80ae5df80bd3e08383ed80f48e4dab58.bam 8205d14c-6f7e-4592-b8e1-76e3bc5f9613.bam  根据DATA TYPE选中Aligned Reads得到BAM文件，这里有四个BAM文件，前两个只有几百M，后两个是几百G，前者下载的是mini bam，看它的覆盖度可能是浅全基因组测序的结果，后者是深全基因组测序的结果。注意四个文件中有两个是正常配对样本。这里根据我的任务需求把这四个全部选中点下载。   \n 由于我使用的是score-client工具，Repository选择Collaboratory - Toronto。   \n tar xvzf解压下载的文件，manifest.collaboratory.1697079740643.tsv，内容如下(仅展示了前两行)：  repo_code\tfile_id\tobject_id\tfile_format\tfile_name\tfile_size\tmd5_sum\tindex_object_id\tdonor_id/donor_count\tproject_id/project_count\tstudy collaboratory\tFI42053\tfb96fd5f-5a6f-5fc7-baf9-2fd8f43591df\tBAM\t73c4a73954614dda9be097dfb8305bd8.bam\t128476988498\t73c4a73954614dda9be097dfb8305bd8\t4634f96e-7e96-5c0d-9cc8-f08f9a305001\tDO218440\tBTCA-SG\tPCAWG 下载安装配置工具score-client  安装java11：\napt-get install openjdk-11-jdk 下载工具：\nwget -O score-client.tar.gz https://artifacts.oicr.on.ca/artifactory/dcc-release/bio/overture/score-client/%5BRELEASE%5D/score-client-%5BRELEASE%5D-dist.tar.gz  tar xvzf score-client.tar.gz  cd score-client-5.9.0 配置，修改conf/application.properties文件，写入token即可（如果是controlled数据，除了有账号，还需要完成数据申请）。\naccessToken=your token 下载数据  bin/score-client --profile collab download --manifest ./manifest.collaboratory.1697079740643.tsv --output-dir /home/data/sda/tzy/pcawg 开始下载。\n参考资料   ICGC数据下载官方文档\n  ICGC工具官方文档\n ","date":"October 12, 2023","image":null,"permalink":"/post/2023-10-11_icgcdownload/","title":"使用score-client下载ICGC数据"},{"categories":["生信工具"],"contents":"对模型的泛化性能进行评估，需要有衡量模型泛化能力的评价标准。\n按照学习任务分为回归任务和分类任务。回归任务的性能度量通常有均方误差，分类任务的性能度量通常有错误率准确率等等。\n1. 回归任务的性能度量 设模型预测结果\\( f(x) \\)与真实标记y进行比较，计算均方误差（mean squared error, MSE）：\n$$ E(f;D)=\\frac{1}{m} \\sum_{m}^{i=1}(f(x_{i})-y_{i})^{2} $$\n对于数据分布D和概率密度函数\\( p(\\cdot ) \\)，均方误差可描述为：\n$$ E(f;D)=\\int_{x\\sim D}^{} (f(x)-y)^{2}p(x)\\mathrm{d}x $$\n2. 分类任务的性能度量 2.1 PR曲线    真实情况 预测结果(正例) 预测结果(反例)     正例 TP（真正例） FN（假反例）   反例 FP（假正例） TN（真反例）    查准率（precision，P）：\n$$ \\frac{TP}{TP+FP} $$\n查全率（recall，R）：\n$$ \\frac{TP}{TP+FN} $$\n查准率和查全率是一对矛盾的度量，往往此高彼低，下面介绍查准率-查全率曲线（P-R曲线），根据模型的预测结果对样例进行排序，排在最前的是模型认为“最可能”是正例的样本，按此顺序逐个把样本作为正例进行预测，每次可以计算出当前的查全率、查准率，以查准率为纵轴、查全率为横轴作图，得到PR曲线。\n \n PR曲线直观的展示模型在总体上的查全率和查准率，那么如何比较PR曲线？通常情况下认为曲线下面积大的更准确，由于不容易估算，还可以通过“平衡点”（Break-Even Point，简称BEP）来进行度量，是查全率=查准率时的取值，图中认为模型A优于B。\n2.2 F1 score F1是基于查准率和查全率的调和平均定义的，F1度量的计算公式：\n$$ F1 = \\frac{ 2 }{\\frac{1}{P} + \\frac{1}{R}} = \\frac{2\\times P\\times R}{P+R} = \\frac{2\\times TP}{样例总数+TP-TN} $$\n不同的任务中对查准率和查全率的重视程度不同，比如查找罪犯则希望查全率更重要，尽可能少漏掉逃犯。F1度量的一般形式\\( F_{\\beta} \\)，\\( F_{\\beta} \\)是加权调和平均，展示对查准率/查全率的不同偏好，定义为：\n$$ F_{\\beta} = \\frac{ 1+\\beta^{2} }{\\frac{1}{P} + \\frac{\\beta^{2}}{R}} =\\frac{(1+\\beta^{2}) \\times P \\times R }{(\\beta^{2} \\times P)+R}\n$$\n其中\\( \\beta \u0026gt; 0\\)度量了查全率对查准率的相对重要性，\\( F_{\\beta}=1 \\)时退化为标准的\\( F1 \\)，\\( \\beta \u0026gt; 1\\)时查全率有更大的影响，\\( \\beta \u0026lt; 1\\)时查准率有更大的影响。\n注意：相对于算术平均\\( \\frac{P+R}{2} \\)和几何平均\\( \\sqrt{P\\times R} \\)，调和平均更重视较小值。\n针对多个二分类混淆矩阵的情况，比如多次训练或测试，每个得到一个混淆矩阵；多个数据集上训练或测试，估计算法的“全局”性能；执行多分类任务，每两两类别的组合都对应一个混淆矩阵等等。具有以下几种做法：\n做法1. 直接在各个混淆矩阵上分别计算查准率和查全率，记为\\( (P_{1}，R_{1}) \\)，\\( (P_{2}，R_{2}) \\)，\u0026hellip;，\\( (P_{n}，R_{n}) \\)，再计算平均值，得到宏查全率（macro-R）和宏查准率（macro-P），相应的得到宏F1（macro-F1）。\n做法2. 先将各混淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，在基于这些平均值计算出微查全率（micro-R），微查准率（micro-P），微F1（micro-F1）。\n【多分类任务的评价标准？】\n2.3 ROC和AUC 很多模型为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，大于阈值的为正类，小于阈值的为反类，根据这个实值或预测结果可以直接将测试样本进行排序，“最可能”是正例的排在最前面，分类过程就相当于在这个排序中以某个“截断点”（cut point）将样本分为两个部分，前部分是正例，后部分为反例。不同的应用任务中，根据任务需求来采用不同的截断点，重视查准率就选排序靠前的，重视查全率就选排序靠后的。比如神经网络在一般情况下对每个测试样本预测出一个[0.0,1.0]之间的实值，然后将这个值和0.5比较，大于0.5的判为正例，否则为反例，这个实值或概率预测结果的好坏，直接决定了模型的泛化能力。\n真正例率（True Positive Rate，TPR）：\n$$ TPR = \\frac{ TP }{TP+FN} $$\n假正例率（False Positive Rate，FPR）：\n$$ FPR = \\frac{ FP }{TN+FP} $$\nROC曲线的纵轴是：”真正例率“，横轴是：”假正例率“，根据模型结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出TPR和FPR作图，得到ROC曲线。图中对角线对应于”随机猜测“模型，点（0，1）对应于将所有正例排在所有反例之前的”理想模型“。\n \n 图中右图是利用有限个测试样例来绘制ROC图，坐标对有限，因此无法产生左图中光滑的ROC曲线，绘图过程：\n  给定\\( m^{+} \\)个正例和\\( m^{-} \\)个反例，根据模型预测结果对样例进行排序\n  把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标（0，0）处标记一个点\n  将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为（x,y），当前若为真正例，对应标记点坐标为（x,y+\\( m^{+} \\)），若当前为假正例，对应标记点的坐标为（x+\\( m^{-} \\),y）\n  用线段连接相邻点得到ROC曲线\n  AUC（Area Under ROC Curve）是ROC曲线下面积。AUC可以通过对ROC曲线下各个部分的面积求和而得，假设ROC曲线是由坐标为{(\\( x_{1} \\),\\( y_{1} \\)),(\\( x_{2} \\),\\( y_{2} \\)),\u0026hellip;,(\\( x_{m} \\),\\( y_{m} \\))}的点连接而成，AUC可以估算为：\n$$ AUC=\\frac{1}{2}\\sum_{m-1}^{i=1}(x_{i+1}-x_{i})\\cdot (y_{i}+y_{i+1}) $$\n \n 一个模型AUC数值大于另一个模式AUC数值，认为从这个角度前者模型优于后者。\n2.3 代价敏感错误率与代价曲线 解决实际问题中，不同类型的错误造成的后果不同，可以为错误赋予“非均等代价”（unequal cost），上述的性能度量中都默认假设了均等代价，关注的是错误次数。非均等代价下希望的不是最小化错误次数，而是最小化“总体代价”。\n \n 参考资料   向单元格中添加表格\n  [机器学习-周志华]图片和内容均来自于此\n ","date":"October 11, 2023","image":null,"permalink":"/post/2023-10-08_ml/","title":"机器学习——性能度量"},{"categories":["生信工具"],"contents":"罗列生存分析中最常用的2个绘图方法：\n  KM曲线\n  cox分析\n  以下内容主要来源于《医学统计学基础》姜晶梅 主编，在此基础上按照自己的理解习惯稍作修改。\n1. 概念引入 在研究中，观察结果在短期内不确定，研究者不仅关心事件的发生与否（比如肿瘤是否转移），还关心事件发生所经历的时间，同时考虑“事件”和“时间”。另外还存在失访从而无法获得事件发生的准确时间。\n  起始事件（initial event）：标志研究对象生存过程开始的事件，比如疾病确诊或首次用药。根据研究目标确定。\n  终点事件（endpoint event/failure event）：研究对象的特定结局，比如肿瘤转移。根据研究目标确定。\n  生存时间（survival time/failure time）：从起始事件到终点事件所经历的时间称为生存时间。\n  完全数据（complete data）：终点事件发生在研究结束前，并且能够得到准确的生存时间称为完全数据。\n  删失数据（censored data/incomplete data）：由于各种原因未能观察到研究对象的终点事件，从而无法获知研究对象的准确生存时间，称之为“删失”。删失分为左删失和右删失两种情况，右删失指在终点事件发生在最后一次随访时间的右方，个体的真实生存时间长于观察到的生存时间，左删失指的是个体的生存时间短于观察到的生存时间。\n  删除的原因主要有三种：\n  失访（loss to follow-up）：失去联系（研究对象主动退出，无应答等等）或者由于其他原因死亡而未观察到规定的终点事件。\n  患者的生存时间超过了预先设定的研究终止期。\n  动物实验中，有时预先规定多少个动物发生了终点事件就中止实验，实验停止时，有一部分动物没有出现终点事件，也属于删失数据。\n  生存时间是一个随机变量（常用\\( T \\)表示），常用的统计量：\n 概率密度函数：生存时间是随机变量，因此有相应的概率分布。假定生存时间T服从含有未知参数的某种分布，概率密度函数表示为：  f(t)=lim△t→0P{t≤T\u0026lt;t+△t}△t =lim△t→0P{个体在[t,t+△t)内发生终点事件}△t\n\\( \\bigtriangleup t \\)是指一段非常小的时间区间，生存时间的概率密度函数表示在某时刻t终点事件发生的瞬时速率。\n生存函数：概率密度函数\\( f(t) \\)的累积形式称为生存函数（survival function），也称累积生存概率或生存率。生存函数用\\( S(t) \\)表示，是指个体生存时间\\( T \\)大于t的可能性，表达式为：  \\( S(t) \\)是对生存过程的描述，表示没有发生终点事件的累积概率，该函数随着时间t而减小，\\( S(0)=1 \\)，\\( S(\\infty)=0 \\)\n实际计算时一般如下估计：\n$$ S(t)=\\frac{生存时间T\u0026gt;t的个体数}{随访个体总数} $$\n生存曲线：以生存时间t为横轴，以\\( S(t) \\)为纵轴，讲各个时间点所对应的生存率连接在一起绘制成的曲线为生存曲线。理论上，生存曲线是一条平滑曲线，由于实际数据分析中常采用非参数方法估计生存率，所以生存曲线多呈现阶梯形状。   \n 风险函数（hazard function）：指研究对象活过t时刻后，在t到\\( t + \\bigtriangleup t \\)这一段很短时间内终点事件发生概率和\\( \\bigtriangleup t \\)之比的极限值，一般用\\( h(t) \\)表示。计算公式为：  h(t)=lim△t→0P{t≤T\u0026lt;（t+△t）|T≥t}△t\n含义为生存时间已达t的个体，在t时刻发生终点事件的瞬时风险率，反应的是速率变化的快慢而不是概率值，因此风险函数又称为条件死亡率，瞬时死亡率等等。\n生存函数是对生存过程的描述，表示没有发生终点事件的累积概率，与生存函数所呈现的信息相反，风险函数\\( h(t) \\)表示了终点事件发生的瞬时风险，通常反映某疾病对生命的影响过程，\\( h(t) \\)的取值范围是\\( [0,\\infty) \\)，实际计算时，\\( h(t) \\)如下估计：\n$$ h(t)=\\frac{在[t,t+\\bigtriangleup t)内发生终点事件的个体数}{在t时刻尚存活的个体数\\times \\bigtriangleup t} $$\n\\( h(t) \\)可以是常量或者增函数、减函数等等。常见的\\( h(t) \\)如下所示：\n【图片】\n\\( S(t) \\)，\\( f(t) \\)和\\( h(t) \\)是紧密相连的，对\\( h(t) \\)进行数学变换：\n \n 2. 生存率的估计 为了了解生存资料的特征，在获得生存数据后首先对其进行描述性分析，包括参数方法和非参数方法。参数方法可以借助现有的理论分布来描述样本生存时间，如指数分布、Weibull分布、Gamma分布、对数正态分布、线性指数分布和Gompertz分布等。但由于随访资料中生存时间的分布难以确定，并经常存在删失数据，限制了参数方法在实践中的应用。非参数方法在生存资料的描述中更方便实用。\n2.1 乘法极限法（KM曲线） 乘积极限法是针对随机删失数据估计生存函数的非参数方法，又称为Kaplan-Meier method（KM），可用于右删失数据的处理，主要应用于小样本未分组资料，也可以用于大样本生存数据。\n乘积极限法的基本原理是利用条件概率的乘法原理来估计生存率，假设将终点事件发生的时间按照从小到大排序\\( t_{1} \u0026lt; \u0026hellip;\u0026lt; t_{D} \\)，\\( Y_{i} \\)表示\\( t_{i} \\)时刻刚开始的个体数，\\( d_{i} \\)表示\\( t_{i} \\)时刻发生的终点事件数，\\( d_{i}/Y_{i} \\)是条件概率的估计值，即生存时间恰好达到\\( t_{i} \\)时刻的个体在\\( t_{i} \\)时刻发生终点事件的条件概率，则KM估计量计算公式为：\n \n KM估计量是一个阶梯函数，终点事件发生的时间点处是其跳跃间断点，以上计算的样本生存率是总体生存率的点估计值，生存率的（1-a）x100%置信区间计算公式为：\n \n  \n 示例数据 mdat \u0026lt;- matrix(  c(  37.5,0,\u0026#34;失访\u0026#34;,5,  44.3,1,\u0026#34;死亡\u0026#34;,5,  25.3,1,\u0026#34;死亡\u0026#34;,5,  18.1,1,\u0026#34;复发\u0026#34;,5,  56.7,0,\u0026#34;生存\u0026#34;,5  ),  nrow = 5,  ncol = 4,  byrow = TRUE,  dimnames = list(  c(\u0026#34;C1\u0026#34;, \u0026#34;C2\u0026#34;, \u0026#34;C3\u0026#34;,\u0026#34;C4\u0026#34;,\u0026#34;C5\u0026#34;),  c(\u0026#34;time\u0026#34;, \u0026#34;status\u0026#34;,\u0026#34;status.raw\u0026#34;,\u0026#34;group\u0026#34;))) %\u0026gt;%  as.data.frame()  km_fit \u0026lt;- survfit(Surv(time, status)~group, data = mdat)  km_fit %\u0026gt;%ggsurvplot(data = mdat,  fun = \u0026#34;pct\u0026#34;,  risk.table = TRUE,  fontsize = 3, # used in risk table  surv.median.line = \u0026#34;hv\u0026#34;, # median horizontal and vertical ref lines  ggtheme = theme_light(),  # title = \u0026#34;Kaplan-Meier Survival Function Estimate\u0026#34;,  legend.title = \u0026#34;\u0026#34;,  # legend.labs = levels(mdat$group) ) 对于生存资料，应用图形展示生存过程会更加直观。示例的计算结果以生存时间为横坐标，生存率为纵坐标绘制生存曲线：\n \n 根据公式计算生存率和标准误：\n \n  \n 生存曲线的每一级阶梯代表一个终点事件发生的时间点，图中的三个阶梯依次代表了4、3和2号患者的死亡/复发事件，阶梯对应的横坐标为终点事件发生的时间点。在删失时间点无阶梯，圆圈依次表示1号和5号患者删失，其对应的横坐标为删失时间点。由于本例数据中最大的生存时间点是删失数据，因此生存曲线不与横轴相交，否则生存曲线在生存时间点处与横轴相交。由于随着观察时间的增加，观察例数越来越少，一般曲线尾部显示的生存率可能不稳定。\n \n 注意一下这里的risk table里面，样本数对应的位置是（未删失+未发生事件）的样本数【这个画图的结果没有从5开始，有点奇怪】。\n2.2 寿命表法 样本量较大的时候【这里的样本量多大的时候需要用寿命表法呢，不手动计算生存率的情况下还有必要去使用寿命表法吗】，可以将生存时间的观察值范围划分为若干小区间，采用寿命表法（life-table method）计算生存函数。区间的划分方法不同，寿命表的计算结果也不同，一般情况下使用较小的时间单位准确性更高。\n乘法极限法与寿命表法本质相同，都是通过一系列条件概率的乘积来计算生存概率，二者的区别在于寿命表法是在小区间上估计条件概率，称发际线法是将观察到的时间点作为小区间的极限。\n3. 生存过程的比较 实践中经常会对两个或多个生存曲线进行比较，由于抽样误差的影响，还需要对其进行假设检验，检验方法有很多种，以下主要介绍常用的时序检验法（log-rank test）。\n3.1 时序检验 该检验属于非参数方法，比较的是整个生存时间的分布（整条生存曲线），并不是比较某个时间点上的生存率。时序检验的原假设H0：两条生存曲线相同，备择假设H1：两条生存曲线不同。时序检验的本质是大样本\\( X_{2} \\)检验，基本思想是：计算不同时间点上两组的期初观察人数及终点事件发生人数，并在H0条件下分别计算出两组在该时间点的期望终点事件发生人数，实际值与期望值一旦相差过大，则有理由认为这个差异不仅仅由抽样误差造成，即两个总体生存曲线有统计学差异。\n除了时序检验以外，还有Wilcoxon检验、Tarone-Ware检验、Peto检验等，他们的区别在于各时间点上终点事件发生数实际值和期望值的差赋予的权重不同。以两组生存曲线比较为例，检验统计量的一般形式：\n \n 上述这些方法都属于单因素分析，考虑多种预后因素的影响需要借助多因素分析方法，比如cox回归模型。\n参考资料  封面from hiplot ","date":"September 25, 2023","image":null,"permalink":"/post/2023-09-25_kmcurve/","title":"生存分析——KM曲线"},{"categories":["生信工具"],"contents":"使用ACE提取浅全基因组测序数据的拷贝数变异。\n输入文件 bam文件或者QDNAseq得到的对象\nACE可以实现\n 染色体片段，基因，突变片段的绝对拷贝数 样本中肿瘤细胞占比（肿瘤纯度）  结果的得到\n根据文章中的描述，该工具在不同倍性或给定倍性情况下计算fit error，当误差达到最小时，会给出不同倍性下的cellularity(当变量为倍性时)，再根据得到的cellularity和ploidy来计算绝对拷贝数。\n注意\n 注意肿瘤纯度最小值为5%。 从定义上看，“best fit”(误差最小)并不是最可能的匹配!  分析步骤 输入文件只支持两种，一种是bam文件，一种是QDNAseq得到的object\n  runACE\n  选择结果\n  选择最小error的fit结果，或者是最高cellularity的结果，这两者在summary_likelyfits文件中都可以找到。fitpicker.tsv文件中也有具体记录。\n另外一个penalty参数，该参数主要惩罚cellularity很低的情况，\n输出数据  上一步得到的都是图和fit误差以及倍性等计算的结果，这一步把数据进行输出。\ndata(\u0026#34;copyNumbersSegmented\u0026#34;) object \u0026lt;- copyNumbersSegmented # Since you\u0026#39;re convinced the mode is 3N, you can run the singlemodel function to # fit at ploidy = 3 model \u0026lt;- singlemodel(object, QDNAseqobjectsample = 2, ploidy = 3) ls(model) bam文件分析步骤\n# specify the directory containing your bam-files userpath \u0026lt;- tempdir() # if you do not want the output in the same directory, use argument outputdir runACE(userpath, filetype=\u0026#39;bam\u0026#39;, binsizes = c(100, 1000),  ploidies = c(2,4), imagetype=\u0026#39;png\u0026#39;) 片段化文件分析步骤：\n示例数据，runACE函数自动分析指定路径下所有的样本：\n选择模型\n推荐更大一点的bin大小\nThe most likely fit of a tumor is generally 1) the fit with the smallest error, or 2) the fit at the highest cellularity, so those two are presented in the summary_likelyfits file.\n或者fitpicker.tsv file看参数\ndata(\u0026#34;copyNumbersSegmented\u0026#34;) object \u0026lt;- copyNumbersSegmented model1 \u0026lt;- singlemodel(object, QDNAseqobjectsample = 1) bestfit1 \u0026lt;- model1$minima[tail(which(model1$rerror==min(model1$rerror)), 1)] besterror1 \u0026lt;- min(model1$rerror) lastfit1 \u0026lt;- tail(model1$minima, 1) lasterror1 \u0026lt;- tail(model1$rerror, 1) singleplot(object, QDNAseqobjectsample = 1, cellularity = bestfit1,  error = besterror1, standard = model1$standard,  title = \u0026#34;sample1 - binsize 1000 kbp - 379776 reads - 2N fit 1\u0026#34;) 当设置为TRUE时，ACE将在fitpicker文件的likely_fit列中填充最适合的单元。默认= FALSE\n参考资料  官方文档 ","date":"September 21, 2023","image":null,"permalink":"/post/2023-09-21_ace/","title":"使用ACE提取拷贝数变异"},{"categories":["生信工具"],"contents":"使用FREEC提取浅全基因组测序数据的拷贝数变异。\n1. 安装 R已安装，还需安装samtools，bedtools和sambamba\n 安装samtools  官方文档：http://www.htslib.org/download/\nwget https://github.com/samtools/samtools/releases/download/1.17/samtools-1.17.tar.bz2 cd samtools-1.17 ./configure --prefix=/where/to/install make make install export PATH=/where/to/install/bin:$PATH  安装bedtools，直接下载预编译好的二进制文件  官方文档：https://bedtools.readthedocs.io/en/latest/content/installation.html\n最新的软件地址：https://github.com/arq5x/bedtools2/releases\n下载名为bedtools.static.binay的文件\nwget https://github.com/arq5x/bedtools2/releases/download/v2.30.0/bedtools.static.binary mv bedtools.static.binary bedtools chmod a+x bedtools export PATH=/where/to/install:$PATH  安装sambamba  官方文档：https://github.com/biod/sambamba\n多种方式可以安装，直接用conda也可以\nconda install -c bioconda sambamba  下载最新的Control-FREEC版本  wget https://github.com/BoevaLab/FREEC/archive/refs/tags/v11.6.tar.gz  解压并编译  tar -zxvf v11.6.tar.gz cd FREEC-11.6/src make export PATH=/path/FREEC-11.6/src/freec  下载mappability tracks文件  # hg38 wget http://xfer.curie.fr/get/vyIi4w8EONl/out100m2_hg38.zip uzip out100m2_hg38.zip 官网还提供hg19，hg18，mm9以及mm10的，其他的可以通过GEM来自己构建。\n 如果处理的是深度测序数据，且想要检测allelic status，必须把read文件转化成pileup格式，需要下载SNPs文件。此处仅展示hg38  wget https://cloud.inf.ethz.ch/s/idTaGpZdnS9To5c/download/dbSNP151.hg38-commonSNP_minFreq5Perc_with_CHR.vcf.gz  示例数据  # download example file wget http://xfer.curie.fr/get/l4nQtrIsGmo/test.zip  Type in the command line:  cd /PATH_TO_TEST/  to run it with a control sample (window size is automatically selected):  /PATH_TO_FREEC/freec -conf config_ctrl.txt  to run it without a control sample (50kb window):  /PATH_TO_FREEC/freec -conf config_GC.txt  to run it only for the normal sample (50kb window):  /PATH_TO_FREEC/freec -conf config_BL.txt 我把安装在了zhukun账户下的CNVkit环境中，安装完成。\n2. 创建配置文件 WGS参考文件：https://github.com/BoevaLab/FREEC/blob/master/data/config_WGS.txt\nWES参考文件：https://github.com/BoevaLab/FREEC/blob/master/data/config_exome.txt\n配置文件中有2-5组参数：general，sample，control，BAF，target。没有对照样本情况下；不想计算BAF和基因型；不处理target数据情况下（比如外显子测序和其他的靶向测序数据），后3个可以空着。\n general：大部分是可选的  BedGraphOutput：默认是FALSE，如果要得到BedGraph格式来使用UCSC genome browser时设为TRUE\nbedtools：bedtools的路径，仅在针对全外显子组测序创造.pileup文件时使用\nbreakPointThreshold：片段化的阈值，默认是0.8，需要更多片段时使用0.6等更小的数字，推荐数值在0.1～1.2之间。\nbreakPointType：模糊区域的判断标准（比如全是N的区域，或者两个不同的拷贝数数值之间低匹配区域），默认是2，即把片段分开，和左片段统一或和右片段统一，但是拷贝数有优先级【没懂这个】\nchrFiles：指定染色体fasta文件，没有对照样本的情况下必须计算GC含量，或者直接有GC profile文件也可。举例：下载地址：path/hg19/chromosomes.http://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/\n整个文件夹需要下载和解压：\nwget --timestamping \u0026#39;ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/*\u0026#39; gunzip ./* chrLenFile：具有染色体长度的文件，没有在这里指定的染色体不会被分析到！举例：path/hg18.len 或hg19.len，hg19.fa.fai也可以\ncoefficientOfVariation：默认是0.05，coefficient of variation to evaluate necessary window size\ncontamination：默认是0，肿瘤样本中正常样本的数值，或者设置contaminationAdjustment=TRUE来矫正污染情况，也可以自己设置数值，比如0.25\ncontaminationAdjustment：默认是FALSE，contamination不提供情况下，会自动估计污染程度\ndegree：多项式次，默认3\u0026amp;4是全基因组（GC-content based normalization）,1是全外显子组（control-read-count-based normalization）\n**forceGCcontentNormalization：**针对GC含量偏差和低匹配度来来矫正read count（有对照样本也需要矫正），0代表简单模型，样本read count~对照read count，1是使用GC含量来标准化样本和对照的read count，计算ratio：样本read count/对照read count，2是样本read count～对照read count偏差模型，并针对GC含量来进行标准化，默认WGS使用0，WES使用1（大于9.5版本），0（小于9.5版本）\nGCcontentProfile：可选项，给定窗口大小下的GC含量profile，当染色体序列（.fasta文件）和对照样本这2个都没有的情况下，是必须指定的。举例：path/GC-profile.cnp\ngemMappabilityFile：具有mappable position的.gem文件，比如:out76.gem\nintercept：多项式截距，默认1是GC含量，0是对照组\nminCNAlength：调用CNA的最小连续窗口数，默认3是WES，1是WGS\nminMappabilityPerWindow：默认是0.85，当窗口中mappable position高于或等于这个阈值时才会被考虑。（如果gemMappabilityFile文件没有提供，则每个窗口使用非N的百分比）\nminExpectedGC：GC-content的最小期望值，用于“Read Count ~ GC-content”关系的先验评估，默认是0.35，不需要进行更改，除非应用在细菌基因组上。\nmaxExpectedGC：GC-content的最大期望值，默认是0.55，不需要进行更改，除非应用在细菌基因组上。\n**minimalSubclonePresence：**在百分之x的细胞群体中检测到亚克隆。默认是100，即没有亚克隆，建议WGS时设为20或0.2，WES时设为30或0.3\nmaxThreads：多线程计算，默认是1\nnoisyData：默认是FALSE，针对靶向测序数据或外显子测序数据设为TRUE，避免由于非均匀捕获而导致的假阳性预测。\noutputDir：输出目录，默认是当前目录，必须是存在的文件夹。/public/slst/home/zhukun/taozy/swgs_freec\nploidy：倍性，设为整数。\nprintNA：默认是TRUE，在分析外显子组测序或靶向测序数据中，为了避免在ratio.txt文件中输出-1，需要设为FALSE。\nreadCountThreshold：默认是10，针对外显子数据推荐数值是大于等于50，对照组中每个窗口最小的read数值阈值。\n**sambamba：**仅在读取BAM文件时设置sambamba的路径，默认情况下使用samtools而不是sambamba。\n**SambambaThreads：**SambambaThreads线程。\nsamtools：在读取BAM文件或创建.pileup文件时使用samtools路径，默认直接使用。\nsex：样本性别。XX则不考虑Y，XY时不会把X和Y注释成丢失。\nstep：当window参数指定时，需要设置该参数，分析外显子测序数据时设为0。【跑freec的时候发现自己忘记设置step了，经过对比，设置了windows后不设置step并不影响结果】\n**telocentromeric：**端粒前和中心点前区域的长度：Control-FREEC不会输出这些区域内发现的小的CNAs和LOH(由于mappability/基因组组装问题，它们很可能是假的)，默认是50,000，对于人类和mouse基因组50000合适，果蝇和酵母适合更小的数值。\nuniqueMatch：默认FALSE，使用mappability文件矫正read count，设为TRUE时需要提供gemMappabilityFile文件。\nwindow：显式窗口大小(优先级高于coefficientOfVariation);WGS设为50000，WES设为0\nchrLenFile和ploidy是必须的，如果没有对照样本，chrFiles和GCcontentProfile是必须的。染色体文件夹下，确保只有序列文件。如果没有gemMappabilityFile，每个窗口的non-N碱基比例被用作mappability，gemMappabilityFile只能用于没有对照样本的情况下。\n sample：给出输入文件的路径并指定其类型  matefile：path/sample.bam，mapped read文件（可以是单端测序，也可以是双端测序）\nmateCopyNumberFile：path/sample.cnp，给定窗口大小的原始拷贝数profile（优先级高于matefile）\nminiPileup：path/sample_minipileup.pileup，根据对应BAM文件创造的pileup文件，可以对整个过程提速。\ninputFormat：matefile中read的格式，SAM, BAM, pileup, bowtie, eland, arachne, psl (BLAT), BED, Eland. 除了BAM文件其他文件都接受GZipped压缩格式。\nmateOrientation：matefile中read的格式：0 (for single ends), RF (Illumina mate-pairs),\nFR (Illumina paired-ends), FF (SOLiD mate-pairs)，当输入是sort的BAM或SAM文件时，设置为0\n BAF  makePileup：具有SNP位点的BED或vcf文件的路径，用来从mateFile中提供的原始BAM文件中创建mini pileup文件，提供后，BAM文件可以直接用来计算BAF，不需要用.pileup.gz文件，不提供的话，pileup文件需要作为matefile提供。\nfastaFile：path/hg19.fa全基因组的fasta文件。当makePileup设定时，需要提供fasta文件，从.BAM文件来创建minipileup文件。\nminimalCoveragePerPosition：BAF分析中要考虑的位置的最小读覆盖范围，默认是0\nminimalQualityPerPosition：BAF分析中要考虑的位置的最小测序质量。默认是0\nshiftInQuality：basis for Phred quality，默认是0，通常设为33或64\nSNPfile：SNP文件，举例：path/hg19_\u0026lt;strong\u0026gt;snp142\u0026lt;/strong\u0026gt;.SingleDiNucl.1based.txt.gz，.vcf or .vcf.gz files are also accepted in \u0026gt;v9.3\n注意：SNP文件使用前提，是拥有覆盖度深的数据，并且需要检测allelic status的情况下否则不需要。\n target  captureRegions：BED格式的capture区域文件，sorted的BED文件应该包含以下列：chr 0-based start 1-based end。\n注意：在模块下的参数设置才有用。\n3. 示例样本 PathToFREEC/freec -conf myConfig.txt -sample sample.bam -control control.bam PathToFREEC/freec -conf myConfig.txt (if BAM files are provided directly in the config file) 针对sWGS的参数\nsample.txt\n###For more options see: http://boevalab.com/FREEC/tutorial.html#CONFIG ###  [general]  ##parameters chrLenFile and ploidy are required. chrLenFile = /public/slst/home/zhukun/taozy/swgs_freec/hg38_head.fa.fai ploidy = 2  ##Parameter \u0026#34;breakPointThreshold\u0026#34; specifies the maximal slope of the slope of residual sum of squares. ##This should be a positive value. The closer it is to Zero, the more breakpoints will be called. Its recommended value is between 0.01 and 0.08. breakPointThreshold = 0.8  ##Either coefficientOfVariation or window must be specified for whole genome sequencing data. Set window=0 for exome sequencing data. window = 100000  ##Either chrFiles or GCcontentProfile must be specified too if no control dataset is available. ##If you provide a path to chromosome files, Control-FREEC will look for the following fasta files in your directory (in this order): ##1, 1.fa, 1.fasta, chr1.fa, chr1.fasta; 2, 2.fa, etc. ## Please ensure that you don\u0026#39;t have other files but sequences having the listed names in this directory. chrFiles = /public/home/liuxs/biodata/reference/genome/gdc_hg38/chromosome  numberOfProcesses = 2 outputDir=/public/slst/home/zhukun/taozy/swgs_freec/results/win_100000bp  [sample]  mateFile = /public/home/wangjy10/work/swgs/colon/sample/bam/sample_sorted.bam inputFormat = BAM mateOrientation = 0  ##use \u0026#34;mateOrientation=0\u0026#34; for sorted .SAM and .BAM  #[BAF]  ##use the following options to calculate B allele frequency profiles and genotype status. This option can only be used if \u0026#34;inputFormat=pileup\u0026#34;  #SNPfile = /bioinfo/users/vboeva/Desktop/annotations/hg19_snp131.SingleDiNucl.1based.txt #minimalCoveragePerPosition = 5  ##use \u0026#34;minimalQualityPerPosition\u0026#34; and \u0026#34;shiftInQuality\u0026#34; to consider only high quality position in calculation of allelic frequencies (this option significantly slows down reading of .pileup)  #minimalQualityPerPosition = 5 #shiftInQuality = 33 outputDir=/public/slst/home/zhukun/taozy/swgs_freec/script_loh/results #/public/slst/home/zhukun/taozy/swgs_freec/results 参考资料  官方文档 ","date":"September 20, 2023","image":null,"permalink":"/post/2023-09-21_freec/","title":"使用FREEC提取拷贝数变异"},{"categories":["生信工具"],"contents":"使用QDNAseq提取浅全基因组测序数据的拷贝数变异。\n0.构建bin annotation 适合非hg19，hg38的参考基因组，可以构建其他物种或其他的基因组。第一步是基于染色体大小构建bins，设定bin的大小，并计算GC含量，以及非N碱基的比例：\nlibrary(QDNAseq) library(Biobase) library(BSgenome.Hsapiens.UCSC.hg19) # set the bin size binSize \u0026lt;- 15 # create bins from the reference genome bins \u0026lt;- createBins(bsgenome=BSgenome.Hsapiens.UCSC.hg19, binSize=binSize) 计算平均的匹配度，需要bigWig格式的匹配文件，以及二进制的bigWigAverageOverBed，匹配文件可以通过GEM library来构建，或者直接通过ENCODE进行下载。\n1. bin annotation 自定义binsize的大小，单位是kb，默认的参考基因组是hg19，其他的基因组需要自己构建。binsize可选1，5，10，15，30，50，100，500，1000kbp，可通过QNDAseq.hg19包获取，hg38在github上也有。\nbins \u0026lt;- getBinAnnotations( binSize=15 # genome=\u0026#34;hg38\u0026#34; ) # 15 kbp 2. 读入BAM文件，计算read count，展示需要筛除的片段 有3种方法读入bam文件，一种是所有的bam文件都在目前的工作路径下\nreadCounts \u0026lt;- binReadCounts(bins) 另一种是指定bam文件名称，可以直接读绝对路径的bam文件\nreadCounts \u0026lt;- binReadCounts(bins, bamfiles=\u0026#34;tumor.bam\u0026#34;) 还有一种是指定路径，所有的bam文件都在子目录tumors下\nreadCounts \u0026lt;- binReadCounts(bins, bamfiles=\u0026#34;tumors\u0026#34;) 输出文件是QDNAseqReadCounts对象，如果多次运行使用的是同一个样本，可以设置参数cache=TRUE，使用缓存文件来提速。\n如果BAM文件很大，可以使用chunkSize参数来控制内存。\n 这里可以使用LGG150样本的7-10号染色体作为示例：  \u0026gt; data(LGG150) \u0026gt; readCounts \u0026lt;- LGG150 \u0026gt; readCounts QDNAseqReadCounts (storageMode: lockedEnvironment) assayData: 38819 features, 1 samples element names: counts protocolData: none phenoData  我直接使用自己收集到的样本进行展示：  readCounts \u0026lt;- binReadCounts(bins, bamfiles=\u0026#34;/home/tzy/projects/CRCdata/data/test/C120115_sorted.bam\u0026#34;,cache=TRUE) 绘制原始的拷贝数图谱：基因组上的read count数，高亮的红色bins是将要默认筛除的，这里筛除的部分是：residual和blacklist\nplot(readCounts, logTransform=FALSE, ylim=c(-50, 200))  # Plotting sample LGG150 (1 of 1) ...  highlightFilters(readCounts, logTransform=FALSE, + residual=TRUE, blacklist=TRUE) Highlighted 3,375 bins.  \n 按照原文中的意思：\nresidual：\n \n blacklist：\n \n  \n 3. 删除标红bin： 把readcount进行筛选\nreadCountsFiltered \u0026lt;- applyFilters(readCounts, residual=TRUE, blacklist=TRUE)  206,391\ttotal bins 192,080\tof which in selected chromosomes 179,187\tof which with reference sequence 166,909\tfinal bins 绘制read count中位数值和GC含量、匹配度的二维图\nisobarPlot(readCountsFiltered)  \n 4. 矫正GC含量和匹配度 估计GC含量和匹配度的矫正，并绘制observed standard deviation和读深之间的关系，理论上的关系是线性关系，在图中是黑色实线，低质量DNA的样本会出现在直线上方，这种样本的噪音值更大。\nreadCountsFiltered \u0026lt;- estimateCorrection(readCountsFiltered) noisePlot(readCountsFiltered)  \n 接下来，对GC含量和匹配度进行矫正，将会返回QDNAseqCopyNumber对象，随后进行标准化，平滑离群值，并绘制拷贝数图谱。\ncopyNumbers \u0026lt;- correctBins(readCountsFiltered) copyNumbersNormalized \u0026lt;- normalizeBins(copyNumbers) copyNumbersSmooth \u0026lt;- smoothOutlierBins(copyNumbersNormalized) plot(copyNumbersSmooth) 这一步骤得到的结果可以存储为数据：\n\u0026gt; exportBins(copyNumbersSmooth, file=\u0026#34;LGG150.txt\u0026#34;) \u0026gt; exportBins(copyNumbersSmooth, file=\u0026#34;LGG150.igv\u0026#34;, format=\u0026#34;igv\u0026#34;) \u0026gt; exportBins(copyNumbersSmooth, file=\u0026#34;LGG150.bed\u0026#34;, format=\u0026#34;bed\u0026#34;) 5. 对bin数据进行片段化处理 用DNAcopy包CBS算法进行片段化处理，使用CGHcall获取拷贝数变异\n默认使用log2转化后的数据进行片段化，sqrt(x+3/8)也可以\n参考资料  Scheinin, Ilari, et al. \u0026amp;ldquo;DNA copy number analysis of fresh and formalin-fixed specimens by shallow whole-genome sequencing with identification and exclusion of problematic regions in the genome assembly.\u0026amp;rdquo; Genome research 24.12 (2014): 2022-2032. ","date":"September 20, 2023","image":null,"permalink":"/post/2023-09-21_qdnaseq/","title":"使用QDNAseq提取拷贝数变异"},{"categories":["生信工具"],"contents":"以Canvas工具为例阐述提取拷贝数变异的基本思路。\n讨论点：\n  对比浅全基因组测序和深全基因组测序的技术区别\n  阐述种系变异和体细胞变异提取的技术区别\n  首先简单介绍一下Canvas工具，该工具适用于不同测序平台测序数据的拷贝数提取，包括体细胞变异和种系变异，有四个模块：\n  Germline-WGS：通过全基因组测序提取二倍体种系样本中的CNV\n  Somatic-enrichment：通过靶向测序数据提取体细胞样本中的CNV\n  Somatic-WGS：全基因组测序数据中提取体细胞样本中的CNV\n  Tumor-normal-enrichment：靶向测序正常和肿瘤配对样本提取CNV（全基因组的情况下不设置筛选区域即可）\n  除此之外该工具还可以推断其他参数比如：肿瘤倍性，纯度，以及异质性。\n1. CanvasBin  该步骤创建基因组窗口（windows/bins），并统计落在每个窗口内的观测到的比对序列（observed alignment）的数量。\n  推荐输入为测序深度至少为10X的bam文件（还支持kmer.fa，下面会讨论），因为需要计算等位基因频率。该步骤读取BAM文件并应用多个筛选条件：   只保留与正链（forward strand）匹配（proper pairs） 的读段（reads）【我的理解：这里proper pairs的意思应该是正确定向的片段，读段相对于彼此方向正确，即一个配对到正向链，一个配对到反向链】  \nCIGAR字符串【点击跳转】 至少满足起始有35个的匹配（matches）  该工具把观察到的比对序列（alignment）存储在RAM中，使用每个序列的最左边碱基作为索引，分别存储在字节数组 \\( O_i \\) 中，对应每个染色体i。\n 除了BAM之外，输入文件还可以是kerm.fa的FASTA文件，可通过Canvas distribution得到，该文件中包含了用于序列比对的基因组序列，FASTA文件中每个35-mer【点击跳转】 的初始碱基被标记为特殊（大写）或非特殊（小写）。如果一个35-mer是特殊的，那么它的首字母就是大写的。举个例子：  acgtttaATgacgatGaacgatcagctaagaatacgacaatatcagacaa\n那么特殊的35-mer就是：\nATGACGATGAACGATCAGCTAAGAATACGACAATA\nTGACGATGAACGATCAGCTAAGAATACGACAATAT\nGAACGATCAGCTAAGAATACGACAATATCAGACAA\n特殊的35-mer在基因组上的位置存储在位数组 \\( E_i \\) 中，特殊的被标记为1，非特殊的标记为0。FlagUniqueKmers工具可为参考FASTA文件生成此注释。可以选择屏蔽一些特殊的35-mer子集，在命令行中提供一个BED文件，指定要忽略的基因组区域（-f，-filter=VALUE）。默认的屏蔽文件是改工具提供的fiter13.bed，该文件包含了着丝粒和端粒，以及一些中心体周围的区域。\n CanvasBin中另一个重要的输入是参数bindepth，设置二倍体区域中每个窗口预期的平均比对序列数量(desired average number of alignment per bin)（参数-d，\u0026ndash;bindepth=VALUE，默认设置为每个窗口内有100个比对序列，和aCGH array的信噪比大致相同），然后将此值转换为“bin size”。注意，在其他的工具中，“bin size”是用来设置窗口大小的，但在这里它指代每个窗口中特殊的35-mer的数量。由于人类基因组的某些区域重复性很高，因此每个窗口的物理大小（在基因组的坐标）不相同。对每个常染色体计算观察到的比对序列数量比上特殊的35-mer数量。然后，将每个bin的期望比对数除以这些比率的中值以获得bin size。   \n 全基因组测序数据中，bin size通常在800～1000范围内，相应的，大多数物理窗口的大小将在1～1.2kb的范围内。相比使用固定的基因组间隔，这种方法的优势在于，每个窗口里都会有几乎相同数量的读段（reads），而不用考虑窗口的“映射性”（mappability）或“唯一性”（uniqueness）。\n  CanvasBin还可以添加Nextera manifest文件（-t，\u0026ndash;manifest=VALUE），在这种情况下，只会使用manifest中指定的目标区域来计算bin size，比如覆盖到靶向区域的（on-target region）窗口（满足desired median number of alignment）。\n  CanvasBin还允许在BED文件中自定义窗口（通过-n参数设置）。例如，可以使用探针区域作为窗口。\n  CanvasBin的输出是G1_P1_0.binned，本质是一个压缩的BED文件，包含染色体，起始和终点位置，观察到的计数值，每个窗口的GC含量，内容如下：\n  chr1 754148 755200 90 46 chr1 755200 756030 93 54 chr1 756030 756783 90 54 chr1 756783 757489 89 53 chr1 757489 758242 80 55  CanvasBin流程汇总\n 2. CanvasClean  该步骤对CanvasBin得到的结果去除离群值和系统偏差。\n 一共包含4个步骤：\n  单点离群值移除\n  物理大小离群值移除\n  GC含量矫正\n  福尔马林固定，石蜡包埋的样本的标准化（只用在体细胞流程中）\n  输入数据是CanvasBin输出的BED文件。\n2.1 Single point outlier removal 定义两个bin计数：a和b，如果它们的差异大于我们在偶然情况下所预期的差异，（假设 a 和 b 来自相同的潜在分布），我们使用卡方检验统计量：\n \n X² 超过6.635（自由度为1的卡方分布的第99百分位数），该数值可以作为排除窗口区间的阈值。\n2.2 Physical size outlier removal 因为Canvas的流程中窗口没有相同的物理（指的是基因组）大小，在全基因组测序数据中他们的平均值大约是默认的1kb。然而，有些窗口可能达到Mb的大小，因为他们覆盖了基因组上的重复区域，这种高度重复区域在该步骤中被移除。计算观察的物理大小的第98百分位数，移除大于这个阈值的窗口。\n2.3 GC content correction 窗口计数（bin counts）的重要影响因素之一就是GC含量。该工具提供了矫正GC含量偏差的机制：\n Exmple of bias\n 矫正是基于平均的中位数，首先把每个窗口的GC含量四舍五入到最近的整数，把GC含量相同的放在一起，在具有相同GC含量的窗口内，每个窗口的计数除以窗口的中位计数值。最后，这些数值再乘上总体的中位数。这一步的目的是使上面显示的箱线图中的箱体中点变平。\n  小于给定阈值：有些GC含量下只有很少的窗口，这样会降低中位估计的置信度，为防止这种情况发生，CanvasClean舍弃了窗口数量低于给定阈值的所有窗口（max(minNumberOfBinsPerGCForWeightedMedian, number of bins/100)）。\n  小于100：如果一个GC值下少于100个窗口，使用相邻GC值的窗口来计算加权中位数。目标GC值的窗口具有全权重，两个相邻GC值的窗口具有一半权重，相隔两个窗口的具有1/4权重，依此类推。相邻的GC值将被包括在内，直到我们至少有100个b窗口来估算加权中位数的窗口数量。可以使用命令行参数 -w 来设置参数：minNumberOfBinsPerGCForWeightedMedian。\n  对于外显子测序数据中，CanvasClean使用Nextera manifest file(-t,\u0026ndash;manifest=VALUE），这种情况下只有on-target的窗口才会被用来估计每个GC下的窗口的计数估计。和全基因组一样，canvas会丢弃小于阈值的窗口，在某GC含量下少于100个窗口时计算权重中位数。\n在有些情况下，GC含量的矫正有非常重要的作用。比如在低深度测序的实验中使用nextera建库。\n \n 对于全基因组测序数据，通常会看到中位绝对误差（median absolute deviations, MADs）是10.3，这非常接近泊松模型预测的平均计数是100情况下MAD为10的情况，这表示完成CanvasClean步骤后误差非常微小。更重要的是，归一化并没有弱化CNV的信号。见下图：\n \n 2.4 Normalize for FFPE 福尔马林固定石蜡包埋的样本（FFPE）由于对样本的固定操作会引入噪音和偏差（比如DNA片段化问题，以及需要更高的温度进行DNA提取），导致难以解释深度异常现象。尽管其中一部分可以追溯到AT富集测序片段的丢失（部分通过GC标准化进行校正），但这并不能完全解释偏差，还存在其他尚不太清楚的因素，如染色质构象，可能也参与其中。Canvas采用两步策略来标准化FFPE样本：\n  FFPE去噪算法来移除局部偏差\n  以片段化为基础的标准化\n  2.4.1 FFPE去噪算法来移除局部偏差 去除FFPE特色的噪音和方差。FFPE引起的噪音主要特征是在高偏差区域的覆盖差异增加，这种偏差不能通过依赖均值和其他统计数据（中位数、众数等）的规范化操作来纠正。可以通过以下步骤来处理：\n \n （1）计算连续窗口之间的覆盖度差异（即连续覆盖区域之间的覆盖深度的差异，这里的1可能指的是1个窗口，不确定），时间滞后差异显示了FFPE偏差区域中不属于拷贝数变异导致的方差峰值。该数据的特征有以下几个优点：\n  所有区域的均值为零（包括大范围的拷贝数变化）；\n  方差是原始数据的两倍，可以使噪声区域更容易检测；\n  可以通过拷贝数变化之间更锐利的过渡来区分正确的断点区域，而不是从混乱区域中判断。\n  【如何区别方差的区别是由于拷贝数变化而不是由于噪音呢】\n（2）这种峰值可以用于一个简单的局部FFPE偏差/方差去噪算法，CanvasClean运行一个滑动区间（默认为20个覆盖窗口），计算每个窗口内的标准差（SD），然后为每个样本估算在所有窗口和染色体上的平均标准差；为简单起见，将这样的度量称为局部标准差（localSD），在CanvasClean步骤中会移除达到局部标准差阈值的区间。由于这是一种有损转换，所以只针对噪音很多的FFPE样本，而正常和噪音较少的FFPE样本不受影响（如何判断是否需要第2步见下面的第3步）。\n（3）从参考数据中针对同一个人FFPE和新鲜冻存（FF）样本的覆盖数据进行比较，局部标准差（localSD）显示，通过该度量阈值的设置（从图5中的示例中该阈值设为10）清晰地区分了FFPE和FF组，可见FF组的SD都小于10。该特征可以用来区分是FFPE样本是否需要进行去噪处理。\n \n 2.4.2 以片段为基础的标准化 虽然基于窗口的归一化可以纠正大部分的噪音，但仍然无法捕捉到在单个片段（fragment）大小水平的噪音。后者特别受到在FFPE文库制备过程中发生的PCR扩增步骤的影响。为了改进这一点，Canvas通过加权的GC含量校正因子来对区段计数进行标准化，该校正因子是通过对各个片段对总窗口计数的贡献进行平均得出的。片段的平均值是根据给定片段序列的观察值与期望的GC含量计数比例进行加权（图6）。片段基于的归一化可以通过在Canvas可执行命令行中指定\u0026ndash;custom-parameters=CanvasBin,–mode=GCContentWeighted参数来启用。\n \n  \n CanvasClean的输出和CanvasBin的输出格式是一样的。\n3. CanvasPartition  该步骤对于标准化过的数据，把每个窗口（bin）根据相等的平均覆盖度（mean coverage）划分成不同的片段（segment）。\n 把标准化之后的BED文件作为输入，并将所有的窗口划分为一组具有相等平均覆盖度的不同片段。CanvasPartition实现了多种分段算法，包括 unbalanced Haar wavelets（默认）和循环二分法分割（CBS） 。\n算法内容此处不做赘述，详细解释分别参考超链接。\n4. CanvasSNV  该步骤用来估计SNV等位基因频率（又称最小等位基因频率，MAF）。\n CanvasSNV 步骤用于估计 SNV 等位基因频率（也称为次等位基因频率，MAF=minor allele frequency），输入文件是：\n  包含变异记录的VCF文件，或正常的参考样本，或dbSNP参考VCF文件的路径\n  肿瘤和正常样本匹配的，包含比对序列读段（reads）的BAM文件\n  在肿瘤/正常样本分析流程中，正常样本中得到的种系SNV用来鉴定肿瘤中所有通过筛选的（FILTER一列中为PASS）杂合性SNV。（GT=0/1或1/0），另外如果GQX tag存在（变异位点的经验校准变异质量得分），需要选择该数值大于30的数据。提取种系变异时，CanvasSNV使用种系SNV或者遍历所有的dpSNP位点，对于每个SNV位点，Canvas都会计算REF和ALT等位基因的序列比对数目。这个计算过程中去除PCR重复，次要对准（secondary alignment）和补充对准（supplementary alignment）。\nPS：次要对准通常指的是在DNA测序过程中，当测序仪器无法确定一个特定位置的碱基时，会有多个可能的对准方式。补充对准则是指在进行DNA序列分析时，将次要对准或低质量的序列数据与参考基因组或DNA序列进行比对，以提高数据质量和准确性。\n设A为ref allele的read count，B 为alt allele的read count，MAF=min(A,B)/(A+B)，MAF在二倍体区域约为0.5（实际情况下，二倍体的MAF会更低，随着测序深度的增加会接近于0.5），CanvasSNV的输出是临时文件VFResultsG1_P1.txt.gz，示例如下：\n#Chromosome Position Ref Alt CountRef CountAlt chr1 12783 G A 53 135 chr1 13116 T G 24 70 chr1 13118 A G 25 69 chr1 14907 A G 73 102 chr1 14930 A G 48 98 chr1 15190 G A 160 13 chr1 16298 C T 18 54 5. CanvasDiploidCaller  提取种系拷贝数变异的最后一步是根据CanvasPartition得到每个区域的拷贝数状态，再确定变异的最大拷贝数（copy number of major allele）。\n 该步骤的输入数据是CanvasPartition得到的每个片段（segment）的覆盖度和MAF数值。\n提取种系拷贝数变异的最后一步，是推测Canvaspartition得到的每个区域的拷贝数状态，并且确定拷贝数变异区域的最大拷贝数（MCC，copy number of major allele），这一步骤的输入是覆盖度（coverage）和每个片段（each CanvasPartition segment）的MAF数值，使用具有每个拷贝数和MCC状态的单独分量的高斯混合模型，例如对于CN=2模型有两个状态，分别表示二倍体（MCC=1，CN=2）和LOH（MCC=2，CN=2）。均值和协方差矩阵是从二倍体模型的数据中估计出来，然后通过期望最大化（EM）算法进行调整。例如，如果在二倍体模型中估计的均值µ和标准差σ为100和15，那么在单倍体模型中的对应估计值将初始化为µ/2和σ/2。在进行EM拟合后，将使用具有最高后验概率的[CN，MCC]模型元组来为每个片段分配拷贝数。在片段没有跨越SNPs（单核苷酸多态性位点）且因此MAF值为空的情况下，不考虑LOH状态。\n这个步骤中具有相同拷贝数的临近区域合并为一个片段。\n最终，类似于Phred的q分数被分配给每个区域。为了得出这些分数，在一个包含约10个筛选过的种系样本的测试语料库上训练了一个 logistic 回归模型，以计算每个片段具有正确方向的概率，即删除、扩增或无变化。模型使用的三个特征是：\n  LogBinCountlog10(1+bin的数量)。拥有更多bin的分段可以更准确地分配拷贝数。\n  ModelDistance：观察到的覆盖度和MAF值与所选模型预测值之间的距离。较低的距离表示拟合更好。\n  DistanceRatio：最佳模型点的模型距离与次优模型距离之间的比率。这个值越大，分配的多倍体数是正确的可信度就越大。\n  然后，概率被缩放以生成Phred-scaled似然度。非二倍体区域的坐标及其q分数被写入VCF文件。在为变异分配PASS标志时应用了两个过滤条件，首先，变异必须具有10或更高的q分数。其次，变异的大小必须为10kb或更大。\n6. CanvasSomaticCaller  该步骤用于肿瘤和正常样本匹配的情况，鉴定拷贝数变异区域。\n 该步骤用于提取拷贝数变异的区域（比如：常染色体上的拷贝数不是2，或者b等位基因频率不是50%左右的区域）来估计样本纯度或者肿瘤倍性。\n6.1 体细胞模型 设计原理：\n 对于给定的倍性和纯度，可以对每个拷贝数状态计算预期覆盖值和MAF。区域中给定倍性（拷贝数和最大等位基因计数），每个SNV的最小等位基因频率理论上符合：  Ploidy A: MAF 0 Ploidy AB (normal): MAF 0.5 Ploidy AA (copy-neutral LOH): MAF 0 Ploidy AAB: MAF 0.33333 Ploidy AAA: MAF 0 Ploidy AABB: MAF 0.5 Ploidy AAAB: MAF 0.25 Ploidy AAAA: MAF 0 (etc.)  模型的偏差(deviation) = 预期（expected）覆盖度（coverage）-观察覆盖度\n  两个模型具有相似的偏差时，偏向选择具有更小拷贝数的模型（比如，更接近二倍体）。\n  训练样本中具有已知的倍性和纯度数值，用机器学习来推断用来区分正确和错误的体细胞模型的最佳特征。\n  如果肿瘤是多克隆的，该步骤还会估计整体的变异杂合性，对燃预测的倍性和纯度和拷贝数状态是线性关联的，但是杂合性会导致模型更复杂。该步骤使用了很多近似的方法来检测克隆。\n流程：\n  输入CanvasPartition得到覆盖片段（coverage segment）\n  输入CanvasSNV得到的变异频率（variant frequency），保留所有ref+alt覆盖度大于10的位点，如果使用了匹配的正常样本，那么CanvasSNV输出就只保留了杂合性SNV的变异频率，但是，dbSNP VCF可以用来代替正常的VCF，这样的话，CanvasSNV里面并不是所有的位点都是杂合的，这种情况可以可以调用参数-d（\u0026ndash;dbsnpvcf）来对每个片段（segment）调整纯合位点的MAF。MAF选择的标准如下：\n    CanvasSomaticCaller会在以下情况下过滤掉片段中的零MAF（最小等位基因频率）：如果超过10%的MAF是非零的，或者至少有一个位置的MAF \u0026gt;= 0.2。这是因为预期纯合位点的MAF应为零。\n  如果在过滤之前的MAFs数量大于给定的MAF计数阈值（默认为50），但在过滤后小于此阈值，则阈值将降低为过滤后的MAFs数量。\n  对于CanvasPartition得到的每个片段（segment），计算每个窗口（bin）计数的中位数，以及中位最小等位基因频率（MAF）。   每个段都被分配一个权重，该权重用于计算下面所描述的偏差。如果总片段数大于100，则权重等于片段的长度。否则，权重等于片段中的窗口数。如果片段中的MAF少于10个，那么权重将乘以MAF数量除以10的比率。这是因为较少的MAF对片段的真实中位数MAF的估计不太可靠。这种段加权技术使模型拟合更加稳健，能够更好地抵抗噪声。  迭代计算不同的二倍体覆盖度D和纯度p来获得模型的偏差（deviation）。    给定一个候选元组（D，p），计算每个拷贝数值的期望（MAF，Coverage）点。\n  （D，p）的模型偏差是在所有分段上的总和，表示为 ||(SegmentMAF，SegmentCoverage) - (ModelMAF，ModelCoverage)||。\n  估算模型的惩罚项。为了计算惩罚项，对于每个模型，从初步的全基因组拷贝数调用中计算了百分比正常、百分比CN=2和二倍体距离变量。然后，它们被用作逻辑回归模型中的预测变量，该模型在一个包含58个肿瘤/正常配对的数据集上进行训练，数据手动分配了倍性和纯度值。来自这个模型的回归系数被转换成最终惩罚项的新权重，具体如下。    0.25*DiploidDistance：将给定基因组转化为二倍体所需的CN事件数量的比例\n  +0.35*CN2：CN = 2 的百分比\n  +0.3*PercentNormal：正常CN的百分比\n   考虑肿瘤的多克隆性。为此，使用EM算法将每个段元组（SegmentMAF，SegmentCoverage）分配给不同的聚类。聚类是通过从已根据k最近邻距离（其中k设置为总段数）排序的观察段中抽样（SegmentMAF，SegmentCoverage）值来初始化的。只使用前70%的段。这个值是为了偏好初始EM值接近与主要拷贝数变化相关联的聚类而设定的。通过使用Silhouette系数（Rousseeuw P., 1987）选择了最佳聚类数。聚类偏差（D，p）定义为属于该聚类的段与给定模型中计算的最接近的预期（MAF，Coverage）元组之间的距离之和，如第4步所计算。一般来说，距离预期（MAF，Coverage）中心最远的聚类更有可能是异质的：如果聚类偏差\u0026gt;总偏差*1.5，则在计算总体聚类偏差时不考虑该聚类（该值经过训练语料库优化）。\n  总偏差被定义为模型偏差、簇偏差和惩罚项的平均值。使用具有最低总偏差的覆盖度（多倍体）和纯度模型来分配拷贝数调用。\n  6.2 拷贝数估计 最终需要对数据进行汇总。临近的具有相同拷贝数的片段将被合并成一个片段，片段大小低于阈值（默认50000个bases）被合并成最优的片段（最高的q值）。合并需要满足条件：在同一条染色体上，之间没有其他的片段，两者中间没有被屏蔽的片段（比如BED文件中指定需要去除的区域）。对每个变异计算Phred Q值，一个逻辑回归模型已经在一个包含33个经过筛选的肿瘤/正常样本的数据集上进行了训练，以计算每个片段正确的概率。所使用的三个特征分别是：\nLogBinCount：LogBinCount是log10(1+bin数)。拥有更多窗口的片段可以更准确地分配拷贝数。\nModelDistance：模型距离。较低的距离反映出更好的拟合。\nDistanceRatio：距离与最佳模型点距离与次优模型点距离之比。这个值越大，表示分配的倍数确实是正确的可能性越大。然后，将概率缩放以生成类似Phred的似然性，表示结果是正确的。请注意，这个q-scoring模型使用了与germline(CanvasDiploidCaller)相同的特征，但是在肿瘤/正常数据上重新训练。\nCanvasSomaticCaller的主要输出是一个VCF格式。拷贝数根据拷贝数计数和期望计数报告为REF、LOSS或GAIN。通常期望计数为两个（拷贝数2是REF，拷贝数0或1是LOSS，拷贝数3或更多是GAIN）。当传递给Canvas的ploidy BED文件时，它可以更新特定染色体的期望拷贝数。例如，对于XY样本的染色体X和染色体Y，X或Y上的拷贝数为2会被称为GAIN。\n\nCIGAR string CIGAR字符串是一种在生物信息学中常用的格式，用于表示比对两个生物序列的结果，通常是DNA、RNA或蛋白质序列。CIGAR代表\u0026quot;Compact Idiosyncratic Gapped Alignment Report\u0026quot;，它描述了两个序列之间的匹配、插入和删除等信息。\nCIGAR字符串由一系列数字和字母组成，表示了比对的特征。其中常见的一些符号包括：\n M：匹配的碱基对（match） I：插入到参考序列的碱基对（insertion） D：从参考序列中删除的碱基对（deletion） N：跳过参考序列的碱基对（skipped region） S：从比对的序列的开头或结尾剪切的碱基对（soft clipping） H：从比对的序列的开头或结尾硬剪切的碱基对（hard clipping）  一个典型的CIGAR字符串示例可能是：\u0026ldquo;50M2I10M\u0026rdquo;，表示有50个匹配的碱基对，然后插入了2个碱基对，最后又有10个匹配的碱基对。\nCIGAR字符串是在比对算法（如BLAST、Bowtie、BWA等）的输出中常见的一部分，用于描述比对的结果，以便进一步的生物信息学分析。\n\nkmer 就是长度为k的DNA片段，是reads的部分片段。k-mer的作用有3个，一个是用来拼接出contig，一个是用来识别错误测序，杂合等位基因和重复序列的reads，还有通过K-mer估计基因组大小及杂合度。k-mer的数量，假设100bp的reads，35-mer，得到总数为100-35+1个kmer，不同kmer的overlap的长度为 kmer的长度减1。\n \n 返回原文位置【点击跳转】\n参考资料  Roller, Eric, et al. \u0026amp;ldquo;Canvas: versatile and scalable detection of copy number variants.\u0026amp;rdquo; Bioinformatics 32.15 (2016): 2375-2377. ","date":"September 19, 2023","image":null,"permalink":"/post/2023-09-19_cnvcall_1/","title":"提取拷贝数变异基本思路"},{"categories":["shell"],"contents":"介绍如何使用GUN Parallele在shell中并行计算。\n前言 并行指的是每个线程分配给独立的核心，线程同时运行，最大程度的使用所有CPU的核。GUN Parallele是一种shell工具，作业可以是单个命令或脚本。输入是文件列表、主机列表、用户列表、url列表或表列表等等。GNU Parallele分割输入，并行地将其输送到命令中。\n1. 安装 wget http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2 tar xjf parallel-latest.tar.bz2 cd parallel-20211022/ ./configure --prefix=$HOME make \u0026amp; make install 2. GUN Parallele语法介绍 GNU parallel可以读入文件，命令行，以及stdin（标准输入或管道）。\n单个输入 输入\nparallel echo ::: A B C 输出\nA B C 多个输入 输入：\nparallel echo ::: A B C ::: D E F # 或者 parallel -a abc-file -a def-file echo 输出：\nA D A E A F B D B E B F C D C E C F 连接方式  \u0026ndash;link 一对一连接  输入：\nparallel --link echo ::: A B C ::: D E F 输出：\nA D B E C F  \u0026ndash;verbose：在本地显示命令，可以在运行前打印出命令，展现出实际上脚本是怎么跑的。  parallel --verbose echo {} ::: A B C 输出：\necho A echo B A echo C B C  :::和::::  :::是连接变量，::::是连接文件或目录\n -j/-jobs 设置并行任务数目，默认jobs和CPU的核数目相同。  perl -e \u0026#39;for(1..128){print \u0026#34;$_\\n\u0026#34;}\u0026#39; \u0026gt; num128 /usr/bin/time parallel -N0 -j64 sleep 1 :::: num128 # 64个并行任务，sleep是啥用处 在函数中使用并行 # Only works in Bash my_func() {  echo in my_func $1 } export -f my_func parallel my_func ::: 1 2 3 输出：\nin my_func 1 in my_func 2 in my_func 3 对命令行使用 基本语法：\ncat file | parallel command 将file的每一行，传输给需要批量运行的命令来并行运算。\n示例：\nparallel -k echo ::: {1..21} X Y \u0026gt; contig # 先生成每条条染色体的编号 T=/public3/home/sch7779/taozy/sclust_data/HRR232650_bqsr.bam N=/public3/home/sch7779/taozy/sclust_data/HRR232651_bqsr.bam config=/public3/home/sch7779/taozy/script/config  normal=$(basename \u0026#34;$T\u0026#34; _bqsr.bam) tumor=$(basename \u0026#34;$N\u0026#34; _bqsr.bam)  cat ${config} | parallel -j 24 Sclust bamprocess -t ${T} -n ${N} -o ./${tumor} -part 2 -build hg19 -r chr{} 每条染色体的编号并行传入{}位置，24个并行任务（注意查看运算节点有多少核可以使用），这样就可以对每个样本的所有的染色体同时进行处理。\n对脚本使用 示例：并行call mutation\n#PBS -N mutect_split1 #PBS -l nodes=1:ppn=28 # 学校的1个节点上只有28个核，该任务似乎不可以跨节点运行（检查一下） #PBS -S /bin/bash #PBS -j oe #PBS -q slst_fat #PBS -o /public/home/liuxs/taozy/project/CRCsig/GSA-HRA000873/6.mutect/result  start=`date +%s` # 计时  source /public/home/liuxs/anaconda3/bin/activate source activate wes  GATK=/public/home/liuxs/anaconda3/envs/wes/share/gatk4-4.1.3.0-0/gatk-package-4.1.3.0-local.jar ref=/public/home/liuxs/taozy/project/CRCsig/GSA-HRA000873/one_sample/result/ucsc.hg19/ucsc.hg19.fasta bed=/public/home/liuxs/taozy/project/CRCsig/GSA-HRA000873/one_sample/result/ucsc.hg19/hg19.exon.bed path=/public/home/liuxs/taozy/data/GSA-HRA000873/HRA000873/raw_data1  export GATK # This is critical! 一定要记得把所有并行需要用到的变量和函数export，且函数需要添加参数-f export ref export bed export path  # 构建函数 function parallel_call { # 检查$1和$2是否分别对应normal和tumor样本 # $1表示传给shell的第1个参数，$2类推 echo \u0026#34;normal is $1\u0026#34; echo \u0026#34;tumor is $2\u0026#34;  # 检查是否有tmp文件夹，没有则创建 if [ ! -d \u0026#34;./mutect_tmp/\u0026#34; ];then  mkdir mutect_tmp fi  java -jar -Xmx20G -Djava.io.tmpdir=/public/home/liuxs/taozy/data/GSA-HRA000873/HRA000873/raw_data1/$2/mutect_tmp $GATK Mutect2 -R ${ref} \\ -I ${path}/$2/$2_bqsr.bam -tumor $2 \\ -I ${path}/$1/$1_bqsr.bam -normal $1 \\ -L ${bed} \\ -O ${path}/$2/$2_mutect2.vcf  cd /public/home/liuxs/taozy/data/GSA-HRA000873/HRA000873/raw_data1/$2  if [ ! -d \u0026#34;./filmut_tmp/\u0026#34; ];then  mkdir filmut_tmp fi  java -jar -Xmx20G -Djava.io.tmpdir=/public/home/liuxs/taozy/data/GSA-HRA000873/HRA000873/raw_data1/$2/filmut_tmp $GATK FilterMutectCalls \\ -R ${ref} \\ -V ${path}/$2/$2_mutect2.vcf \\ -O ${path}/$2/$2_somatic.vcf  cat ${path}/$2/$2_somatic.vcf | perl -alne \u0026#39;{if(/^#/){print}else{next unless $F[6] eq \u0026#34;PASS\u0026#34;;next if $F[0] =~/_/;print } }\u0026#39; \u0026gt; ${path}/$2/${2}_filter.vcf } export -f parallel_call # export -f 特指函数  # split是把需要运行的样本切割成多份，方便批量提交任务到集群 split=/public/home/liuxs/taozy/project/CRCsig/GSA-HRA000873/6.mutect/script/split_sample export split  normal=$(awk -F\u0026#39;,\u0026#39; \u0026#39;{print $1}\u0026#39; $split) tumor=$(awk -F\u0026#39;,\u0026#39; \u0026#39;{print $3}\u0026#39; $split) export normal export tumor  # --link保证正常样本和肿瘤样本一对一匹配运行 # --verbose 显示将运行的命令 parallel --verbose -j 28 --link parallel_call ::: ${normal} ::: ${tumor}  end=`date +%s` time=`echo $start $end | awk \u0026#39;{print $2-$1}\u0026#39;` echo $time 参考资料   GNU Parallel Tutorial\n  肿瘤外显子数据分析\n ","date":"November 14, 2022","image":null,"permalink":"/post/2022-11-15_gnu_parallel/","title":"使用GUN Parallele并行计算"},{"categories":["生信工具"],"contents":"介绍如何使用Sclust提取绝对拷贝数。\n前言 1. 安装 wget http://www.uni-koeln.de/med-fak/sclust/Sclust.tgz tar xvzf Sclust.tgz cd Sclust/src make vi ~/.bashrc # export PATH=\u0026#34;/path/Sclust/bin:$PATH\u0026#34; source ~/.bashrc 2. 输入数据 必须包含：\n  肿瘤和正常样本配对的bam文件（必须是经过BQSR的bam文件，一般有具有后缀为bai的index文件）\n  包含所有的体细胞点突变（SNV和小的插入和删除）的vcf文件\n  另外可选择增加包含重排断点的拷贝数片段化文件\n  3. 使用方法 Sclust包括三个模块：bamprocess，cn，cluster\n  染色体名称规则：chr1-chr22，chrX，chrY\n  肿瘤和正常样本：匹配到人类参考基因组hg19，或者老鼠参考基因组mm10。\n  步骤1：预处理文件 Timing 15 min–15 h 从bam文件提取染色体read ratio和SNP信息\nSclust bamprocess -t \u0026lt;sample\u0026gt;_T.bam -n \u0026lt;sample\u0026gt;_N.bam -o \u0026lt;sample\u0026gt; -part 2 -build hg19 -r \u0026lt;chr\u0026gt;   chr，染色体名称，\u0026lt;chr\u0026gt;=chr1,\u0026hellip;, \u0026lt;chr\u0026gt;=chr22, \u0026lt;chr\u0026gt;=chrX, \u0026lt;chr\u0026gt;=chrY\n  \u0026lt;sample\u0026gt;，保持一致，方便后续操作\n  -part，1是外显子组测序，2是全基因组测序\n  每个染色体单独运行，运行完所有的染色体，输出如下:\n\u0026lt;sample\u0026gt;_chr1_bamprocess_data.txt,...,\u0026lt;sample\u0026gt;_chrY_bamprocess_data.txt 对临时文件进行组合（此处不用输入临时文件名称，直接输入样本名就可以，非常快这一步）\nSclust bamprocess -i \u0026lt;sample\u0026gt; -o \u0026lt;sample\u0026gt; 输出文件为_rcount.txt 和包含base counts of common SNPs: _snps.txt\n\u0026lt;sample\u0026gt;_rcount.txt chromosome start end rcount_t rcount_n GC  \u0026lt;sample\u0026gt;_snps.txt part_no chromosome position T_A T_C T_G T_T N_A N_C N_G N_T allele_A allele_B 步骤2.0 预处理vcf转化成Sclust特殊格式 把vcf文件转化成Sclust特殊的格式，该格式类似于标准vcf文件但是必须包含要求的内容。\n \n 必须要有DP、DP_N、AF、AF_N这四列信息。我的输入文件是使用Mutect2后又使用FilterMutectCalls的vcf文件，注意，只有将PASS设置为筛选选项的突变才会被考虑为突变聚类。示例.vcf文件可以在安装文件夹下的example/data/H2171_mutations下找到。\n由于我的vcf文件格式和标准格式不同，标准格式如下（注释开头未展示）\n#CHROM POS ID REF ALT QUAL FILTER INFO chr1 978697 . G T 77 Ndb DP=19;DP_N=26;AF=0.421053;AF_N=0;FR=0.333333;TG=entg|AGRN:ccds|CCDS30551.1 我的vcf格式如下（注释开头未展示）：\n#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT HRR232652 HRR232653 chr1 1356341 . TTCC T . normal_artifact;slippage;weak_evidence CONTQ=93;DP=202;ECNT=1;GERMQ=93;MBQ=20,20;MFRL=200,193;MMQ=60,60;MPOS=47;NALOD=-5.834e+00;NLOD=14.82;POPAF=6.00;RPA=5,4;RU=TCC;SEQQ=1;STR;STRANDQ=23;STRQ=1;TLOD=3.68 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:71,2:0.039:73:32,2:31,0:37,34,1,1 0/0:97,5:0.056:102:35,3:49,1:45,52,2,3 其中GT:AD:AF:DP:F1R2:F2R1:SB对应的数值在后两列中，后两列分别是tumor和normal，分隔符号为:，转化成Sclust格式分割符为;，因此这里：AF=0.039;AF_N=0.056;DP=73;DP_N=102，得到必须的信息。\n批量对所有的vcf进行格式转化（代码需要优化）：\n# 分离注释和突变信息 grep -v \u0026#39;#\u0026#39; HRR234505_somatic.vcf\u0026gt;uninfo_HRR234505_somatic.vcf grep \u0026#39;#\u0026#39; HRR234505_somatic.vcf\u0026gt;info # 提取AF信息 cat uninfo_HRR234505_somatic.vcf|cut -f11|cut -d\u0026#39;:\u0026#39; -f3\u0026gt;af_line sed \u0026#34;s/^/AF=\u0026amp;/g\u0026#34; af_line\u0026gt;af_file rm af_line # 提取AF_N信息 cat uninfo_HRR234505_somatic.vcf|cut -f10|cut -d\u0026#39;:\u0026#39; -f3\u0026gt;af_n_line sed \u0026#34;s/^/AF_N=\u0026amp;/g\u0026#34; af_n_line\u0026gt;af_n_file rm af_n_line # 提取DP信息 cat uninfo_HRR234505_somatic.vcf|cut -f11|cut -d\u0026#39;:\u0026#39; -f4\u0026gt;dp_line sed \u0026#34;s/^/DP=\u0026amp;/g\u0026#34; dp_line\u0026gt;dp_file rm dp_line # 提取DP_N信息 cat uninfo_HRR234505_somatic.vcf|cut -f10|cut -d\u0026#39;:\u0026#39; -f4\u0026gt;dp_n_line sed \u0026#34;s/^/DP_N=\u0026amp;/g\u0026#34; dp_n_line\u0026gt;dp_n_file rm dp_n_line # 去除原始文件多余信息 cat uninfo_HRR234505_somatic.vcf|cut -f1-7\u0026gt;CRC1061_new.vcf rm uninfo_HRR234505_somatic.vcf   # 合并 paste CRC1061_new.vcf af_file\u0026gt;CRC1061_new2.vcf rm CRC1061_new.vcf paste -d \u0026#39;;\u0026#39; CRC1061_new2.vcf af_n_file\u0026gt;CRC1061_new3.vcf rm CRC1061_new2.vcf paste -d \u0026#39;;\u0026#39; CRC1061_new3.vcf dp_file\u0026gt;CRC1061_new4.vcf rm CRC1061_new3.vcf paste -d \u0026#39;;\u0026#39; CRC1061_new4.vcf dp_n_file\u0026gt;CRC1061_new5.vcf rm CRC1061_new4.vcf  # 删除info最后一列 sed -i \u0026#39;$d\u0026#39; info  # 在最后一列添加新列名 sed \u0026#39;$a #CHROM POS ID REF ALT QUAL FILTER INFO\u0026#39; info\u0026gt;info2 rm info  # 合并到最终文件info2 cat CRC1061_new5.vcf \u0026gt;\u0026gt; info2 rm CRC1061_new5.vcf mv info2 CRC1061_new_mutations.vcf 文件转化成功。\n步骤2 拷贝数分析 \u0026lt;5 min 使用转化后的vcf文件\u0026lt;sample\u0026gt;_mutations.vcf，第一步得到的read-count file \u0026lt;sample\u0026gt;_rcount.txt,和SNP base-count file \u0026lt;sample\u0026gt;_snps.txt作为输入，进行拷贝数分析。\nSclust cn -rc \u0026lt;sample\u0026gt;_rcount.txt -snp \u0026lt;sample\u0026gt;_snps.txt -vcf \u0026lt;sample\u0026gt;_mutations.vcf -o \u0026lt;sample\u0026gt; 输出文件为\n\u0026lt;sample\u0026gt;_cn_summary.txt \u0026lt;sample\u0026gt;_allelic_states.txt \u0026lt;sample\u0026gt;_subclonal_cn.txt \u0026lt;sample\u0026gt;_uncorr_cn.seg \u0026lt;sample\u0026gt;_iCN.seg \u0026lt;sample\u0026gt;_muts_expAF.txt \u0026lt;sample\u0026gt;_cn_profile.pdf  \u0026lt;sample\u0026gt;_cn_summary.txt  sample_name purity ploidy fraction_subclonal_cn sex_estimated status fraction_inconsistent_segs H2171 0.99 1.92056 0.0871417 m optimum 0.00323938   purity 估计的肿瘤纯度 ploidy 倍性 fraction_subclonal_cn 亚克隆的拷贝数占比 sex 病人的性别\n  status 有四个值：optimal（达到了最优解）;invatiant（没有足够拷贝数变异，并且从突变计算纯度）;forced（设置了-f2）;failed（该方法失败）\n  fraction_inconsistent_segs 在没有找到唯一的拷贝数解的情况下，不一致的亚克隆片段的占比\n   \u0026lt;sample\u0026gt;_allelic_states.txt  Sample Chromosome Start End Copy_Nr_Raw CopyNr A B LOH Theta Theta_Exp n_SNPs Is_Subclonal_CN Subclonal_P_value Is_Inconsistent_State H2171 chr1 865532 24022908 2.11065 2 1 1 0 0.121093 0.082753 278 0 0.010588 0   Copy_Nr_Raw 未矫正的拷贝数\n  CopyNr 矫正的整数拷贝数\n  A 指的是major allele拷贝数\n  B 指的是minor allele拷贝数\n  LOH 片段的LOH状态，数值为1或0\n  Theta 观察到的allelic imbalance\n  Theta_Exp 通过建模预测得到的allelic imbalance\n  n_SNPs SNP的总数\n  Is_Subclonal_CN 片段的克隆性（0=不是，1=是）\n  Subclonal_P_value 片段是否是克隆性的P值\n  Is_Inconsistent_State 如果没有找到该段的唯一解，该列为1\n   \u0026lt;sample\u0026gt;_subclonal_cn.txt  Sample Chromosome Start End Subclonal_CN Clone1_A Clone1_B Clone1_Fraction Clone2_A Clone2_B Clone2_Fraction H2171 chr2 41557 29404778 3.16433 2 2 0.164326 2 1 0.835674 该文件包含所有的确定的亚克隆拷贝数，这里start和end是亚克隆拷贝数片段的起始位置。\n  Subclonal_CN 亚克隆总的拷贝数\n  Clone1_A Clone1_B 分别是major和minor拷贝数\n  Clone1_Fraction 克隆1的克隆占比\n  Clone2_Fraction 克隆2的克隆占比\n    \u0026lt;sample\u0026gt;_uncorr_cn.seg\n  \u0026lt;sample\u0026gt;_iCN.seg\n  \u0026lt;sample\u0026gt;_muts_expAF.txt\n  Mut_ID Chr Position Wt Mut AF_obs Coverage AF_exp Mut_Copies Mut_Copies_Raw Is_Subclonal_CN iCN P_Is_Clonal H2171_chr1:3670813_SNM chr1 3670813 C A 0.5625 16 0.495 1 1.13636 0 2 0.706872   Mut_ID 包含样本名:基因组上的位置:突变的类型\n  Chr Position 这两列信息涵盖在第一列中\n  Wt 野生型 Mut 突变型   AF_obs 观察到的变异的allelic fraction AF_exp 假设这个突变是克隆性的，allelic fraction的期望值\n  Coverage 该突变的覆盖度\n  Mut_Copies Mut_Copies_Raw 估计的和原始的多样性(突变拷贝的数量)\n  Is_Subclonal_CN iCN 对应片段的拷贝数及其克隆性\n   P_Is_Clonal 该P值表明了这个突变是否是克隆性的(例如，检测CCF=1)。\n   \u0026lt;sample\u0026gt;_cn_profile.pdf  步骤3 突变聚类（可得CCF） 这一步必须要有第二步得到的文件，\u0026lt;sample\u0026gt;_muts_expAF.txt\nSclust cluster -i \u0026lt;sample\u0026gt; 输出文件\n\u0026lt;sample\u0026gt;_mclusters.txt \u0026lt;sample\u0026gt;_cluster_assignments.txt \u0026lt;sample\u0026gt;_mcluster.pdf  \u0026lt;sample\u0026gt;_mclusters.txt  Cluster_ID CCF_Cluster Cluster_Peak_Height Mutations_In_Cluster 0 1.00798 14.9095 214 1 0.78729 1.11972 49   Cluster_ID 确定的突变类别号\n  CCF_Cluster 该类别的CCF，克隆性的突变一般都在0类别中\n  Cluster_Peak_Height 类别的顶峰高度，这个和分配给每个类别的突变数量有关\n  Mutations_In_Cluster 每个类别的突变数量有关\n   \u0026lt;sample\u0026gt;_cluster_assignments.txt  Mut_ID Chr Position Wt Mut CCF Coverage Cluster_Id Cluster_CCF Probability P0 P1 P2 P3 P4 H2171_chr1:3670813_SNM chr1 3670813 C A 1.13636 16 0 1.00798 0.800866 0.800866 0.188233 0.00998492 0.000890016 2.61627e-05   前5列包含\u0026lt;sample\u0026gt;_muts_expAF.txt同样的信息\n  CCF 原始的突变的CCF\n  Coverage 覆盖度\n  Cluster_Id 类别的号 Cluster_CCF 该类别的CCF Probability 分配概率\n  P1 P2\u0026hellip; 每个类别的分类概率（所有的概率加在一起为1）\n  4. 参数表  \n 参考资料   GNU Parallel Tutorial\n  肿瘤外显子数据分析\n ","date":"November 14, 2022","image":null,"permalink":"/post/2022-11-15_sclust/","title":"使用Sclust提取拷贝数"},{"categories":["shell"],"contents":"介绍如何使用pyEGA3下载EGA数据。\n注意 一. 安装下载工具 要求Python版本大于等于3.6。\n使用pip安装，还有其他的安装方式，详情参考github主页：https://github.com/EGA-archive/ega-download-client\nsudo pip3 install pyega3 linux和mac用户检查端口8443和8052是否打开：\nopenssl s_client -connect ega.ebi.ac.uk:8443 openssl s_client -connect ega.ebi.ac.uk:8052 如果端口是打开的，命令行会显示CONNECTED。\nWindows用户，打开下面的网址：\nhttps://ega.ebi.ac.uk:8443/ega-openid-connect-server/ https://ega.ebi.ac.uk:8052/elixir/central/stats/load\n如果端口是打开的，可以直接打开网址。\n二、使用流程 2.1 构建一个credentials文件 将文件命名为：default_credential_file.json\n{  \u0026#34;username\u0026#34;: \u0026#34;ega-test-data@ebi.ac.uk\u0026#34;,  \u0026#34;password\u0026#34;: \u0026#34;egarocks\u0026#34; } 2.2 下载数据 在下载数据前，检查授权数据集，没有授权的数据集无法直接通过这种方法下载。\npyega3 -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; datasets 显示数据集中的文件\npyega3 -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; files EGAD\u0026lt;NUM\u0026gt; 根据检索到的数据集的ID：EGAD\u0026lt;NUM\u0026gt; 来下载数据集\npyega3 -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; fetch EGAD\u0026lt;NUM\u0026gt; --output-dir \u0026lt;/Path/To/OutputDirectory\u0026gt;   \n 下载单个文件\npyega3 -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; fetch EGAF\u0026lt;NUM\u0026gt; --output-dir \u0026lt;/Path/To/OutputDirectory\u0026gt; 还可以限定范围下载，比如从BAM文件中下载1号染色体的数据：\npyega3 fetch -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; --reference-name 1 --format BAM --output-dir \u0026lt;/Path/To/OutputDirectory\u0026gt; EGAF\u0026lt;NUM\u0026gt; 从BAM文件中下载1号染色体0-1000000范围的数据：\npyega3 fetch -cf \u0026lt;/Path/To/default_credential_file.json\u0026gt; --start 0 --end 1000000 --reference-name 1 --format BAM --output-dir \u0026lt;/Path/To/OutputDirectory\u0026gt; EGAF\u0026lt;NUM\u0026gt; 参考资料  github pyEGA3 ","date":"September 28, 2022","image":null,"permalink":"/post/2022-9-01_ega/","title":"使用pyEGA3下载EGA数据"},{"categories":["shell"],"contents":"介绍如何下载dbGAP数据库需要申请的数据。\n 一、下载步骤 使用NCBI的SRA toolkit中的prefetch命令行功能和cart文件或者SRA accession进行下载。cart帮助批量下载数据，SRA accession帮助单独下载数据。\n1. 下载并安装Aspera connect Aspera：一个高速文件传输系统，方便下载数据。\n下载链接：https://downloads.asperasoft.com/en/downloads/8?list\n 确保安装的是connect\n 2. 选择并保存数据信息在cart文件中 除了cart文件，也可以根据SRA accession下载\n  登录dbgap\n  点击My Requests，查看批准的请求\n   确保已经获得批准\n  查看request file   \n 选择dbGap file selctor下载基因型和表型数据\n \n 选择SRA RUN selector下载SRA数据\n \n  选择数据并下载Cart文件   \n  \n 3. 使用prefetch进行数据下载 SRA Toolkit版本低于2.10.2时，需要在此步骤之前增加使用dbGaP repository key进行编译步骤，在通过编译产生的 dbGaP project directory目录下进行后续操作。\n3.1 找到密钥  dbGaP repository key文件包括了SRA Toolkit所需要用来确定申请人和dbga数据所属项目的信息，那么如何下载dbGaP repository key呢？  在action位置找到对应的批准的数据对应的project的get dbGap repository key，下载得到.ngc格式的文件。\n \n 3.2 表型数据 运行prefetch命令下载：\nprefetch --ngc your_file.ngc --cart cart_prj#####_###.krt 使用nohup和末尾的\u0026amp;后台运行，-X 99999999 是下载大小限制\nnohup prefetch -X 9999999999999 --ngc your_file.ngc --cart cart_prj#####_###.krt \u0026amp;   \n 表型数据需要解密，下载下来的表型数据后缀是.ncbi.enc，进行解密：\n \n $ vdb-decrypt --ngc your_file.ngc ~/ncbi/dbGaP-26086/files/ # 整个表型数据存放的文件夹进行解密 解密完成之后，文件的后缀ncbi_enc不见了。\n3.3 数据 cart文件或SRA accession下载\n cart文件  prefetch --ngc your_file.ngc --cart ###.krt  SRA accession  prefetch --ngc your_file.ngc SRR1234567 如果部分sra文件下载失败，提取下载失败的SRRXXX名字，放入一个新的文件中，对这个新的文件进行prefetch下载。\n参考资料  https://www.ncbi.nlm.nih.gov/sra/docs/sra-dbgap-download/ ","date":"September 26, 2022","image":null,"permalink":"/post/2022-9-14_dbgap/","title":"下载dbgap数据"},{"categories":["shell"],"contents":"介绍经常使用的数据库，以及数据的申请和下载。\n一、常用的数据来源和申请渠道 排名不分先后。\n  NCBI（经常在dbGAP申请和下载原始数据）\n  UCSC Xena（TCGA，PCAWG；多组学数据，单细胞RNA测序\u0026hellip;）\n  EGA（原始数据）\n  NGDC（中国的数据库，申请友好，从申请到批准平均一天，必须有职称的人员申请）\n  UK biobank（原始数据在DNAnexus平台上）\n  cbioportal（靶向测序，TCGA，WES/WGS）\n  TCGA（肿瘤基因图谱计划）\n  ICGC Data Portal（国际肿瘤基因组计划；基因异常表达，体细胞突变，表观遗传修饰，临床数据等）\n  GEO（表达数据，芯片数据，单细胞数据）\n  CCLE（细胞系数据）\n  cosmic（细胞系数据）\n  gdsc（细胞系数据）\n  HMF（国外数据库不对中国开放）\n  文献中的supplement或其他储存方式\n  二、数据下载 2.1 NCBI 官网：https://www.ncbi.nlm.nih.gov/\n 数据范围  NCBI包含六大类数据库：\n \n 下载方式  我平时主要使用dbGAP去申请数据，所以着重介绍一下如何下载dbGAP数据：\n下载dbGAP数据\n2.2 UCSC Xena 官网：http://xena.ucsc.edu/\n包含1500+的数据集和50+种癌症类型，支持几乎任何基因组数据，提供简单的数据分析和数据下载。\n 数据范围  支持下载TCGA，PCAWG，ICGC，TARGET，GTEx（多组织、全基因组序列和rna序列数据），CCLE等等数据。\n数据类型包含：https://ucsc-xena.gitbook.io/project/public-data-we-host/types-of-data-we-have\n下载方式    直接通过网页访问和下载：https://xenabrowser.net/datapages/\n  R包：UCSCXenaTools\n  library(UCSCXenaTools) # 1 get all datasets XenaGenerate() # 2 get TCGA BRCA XenaGenerate(subset = XenaCohorts == \u0026#34;TCGA Breast Cancer (BRCA)\u0026#34;) # 3 get all datasets containing BRCA XenaGenerate(subset = grepl(\u0026#34;BRCA\u0026#34;, XenaCohorts)) 2.3 EGA 官网：https://ega-archive.org/\n 数据范围  包含生物医学研究项目产生的遗传和表型数据，包含癌症的数据。\n \n 下载方式  下载EGA数据\n2.4 NGDC 官网：https://ngdc.cncb.ac.cn/\n 数据范围   \n 下载方式  GSA：人类遗传资源组学原始数据归档库，是NGDC的一部分。\n下载GSA数据\n2.5 UK biobank 官网：https://www.ukbiobank.ac.uk/\n需要所在机构给UK biobank提出申请，才能够申请，数据分为4个等级，影像学数据可以提供下载，基因组数据需要通过dnanexus分析。\n 数据范围   \n 包含全基因组测序，全外显子组测序，影像学数据，临床数据等等。\n下载方式  影像学数据可以下载，基因组数据不提供下载，允许在dnanexus云平台分析。\n2.6 cbioportal 官网：https://www.cbioportal.org/\n 数据范围  多维度癌症基因集数据。整合了包括体细胞突变、DNA拷贝数变异、mRNA和miRNA表达量、DNA甲基化、蛋白质丰度、临床信息等数据类型，能够对数据进行可视化和分析。\n下载方式  下载cbioportal数据\n2.7 TCGA 官网：https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga\n 数据范围  肿瘤基因图谱计划，涉及33种癌症类型，包含基因组，表观基因组，转录组和蛋白质组数据。既包含汇总的处理好的数据，也可以下载原始数据。\n下载方式  测序原始数据和涉及个体信息的数据如生殖细胞突变等等需要获得批准才可下载。\n 使用gdc-client下载  下载地址\n  R包TCGAbiolinks下载\n  UCSC xena网页下载\n  2.8 ICGC 官网：https://dcc.icgc.org/\n 数据范围  是国际肿瘤基因组计划，包含基因异常表达，体细胞突变，表观遗传修饰，临床数据等。\n下载方式   直接通过网站下载  https://dcc.icgc.org/releases\n 原始数据受到限制，需要获得批准才可下载  官网提供了多种下载方式，可以进一步参考：https://docs.icgc.org/download/downloading-data/\n申请数据流程：https://docs.icgc-argo.org/docs/data-access/daco/applying\n2.9 GEO 官网：https://www.ncbi.nlm.nih.gov/geo/\n 数据范围  包含各种高通量实验数据的公共存储库，包含检测mRNA，基因组DNA和蛋白质丰度，以及非阵列技术，如基因表达系列分析（SAGE），质谱蛋白质组学数据和高通量测序数据，以及一些单细胞数据。罗列了数据集(GDS)、研究(GSE)、平台(GPL)、样本（GSM）的ID。\n下载方式   官网下载  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE63216\n R包GEOquery  主函数：\ngetGEO(  GEO = NULL,  filename = NULL,  destdir = tempdir(),  GSElimits = NULL,  GSEMatrix = TRUE,  AnnotGPL = FALSE,  getGPL = TRUE,  parseCharacteristics = TRUE )  GEO：字符串，四种类型的ID都可以，比如：\u0026lsquo;GDS505\u0026rsquo;,\u0026lsquo;GSE2\u0026rsquo;,\u0026lsquo;GSM2\u0026rsquo;,\u0026lsquo;GPL96\u0026rsquo;  library(GEOquery) eList \u0026lt;- getGEO(\u0026#34;GSE11675\u0026#34;) eData \u0026lt;- eList[[1]] eData ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 12625 features, 6 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: GSM296630 GSM296635 ... GSM296639 (6 total) ## varLabels: title geo_accession ... data_row_count (34 total) ## varMetadata: labelDescription ## featureData ## featureNames: 1000_at 1001_at ... AFFX-YEL024w/RIP1_at (12625 ## total) ## fvarLabels: ID GB_ACC ... Gene Ontology Molecular Function (16 ## total) ## fvarMetadata: Column Description labelDescription ## experimentData: use \u0026#39;experimentData(object)\u0026#39; ## Annotation: GPL8300  eList2 \u0026lt;- getGEOSuppFiles(\u0026#34;GSE11675\u0026#34;) 2.10 CCLE 官网：https://sites.broadinstitute.org/ccle/datasets\n 数据范围  是肿瘤细胞系数据，包含各种人源肿瘤细胞系的WES数据，WGS数据，RNAseq数据，扩增子数据，突变数据，甲基化数据，拷贝数变异数据等等。\n下载方式  https://sites.broadinstitute.org/ccle/datasets\n2.11 COSMIC 官网：https://cancer.sanger.ac.uk/cosmic\n 数据范围  汇集了和人类癌症相关体细胞突变的信息，数据来源于文献，且由专业的管理员审查。\n下载方式  https://cancer.sanger.ac.uk/cell_lines/download\n \n 2.12 gdsc 官网：https://www.cancerrxgene.org/\n 数据范围  描述了1000个人类癌细胞系，并用100种化合物对它们进行了筛选。可以找到药物反应数据。\nGDSC和CCLE具有重合473例细胞系：\n \n 下载方式  https://www.cancerrxgene.org/downloads/bulk_download\n参考资料   https://www.ncbi.nlm.nih.gov/books/NBK36439/\n  NGDC人类遗传资源组学原始数据归档库 数据申请及下载说明\n  linux下面ftp/wget命令下载文件夹目录\n  EGA\n  GEOquery\n  UK Biobank: An Open Access Resource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and Old Age\n ","date":"September 26, 2022","image":null,"permalink":"/post/2022-9-14_datadownload/","title":"数据收集、申请和下载"},{"categories":["癌症基因组学","生物学"],"contents":"介绍易混淆的概念。\n  异倍性（aneuploidy）  描述染色体数目不是单倍体的倍数的一种状态。在肿瘤基因组中，该名词通常包括染色体臂的拷贝数变异，导致整条染色体变异和导致染色体臂变异的机制非常不同。不过该定义处于动态变化的过程。经典定义中指的是整条染色体的变化，扩展的定义则包含了染色体臂的扩增和缺失，在定量角度，则还包括以下情况：当受影响的基因组/基因/离散事件超过某阈值时。[1]\n 异倍性的涵盖范围\n 目前仍不清楚异倍性本身是如何影响肿瘤形成和进展的。oncogene以及肿瘤抑制基因几乎在所有的细胞类型和cellular context都驱动转化，但是异倍性不一样，它不是一个普遍的肿瘤发生的启动子。\n 染色体不稳定（chromosome instability, CIN）  导致异倍性产生的过程。通过增加基因组异质性，来促进肿瘤发生和肿瘤演化。虽然CIN和异倍性水平高度相关，但是存在一些癌细胞高度异倍性但是染色体稳定。任何可以定量染色体数目的方法都可以检测异倍性吗，但是这些方法不能够定量CIN，定量CIN需要检测染色体错误分离率，需要分析克隆种群中的染色体变异。\n CIN的后果是aneuploidy\nmutagenesis的后果是mutation\n  拷贝数变异负荷（CNA burden）  肿瘤中拷贝数变异的占比，通常定义为基因组受拷贝数变异影响的比例。\n focal copy number alteration  通常指的是包含很少的基因的小的拷贝数改变，和异倍性不同，该名词通常指的是基因的扩增超过1个拷贝。[1]\n euploidy  染色体数目完全是单倍体的整数倍。包括二倍体，三倍体等等，多倍体细胞都是euploidy。\n参考资料  [1]context is everything:aneuploidy in cancer ","date":"September 21, 2022","image":null,"permalink":"/post/2022-9-14_cnv1/","title":"【异倍性系列1】——易混淆概念"},{"categories":["癌症基因组学","生物学"],"contents":"介绍异倍性的概念。\n 异倍性难研究的原因：\n  大范围的一次染色体变异影响数百个基因，不能确定是哪个基因驱动特定的异倍性。\n  异倍性在context不同的情况下，会产生不同的，甚至相反的作用。\n  介绍或者排除特定的染色体存在技术困难。不能够系统的定义异倍性的后果。\n  一、异倍性的影响 1. 促进肿瘤发生   异倍性也会促进肿瘤发生：临床肿瘤样本的研究中发现，异倍性的程度和增殖富集以及细胞循环转录signature存在着正相关（通常被认为是促进肿瘤发生的指标【什么指标】）。mouse embryonic stem cells（mESCs）和human embryonic stem cells（hESCs）中展现出，特殊的单个三倍体同样可以是促进肿瘤发生。\n  肠癌早期的异倍性水平很低，在晚期上升。不止肠癌，在很多癌症中比如子宫癌等等，异倍性都是随着肿瘤进展增加的，可能标记着从局部到扩散。但是也存在癌症不是这种模式，比如乳腺癌和肺癌，异倍性在癌症早期就已经存在。\n  染色体不稳定性和异倍性不只影响原发肿瘤的生长，还塑造转移过程。染色体错误分开会通过增加karyotypic多样性【增加多样性会怎样？如何影响转移？】或通过激活cGAS-cGAMP-STING通路，来促进转移。\n  2. 抑制肿瘤发生   在酵母中发现单条染色体扩增会导致更慢的细胞增殖，以及有害的代谢和生理后果，在小鼠细胞系和人类细胞系中得到类似的结论：单条染色体扩增通常会损害增殖，改变代谢并诱导不同的压力反应。\n  另外，oncogene-transformed的三倍体细胞相比二倍体细胞，肿瘤发生减少了。另外，在癌细胞中，染色体臂gain或loss的发生频率和染色体臂上的coding gene数目相反。从这个角度，异倍性也会抑制肿瘤发生。【分别通过什么实验验证？什么样的数据验证得到的？】\n  二、异倍性被genomic context影响 2.1 体细胞变异发生的顺序影响癌症的演化 在小鼠模型中，获得Ras和Tp53突变的顺序不同，得到的肿瘤表型不同。同样的，TET2突变和JAK2突变的发生顺序不同，也影响了人类骨髓瘤的表征（manifestation）。\n2.2 异倍性可能对其他基因组变异特别敏感 Recurrent aneuploidy模式和特定的调节异常通路有关，也和特定的驱动突变有关。比如：\n  在乳腺癌小鼠模型中，被诱导过表达Myc基因的肿瘤通常在第15号染色体上会得到额外的拷贝，被诱导Her2过表达的肿瘤经常会丢失在第4号染色体上的拷贝。\n  有时，异倍性会先发生，并支配点突变的获得。在超过90%的肾透明细胞癌病人中，第3号染色体臂的丢失是一个肿瘤形成的早期事件（通常由于染色体碎裂事件），在癌症被检测出来之前十年就存在了。剩下的染色体臂位点上发生肿瘤抑制基因的突变（VHL突变通常发生在该事件之后），导致癌症形成。\n  2.3 全基因组扩增和异倍性的关系 WGD几乎发生在1/3的人类癌症中，通常在肿瘤形成早期发生。WGD和异倍性水平上升有关，尤其是染色体丢失相关。二倍体细胞中的染色体丢失容忍度低，染色体丢失经常发生在四倍体细胞中，并促进肿瘤形成。\n2.4 细胞微环境决定异倍性演化 在不同的环境下，受到的选择压力不同。细胞系，器官，小鼠模型等等，在不同的培养形态中，得到的染色体丢失和扩增都不同。\n2.5 免疫系统管理异倍性耐受 免疫识别和异倍性的关系非常复杂，有研究表明，肿瘤异倍性和免疫逃逸的标志物相关，且和免疫治疗反应降低有关。但是也有研究表明 ，异倍性会激活免疫反应。这取决于肿瘤形成的阶段，以及肿瘤微环境中的免疫细胞环境。\n三、异倍性的预后作用 临床上，可以通过不同的技术手段来检测异倍性，包括SNP array，aCGH和DNA测序以及RNA测序技术。\n \n 3.1 异倍性水平高和差的预后有关   cellular DNA ploidy在转移乳腺癌中、早期子宫内膜癌、早期卵巢癌、前列腺癌、以及结直肠癌中是独立的预后标志物。\n  TCGA的数据表明，CNA burden在原发乳腺癌、子宫内膜癌、肾透明细胞癌、甲状腺癌和结直肠癌中和OS/PFS显著相关。\n  结直肠癌中，对超过7000的病人进行系统的meta分析，表明晚期肿瘤的异倍性比早期的更高，表明异倍性可能是stage的预测标志物。mata分析中超过一半的研究表明，异倍性对OS、disease-specific survival和recurrence-free survival有显著的预后能力（独立于tumor stage）。在研究中，diploidy甚至比MSI的预后能力更强。\n  在卵巢浆液性肿瘤中，高度的异倍性和更差的生存有关。在1期卵巢癌，多变量分析中，异倍性是最强的预测因子。\n  在肺癌中，CIN和高度的CNA burden和肺癌的癌前病变的进展有关，类似的，在食管癌中，更高程度的异倍性在Barrett oesophagus将会转变成食管癌的病人中出现。以及，异倍性可以结合其他的标志物来确定疾病进展成高度的异型增生或者癌症。\n  在前列腺癌中，异倍性和PSA（前列腺癌中的标志物） 的预后（RFS）有关。并且，含有异倍性细胞的前列腺肿瘤，被切除后更容易复发。\n  相反的，在多发性骨髓瘤中，高度的异倍性和好的预后有关。\n  高异倍性的肿瘤细胞对化疗通常敏感度低。\n  核型的异质性是由于CIN造成的，可能是CIN而不是异倍性造成了耐药性，另外，异倍性水平和耐药性水平并不是简单的线性关系，因为肿瘤细胞对核型的复杂性有一定的容忍度。另外，极端的异倍性水平和/或CIN也会似的细胞对药物更敏感（少数情况）。\n  3.2 异倍性预后作用中容易混淆的因素  stage/grade：  因为异倍性在肿瘤形成的晚期更遍布，因此表面上可能是异倍性和预后存在关系，实际上是晚期的癌症更容易存在更多的异倍性和更容易扩散。因此构建异倍性和预后关系的时候，应该排除诊断时间和增殖率的影响。\n CIN和aneuploidy  异倍性水平和高度的CIN相关。研究认为，染色体碎裂在癌症中是另外一种主要的异倍性，CIN，染色体碎裂，p53状态三者之间的关系有综述描述过。\n 肿瘤内异质性（ITH）  由于单细胞组学技术，近些年的研究很多。研究表明了肿瘤内异质性在肿瘤进展和治疗反应的重要性。组织ITH和肿瘤增殖率反映了基因层面的ITH， 研究发现，数量和结构上的CIN驱使ITH发展和维持的能力比点突变还强。另外，CNA的异质性（而不是点突变的异质性）和预后的关系很强。\n有研究发现，高CNA burden+低异质性的病人和更好的预后有关，虽然不是直接表明异倍性， 但是CNA burden指的是基因组上受到拷贝数影响的基因占比，很大程度受到异倍性影响。这个研究表明，探索异倍性和临床关系的时候，需要控制ITH的影响。\n四、异倍性靶点 Angelika Amon认为有两种情况\n  targeting the cellular consequences 【】induced by a high degree of aneuploidy (independently of CIN)\n  targeting the unique vulnerabilities induced by specific recurrent aneuploidies.\n  潜在的specific aneuploidy可以分成两种概念：\n  identifying and targeting drivers of recurrent aneuploidies，可能是特定的癌症基因\n  identifying genes linked to these drivers that do not contribute to, but are invariably associated with, the specific aneuploidy.\n  4.1 靶向aneuploidy状态 异倍性造成的细胞压力主要分成五个部分：\n  蛋白毒性的\n  代谢的\n  复制的\n  有丝分裂的\n  低渗的\n  这些细胞压力可能会导致许多(如果不是全部的话)高度非整倍体细胞所共有的独特脆弱性，而不管哪个染色体的拷贝数发生了改变。与这一观点一致的是，在哺乳动物细胞系中，不同的非整倍体被发现诱导类似的转录程序，这些细胞系被基因操纵以容纳非整倍体。\n4.2 特定aneuploidy靶向的passengers  haploinsufficient  在二倍体中丢失一个基因产生的表型。\n  在经常发生丢失的染色体上的haploinsufficient基因，大约27%～45%的重要基因是haploinsufficient基因。单染色体上发生的拷贝数丢失，导致细胞对抑制这些基因更敏感。比如：编码剪切子SF3B1的基因的一个拷贝通常在11%的人类肿瘤中发生丢失，大多数（81%）情况下都是因为染色体臂2q的丢失。乳腺和造血细胞系发生特殊的aneuploidy后，对SF3B1抑制剂更敏感。重要的是，预测这种类型的弱点在人类癌症中普遍存在。\n  相反的，过表达毒性也可以被靶向标定。很多基因过表达后，会降低细胞生存能力和增殖能力，癌症演化过程中拷贝数图谱会避免获得这类基因。当三倍体中存在剂量敏感的基因时，就需要使得他们的基因或表型沉默来让三倍体忍受或者正向选择（比如，通过启动子甲基化）。逆转这些失活机制(例如，通过去甲基化)将对抗特定三倍体所赋予的适应度优势。这将增加一种有趣的可能性，即一些剂量敏感的癌症基因可以通过抑制和激活来靶向。\n  乘客基因的纯合子删除提供了另外一种治疗机会。丢失常染色体或着常染色体臂的两个拷贝是非常罕见的，但是单染色体体能够让基因完整的失活，通过位点突变了，或者部分缺失。这种部分缺失可能和肿瘤形成不相关，但是会导致癌症细胞特异的合成致死。比如，MTAP基因删除在很多癌症中都是经常发生的，由于靠近肿瘤抑制基因CDKN2A，MTAP缺失的细胞积累了MTA（抑制蛋白质PRMT5甲基转移酶的活性），使得细胞对PRMT5抑制剂更敏感。\n  参考资料   [1]context is everything:aneuploidy in cancer.\n  [2]Pan-cancer analysis distinguishes transcriptional changes of aneuploidy from proliferation.\n ","date":"September 21, 2022","image":null,"permalink":"/post/2022-9-14_cnv2/","title":"【异倍性系列2】——异倍性（aneuploidy）"},{"categories":["癌症基因组学","生物学"],"contents":"介绍染色体不稳定性的机制。\n 一、 背景 非整倍体和CIN之间的界限是模糊的，因为没有工具可以区分非整倍体(描述细胞核型的一种状态)和CIN(染色体错分离率增加)。\n这种区别很重要，因为一种染色体倍数可以以不同的方式产生，单染色体通过CIN可能是最常见的非整倍性途径。\n二、造成CIN的因素 正确的染色体分离过程中，需要染色体在细胞周期的G2和M期保持内聚，然后在后期开始时突然中断。\n2.0 有丝分裂中的染色体分离 有丝分裂是经过精心设计的，为了确保所有的姐妹染色单体分裂到不同的子细胞中，其中姐妹染色单体的cohesion是关键，它存在于DNA复制时，是通过the deposition of the cohesion complex建立的，在细胞周期的G2和M期保持，在后期开始时突然中断。精准的时间控制是通过周期蛋白依赖性激酶和SAC（spindle assembly checkpoint）的协同活动决定的。\n cohesion\n  SAC\n 染色体不合时宜地附着到纺锤体微管时，发出维持周期蛋白依赖性激酶活性的信号，会抑制后期的开始并阻止姐妹染色单体cohesion，比如，单向染色体没有附着到单个着丝粒上会出发等待后期的信号，而且，一个单个的没有附着的着丝粒足以阻止后期的发生。一旦所有的染色体都附着到正确的两个方向的着丝粒上，SAC满足条件，姐妹单色单体会突然分离，同时通过细胞周期机器诱导所有的染色体水解性地分裂复合体内聚物，最终，人类细胞的每个着丝粒平均附着了大约20个微管，每个微管都朝向同样的spindle pole来支持公正的染色体分割。但是着丝点-微管连接的随机性质导致了频繁的错误连接，其中单个着丝点同时与来自主轴两端的微管相结合——称为“嵌合”（merotely）。这种嵌合附着通常发生在早期有丝分裂中，并且不被SAC感知，会造成姐妹染色单体在后期迁移到一个spindle pole，造成染色体错误分离。\n 着丝点-微管连接类型\n 和CIN有关的蛋白质：https://www.cell.com/action/showFullTableHTML?isHtml=true\u0026amp;tableId=tbl1\u0026amp;pii=S0960-9822(10)00076-X\n2.1 Cohesion defects 参与染色体cohesion的蛋白质是通过测量出芽酵母中的the efficiency of transmission of minichromosomes来确定的。研究发现，出芽酵母中诱导CIN产生的基因（基因产物调控姐妹染色单体cohesion，包括cohesion complex的subunits）突变，在人类肿瘤中存在。不过没有直接的研究检验这种突变如何影响染色体cohesion。所以，还不知道细胞是倾向于姐妹染色单体过早分离或在后期开始时染色体分离失败，从而无法评估它们在CIN中的作用。另外，然而，利用RNA干扰，cohesion或cohesion调控因子Sgo1/分离酶的急性严重耗散会增加四倍体细胞的数量。四倍体细胞也会发生在过表达分离酶或不耗散形式的securin（分离酶抑制蛋白：切割cohesion激活分裂后期），这表明，cohesion的失调趋向于造成染色体分离全局性的影响，而不是增加单个染色体分离的错误发生。另外，可能cohesion subunit的显性突变通过阻止及时的染色体分离来增加染色体错误分离率，这一点和细胞融合实验中CIN的主要行为吻合。另一种值得探讨的观点是cohesion subunit的突变是否破坏了着丝粒染色质的有序排列，从而导致姐妹着丝粒典型的背对背定向的破坏。除了这些特定的突变，还有其他迹象表明 cohesion的破坏可能导致CIN。比如，例如，与正常组织对照相比，乳腺癌肿瘤样本中分离酶水平较高，过多的分离酶可引起姐妹染色单体过早分离，导致染色体错分离。然而，由于肿瘤和正常组织有丝分裂指数的差异，对肿瘤中mRNA水平改变的解释尚不清楚。这种差异是普遍存在的，并且适用于比较正常组织和肿瘤组织的许多其他领域。\n cohesion complex and cohesion subunit\n 2.2 SAC defects 2.3 Supernumerary centrosomes 2.4 Defect in Kinetochore-microtubule attachment dynamics 2.5 Defect in cell-cycle regulation 三、CIN作为靶点 四、CIN的检测方式 五、细胞能够耐受多少CIN https://aacrjournals.org/cancerres/article/78/23/6529/543014/Tolerance-of-Chromosomal-Instability-in-Cancer\n六、CIN和肿瘤免疫、炎症、转移之间的关系 染色体不稳定性的程度和部位决定了其致癌潜能https://www.nature.com/articles/s41467-020-15279-9\n七、CIN的作用 CIN和异倍性在肿瘤形成的过程中的作用还不清楚。小鼠模型中显示，CIN会偶然展示出散点的、继承性的长期潜伏的状态，主要是肺和脾脏中国，这表明CIN不是一个潜在的癌症驱动因素。在倾向性获得癌症的小鼠中，CIN在不同的情况下，能够促进肿瘤形成，或抑制肿瘤形成。\n异倍性虽然是CIN造成的，但是异倍性也会诱导CIN，实验发现，多一条染色体的细胞通常会产生更多的扩增或缺失。[The presence of extra chromosomes leads to genomic instability]此外，这些细胞表现出CIN的原因和/或特征，如(超细)后期桥，微核，染色体错误分离和胞质分裂失败。CIN通常会造成异倍性，因为CIN总是导致结构或数目异常。\n有丝分裂由数个关键蛋白严格调控的过程，但是也会发生错误，比如由于有丝分裂调控因子发生突变的情况下，一些有丝分裂错误会造成异倍性。例如，纺锤体微管在染色体上的错误附着导致染色体的错误分离，导致数量非整倍体。另外，来自相反方向/子细胞的多个纺锤体微管的附着可以有效地“撕裂”一条染色体，导致两个子细胞获得或失去染色体的一部分，导致结构非整倍体。非整倍体在有丝分裂过程中变得明显，它可能是由有丝分裂之前的过程引起的，例如DNA损伤或复制应激，还有很多缺陷被认为是癌症CIN的基础，比如有丝分裂检查点错误，lagging 染色体（Whole chromosome instability and cancer: a complex relationship），anaphase bridges，mono-multipolar spindles，cytokinesis failure，telomere dysfunction，replication stress，DNA damage等等。然而，需要注意的一点是，从患者肿瘤中确定CIN的机制是非常困难的。此外，许多研究集中于来自不同肿瘤组的细胞系，而CIN机制可能因肿瘤类型和潜在患者而不同(作者未发表的观察结果)。此外，最近的研究发现，染色体错分离在非转化细胞中具有非随机模式，一些染色体的错分离明显高于其他染色体。[Non-random mis-segregation of human chromosomes]常见脆弱位点(Common vulnerable Sites, CFSs)是已知的当细胞经历复制应激时，染色体内更容易破裂的位点，导致结构非整倍体[Genomic instability in fragile sites—still adding the pieces]，这些研究表明，不管是数量还是结构异倍性，都可能发生在非随机的基因组位点，因此，有可能由于染色体不稳定机制不同，在不同类型的癌症中这种模式是不同的。\n除了细胞核的染色体改变外，CIN还能促进异常细胞结构的获得，如微核。微核是含有少量遗传物质的小核结构。这种遗传物质可以是染色体分离时滞后的单个染色体，也可以是DNA损伤[Cell biology: the micronucleus gets its big break]造成的破碎DNA片段。微核含有较少的DNA修复和复制机制，往往容易破裂，增加了染色体异常的进一步积累[Chromothripsis from DNA damage in micronuclei]。它们还可以激活先天免疫系统，因为DNA释放到细胞质中。\n CIN can only be measured in living cells  作用总结   CIN在不同的情况下，能够促进肿瘤形成，或抑制肿瘤形成\n  异倍性虽然是CIN造成的，但是异倍性也会诱导CIN\n  CIN还能促进异常细胞结构的获得\n  CIN率可能比单独评估非整倍体率更能预测肿瘤结局\n  参考文献   [1] Mechanisms of Chromosomal Instability\n  [2]Sister Chromatid Cohesion Control and Aneuploidy\n  [3]Sister chromatid cohesion and separation\n ","date":"September 21, 2022","image":null,"permalink":"/post/2022-9-14_cnv3/","title":"【异倍性系列3】——chromosome instability 染色体不稳定性 (CIN)"},{"categories":["shell"],"contents":"拷贝数变异内容汇。\n【CNV系列1】——易混淆概念\n参考资料  linux下面ftp/wget命令下载文件夹目录 ","date":"September 21, 2022","image":null,"permalink":"/post/2022-9-14_cnvcombine/","title":"拷贝数变异内容汇总"},{"categories":["shell"],"contents":"介绍如何下载GSA数据库需要申请的数据。\n前言 在GSA数据库中，部分数据是需要申请的，注意：必须是PI进行数据申请，否则就算提交了也会被拒绝。\n数据申请成功后，按照下面的步骤进行下载即可。\n \n 下载  FileZilla  根据NGDC的官方文档，可以使用FileZilla，主机：human.big.ac.cn，用户名和密码，忽略端口，点击“快速连接”即可下载数据。但是FileZilla只限于从GSA下载数据到本地，不支持两个服务器传输数据。\nftp命令行  在命令行中，可以访问ftp服务器，再使用mget *来对目录下所有的文件进行下载，但是ftp命令行不支持下载文件夹。\nwget  使用wget来下载HRA000873目录下所有的文件（包含文件夹），\nwget -r -nH -P/ ftp://human.big.ac.cn/HRA000873/* --ftp-user=xxx --ftp-password=xxx   星号*必须有\n  -r 下载文件夹\n  -nH 不包含主机文件夹\n  -P 下载到指定目录，此处我已在该目录下/\n  参考资料   NGDC人类遗传资源组学原始数据归档库 数据申请及下载说明\n  linux下面ftp/wget命令下载文件夹目录\n ","date":"September 14, 2022","image":null,"permalink":"/post/2022-9-14_gsa%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD/","title":"GSA申请数据下载"},{"categories":["shell"],"contents":" recurrence-free survival  从开始治疗到没有癌症迹象存在。\n disease-specific survival  从检测或开始治疗到病人死亡之前的时间。\n","date":"September 14, 2022","image":null,"permalink":"/post/2022-9-14_%E4%B8%B4%E5%BA%8A%E4%BF%A1%E6%81%AF%E5%90%8D%E8%AF%8D/","title":"【临床系列】——名词解释"},{"categories":["shell"],"contents":"介绍R语言中的因子。\n源于Huang昨天问我的这个问题。\n \n 以及，大家在做图的时候应该也遇到一类问题，对横纵坐标的变量进行排序，以及排序并不是按照1，5，10，而是按照1，10，5这样的顺序排列时，需要人为设置因子从而改变阿拉伯数字的顺序。\n一. 什么是因子 在R中，因子适用于分类变量，变量具有已知的值。当需要对变量进行非字母顺序现实的时候，因子可以发挥作用。\n二、使用因子 forcats包是tidyverse包中的一部分。直接加载tidyverse包来使用。\n假设有一个变量记录了月份:\nx1 \u0026lt;- c(\u0026#34;Dec\u0026#34;, \u0026#34;Apr\u0026#34;, \u0026#34;Jan\u0026#34;, \u0026#34;Mar\u0026#34;) 使用字符串来记录这个变量存在两个问题：\n 只有12个可能的月份，如果拼写错误无法检查。  x2 \u0026lt;- c(\u0026#34;Dec\u0026#34;, \u0026#34;Apr\u0026#34;, \u0026#34;Jam\u0026#34;, \u0026#34;Mar\u0026#34;) 使用sort排序则只能按照首字母顺序，而不是月份的顺序。  这两个问题都可以通过因子来解决，为了创建因子，必须创建一个列表存储有效的级别：\nmonth_levels \u0026lt;- c(  \u0026#34;Jan\u0026#34;, \u0026#34;Feb\u0026#34;, \u0026#34;Mar\u0026#34;, \u0026#34;Apr\u0026#34;, \u0026#34;May\u0026#34;, \u0026#34;Jun\u0026#34;,  \u0026#34;Jul\u0026#34;, \u0026#34;Aug\u0026#34;, \u0026#34;Sep\u0026#34;, \u0026#34;Oct\u0026#34;, \u0026#34;Nov\u0026#34;, \u0026#34;Dec\u0026#34; ) 这样就可以创建因子了：\ny1 \u0026lt;- factor(x1, levels = month_levels) y1 #\u0026gt; [1] Dec Apr Jan Mar #\u0026gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec sort(y1) #\u0026gt; [1] Jan Mar Apr Dec #\u0026gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 任何不在这个集合中的值都会被转换为NA。\ny2 \u0026lt;- factor(x2, levels = month_levels) y2 #\u0026gt; [1] Dec Apr \u0026lt;NA\u0026gt; Mar  #\u0026gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 如果想增加一个警告，可以使用readr::parse_factor()\ny2 \u0026lt;- parse_factor(x2, levels = month_levels) #\u0026gt; Warning: 1 parsing failure. #\u0026gt; row col expected actual #\u0026gt; 3 -- value in level set Jam 如果省略了级别（levels），就默认按照字母顺序。\nfactor(x1) #\u0026gt; [1] Dec Apr Jan Mar #\u0026gt; Levels: Apr Dec Jan Mar 有时候，希望级别的顺序与数据中首次出现的顺序相匹配，可以使用unique或fct_inorder（首次出现的顺序）。\nf1 \u0026lt;- factor(x1, levels = unique(x1)) f1 #\u0026gt; [1] Dec Apr Jan Mar #\u0026gt; Levels: Dec Apr Jan Mar  f2 \u0026lt;- x1 %\u0026gt;% factor() %\u0026gt;% fct_inorder() f2 #\u0026gt; [1] Dec Apr Jan Mar #\u0026gt; Levels: Dec Apr Jan Mar 在可视化过程中，改变因子的级别很有用，举个例子，假设想要研究不同的宗教，每天平均看电视的时间：\nrelig_summary \u0026lt;- gss_cat %\u0026gt;%  group_by(relig) %\u0026gt;%  summarise(  age = mean(age, na.rm = TRUE),  tvhours = mean(tvhours, na.rm = TRUE),  n = n()  ) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument)  ggplot(relig_summary, aes(tvhours, relig)) + geom_point()   \n 现在想根据relig的层次来排序，可以使用fct_reorder()，fcr_reorder()有三个参数：\n  f，更改的levels\n  x，用来排列levels的数值向量\n  fun，如果x含有多个数值，可以对数值进行处理，默认是median\n  ggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +  geom_point()   \n 另外还可以对收入和年龄进行统计，\nrincome_summary \u0026lt;- gss_cat %\u0026gt;%  group_by(rincome) %\u0026gt;%  summarise(  age = mean(age, na.rm = TRUE),  tvhours = mean(tvhours, na.rm = TRUE),  n = n()  ) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument)  ggplot(rincome_summary, aes(age, fct_reorder(rincome, age))) + geom_point()   \n 可以使用fct_relevel()，将非金额的放在最后。\nggplot(rincome_summary, aes(age, fct_relevel(rincome, \u0026#34;Not applicable\u0026#34;))) +  geom_point() 因子的另外一个应用场景是颜色，fct_reorder2把因子根据最大的x数值，把y轴数值进行重排。\nby_age \u0026lt;- gss_cat %\u0026gt;%  filter(!is.na(age)) %\u0026gt;%  count(age, marital) %\u0026gt;%  group_by(age) %\u0026gt;%  mutate(prop = n / sum(n))  ggplot(by_age, aes(age, prop, colour = marital)) +  geom_line(na.rm = TRUE)  ggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +  geom_line() +  labs(colour = \u0026#34;marital\u0026#34;) 如果是条形图，可以使用fct_infreq递增排列：\ngss_cat %\u0026gt;%  mutate(marital = marital %\u0026gt;% fct_infreq() %\u0026gt;% fct_rev()) %\u0026gt;%  ggplot(aes(marital)) +  geom_bar()   \n 修改因子级别 最有效的改变级别的顺序的方法是直接更改数值。可以使用fcr_recode()来处理，\ngss_cat %\u0026gt;%  mutate(partyid = fct_recode(partyid,  \u0026#34;Republican, strong\u0026#34; = \u0026#34;Strong republican\u0026#34;,  \u0026#34;Republican, weak\u0026#34; = \u0026#34;Not str republican\u0026#34;,  \u0026#34;Independent, near rep\u0026#34; = \u0026#34;Ind,near rep\u0026#34;,  \u0026#34;Independent, near dem\u0026#34; = \u0026#34;Ind,near dem\u0026#34;,  \u0026#34;Democrat, weak\u0026#34; = \u0026#34;Not str democrat\u0026#34;,  \u0026#34;Democrat, strong\u0026#34; = \u0026#34;Strong democrat\u0026#34;  )) %\u0026gt;%  count(partyid) #\u0026gt; # A tibble: 10 x 2 #\u0026gt; partyid n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 No answer 154 #\u0026gt; 2 Don\u0026#39;t know 1 #\u0026gt; 3 Other party 393 #\u0026gt; 4 Republican, strong 2314 #\u0026gt; 5 Republican, weak 3032 #\u0026gt; 6 Independent, near rep 1791 #\u0026gt; # … with 4 more rows 另外，如果想要整合更多的级别，可以使用fcr_collapse，\ngss_cat %\u0026gt;%  mutate(partyid = fct_collapse(partyid,  other = c(\u0026#34;No answer\u0026#34;, \u0026#34;Don\u0026#39;t know\u0026#34;, \u0026#34;Other party\u0026#34;),  rep = c(\u0026#34;Strong republican\u0026#34;, \u0026#34;Not str republican\u0026#34;),  ind = c(\u0026#34;Ind,near rep\u0026#34;, \u0026#34;Independent\u0026#34;, \u0026#34;Ind,near dem\u0026#34;),  dem = c(\u0026#34;Not str democrat\u0026#34;, \u0026#34;Strong democrat\u0026#34;)  )) %\u0026gt;%  count(partyid) #\u0026gt; # A tibble: 4 x 2 #\u0026gt; partyid n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 other 548 #\u0026gt; 2 rep 5346 #\u0026gt; 3 ind 8409 #\u0026gt; 4 dem 7180 扩展 扩展阅读：\nIf you want to learn more about factors, I recommend reading Amelia McNamara and Nicholas Horton’s paper, Wrangling categorical data in R. This paper lays out some of the history discussed in stringsAsFactors: An unauthorized biography and stringsAsFactors = , and compares the tidy approaches to categorical data outlined in this book with base R methods. An early version of the paper help motivate and scope the forcats package; thanks Amelia \u0026amp; Nick!\n参考  R for Data Science ","date":"September 8, 2022","image":null,"permalink":"/post/2022-8-31_factor/","title":"R——因子"},{"categories":["shell"],"contents":"介绍混淆矩阵，准确率，召回率，ROC曲线以及AUC面积。\n混淆矩阵 注意一点，混淆矩阵是有顺序的，不能随意变换横纵的顺序。\n 混淆矩阵\n 根据混淆矩阵能够得出以下数值：\n 准确率（Precision）  $$ \\frac{TP}{TP+FP} $$\n正确预测为阳性的样本数/所有预测为阳性的样本数\n 召回率（Recall）  $$ \\frac{TP}{TP+TN} $$\n正确预测为阳性的样本数/所有正确预测的样本数\n准确率和召回率通常是不能同时满足的，提升一个就会损失另外一个。\n 真阳性率（True Positive Rate）又叫敏感性  $$ \\frac{TP}{TP+FN} $$\n正确预测为阳性的样本数/所有真正阳性的样本数\n 假阳性率（False Positive Rate）又叫特异性  $$ \\frac{FP}{FP+TN} $$\n错误预测为阳性的样本数/所有真正阴性的样本数\nROC曲线 ROC（Reveiver operating characteristic curve）以图片的形式，展示在分类阈值下分类模型的性能，包含两个参数：\n  真阳性率（TPR）\n  假阳性率（FPR）\n  ROC曲线绘制的是：每个分类阈值下，TPR比上FRR的值。分类的阈值越低，越多的样本被分类成阳性，假阳性和真阳性率都会增加。\n ROC曲线\n 假设分类模型使用的是逻辑回归，可以不同的分类阈值下，计算ROC曲线的点，多次评估模型，但是这样很低效。更高效的方式可以使用AUC，这是一种基于排序的算法。\nAUC曲线 AUC（Area under the ROC Curve），AUC计算从（0,0）到（1,1）ROC曲线下面积。\n AUC曲线下面积\n AUC对所有的可能的分类阈值进行总体性能度量，AUC是模型对随机阳性的排名高于随机阴性的概率。举个例子：按照逻辑回归预测数值把样本从左到右升序排列。\n 预测结果按照逻辑回归得分的升序排列\n 可以这样理解AUC：假设随机挑选阳性(绿色)和阴性(红色)的样本，我的模型有多大的概率把他们正确进行排序。AUC的取值范围是0到1，预测模型100%正确时AUC为1，预测模型100%错误时AUC为0。\nAUC的优点\n  AUC是尺度不变的，计算预测排序的好坏，而不是绝对的数值。\n  AUC是分类阈值不变的，计算模型的预测质量而不是分类模型阈值的选择。\n  pROC中如何计算auc 晚上的时候debug了一下，好奇pROC是如何计算AUC的。大概分为以下几个步骤：\n  根据分类的数值，设定阈值的个数和数值。\n  对样本进行排序。\n  计算每个样本数值点的真阳性率和假阳性率。\n  具体步骤如下：\n 不同的阈值根据predictor来决定，predictor就是每个样本的数值，response是good/bad分类标签。注意direction：auto（选择方向，即：大于阈值为阳性还是阴性）。  这里有113个样本，阈值选择了51个。\n unique.candidates \u0026lt;- sort(unique(predictor))  thresholds1 \u0026lt;- (c(-Inf, unique.candidates) + c(unique.candidates,  +Inf))/2  thresholds2 \u0026lt;- (c(-Inf, unique.candidates)/2 + c(unique.candidates,  +Inf)/2)  thresholds \u0026lt;- ifelse(abs(thresholds1) \u0026gt; 1e+100, thresholds2,  thresholds1) split对数据进行分组，good在一组，bad在一组。\nsplitted \u0026lt;- split(predictor, response) 在direction是auto的情况下，control默认是levels[1]，这里levels[1]是good。\nlevels [1] \u0026#34;Good\u0026#34; \u0026#34;Poor\u0026#34;  然后做了排序，按照降序来排列   predictor.order \u0026lt;- order(predictor, decreasing = decr)  predictor.sorted \u0026lt;- predictor[predictor.order]  response.sorted \u0026lt;- response[predictor.order] 排序，分数从大到小，predictor.order给出数值所在的位置，predictor.sorted给出排过序的数值，response.sorted给出排序数值对应的response的排序。\n计算出每个点的TP（response为阳性的累加）和FP（response为阴性的累加），se和sp，se是tp/ncase（ncase是所有的真阳性病人的个数），sp是control的数目-fp（所有的阴性样本减得到真正阴性的个数就是假阳性的样本）/control的数目(control是所有的阴性样本)，\nTP 1 2 3 4 5 6 7 8 9 10 11 12 12 12 13 13 14 14 14 14 14 15 16 16 17 17 18 18 18 19 19 20 21 21 22 23 24 ... se  [1] 0.02439024 0.04878049 0.07317073 0.09756098 0.12195122 0.14634146 0.17073171 0.19512195 0.21951220 0.24390244 ... table(response) response  0 1 72 41 dups.pred返回逻辑值，数值排序，从小到大检查和前一个数值是否一致，结果调换顺序。 dups.sesp返回每个位置上，se重复为TRUE且sp重复为TRUE的逻辑值，如果存在都是TRUE的，返回TRUE dups选择数值重复或者真阳性且假阳性都存在的\ndups.pred \u0026lt;- rev(duplicated(rev(predictor.sorted)))  dups.sesp \u0026lt;- duplicated(se) \u0026amp; duplicated(sp)  dups \u0026lt;- dups.pred | dups.sesp 检查：sum(!dups)（数值不重复的）是否等于length(thresholds) - 1（总的阈值数目-1），如果出现这种情况就是错误，立刻停止。\nse[!dups]选择数值不重复的或非真阳性且假阳性都存在的情况，\n if (direction == \u0026#34;\u0026lt;\u0026#34;) {  se \u0026lt;- rev(c(0, se[!dups]))  sp \u0026lt;- rev(c(1, sp[!dups]))  }  else {  se \u0026lt;- c(0, se[!dups])  sp \u0026lt;- c(1, sp[!dups])  }  计算auc  diffs.x \u0026lt;- sp[-1] - sp[-length(sp)] # 特异性就是假阳性率是纵坐标 means.vert \u0026lt;- (se[-1] + se[-length(se)])/2 # 敏感性就是真阳性率，是横坐标 auc \u0026lt;- sum(means.vert * diffs.x) # 将图形分块计算每个区间梯形面积（不对曲线进行平滑的基础上）   AUC曲线下面积\n AUC中的bias范围\n参考资料  Classification: ROC Curve and AUC ","date":"August 23, 2022","image":null,"permalink":"/post/2022-8-31_auc/","title":"AUC"},{"categories":["shell"],"contents":"介绍rCGH包的使用方法。\n一. rCGH介绍 rCGH是针对aCGH分析的流程，rCGH通过保存所有参数，确保了完全的可追溯性，并通过交互式可视化方便基因组图谱的解释和决策。rCGH支持以下商业芯片：\n  Agilent（从44K到400K的芯片）\n  Affymetrix SNP 6.0\n  cytoScanHD\n  custom芯片也支持（需要提供合适的数据格式）\n  aCGH数据也能够获取LOH的信息，如何分析通过分析或者其他的方式得到这个标签？\n二、使用流程 cgh \u0026lt;- readAffyCytoScan(\u0026#34;path/to/cytoScan.CNCHP.txt\u0026#34;) cgh \u0026lt;- adjustSignal(cgh) cgh \u0026lt;- segmentCGH(cgh) cgh \u0026lt;- EMnormalize(cgh) 主要的函数介绍，其他的函数介绍在安装好R包后，在R环境中使用 ?function 可获得具体的使用方法。\n 会议安排 ","date":"August 22, 2022","image":null,"permalink":"/post/2022-8-22_rcgh/","title":"rCGH包"},{"categories":["shell"],"contents":"介绍聚类的含义，聚类能够解决的问题，以及聚类性能的度量。\n一. 聚类是什么 聚类属于无监督学习。聚类过程能够形成多个类，每个类对应的概念语义需要使用者来解释。\n二. 性能度量 聚类重要的两个问题：\n  确定聚类的个数\n  聚类结果稳定\n  对于聚类的结果，需要使用某种性能度量来评估好坏，另一方面，如果明确了最终要使用的性能度量，可以直接讲其作为聚类过程的优化目标，从而得到符合要求的聚类结果。\n聚类的结果应该满足：\n  类别之间相似度高（intra-cluster similarity）\n  类别之间相似度低（inter-cluster similarity）\n  聚类性能的度量大致有两类：\n  将聚类结果与某个“参考模型”进行比较，称为“外部指标”（external index）\n  直接考察聚类结果，不利用任何参考模型，称为“内部指标”（internal index）\n  2.1 外部指标 2.1.1 Jaccarad系数（Jaccarad Coefficient, JC）\n2.1.2 FM指数（Fowlkes and Mallows Index, FMI）\n2.1.3 Rand指数（Rand Index, RI）\n2.2 内部指标 2.2.1 DB指数\n2.2.2 Dunn指数\n来源  会议安排 ","date":"June 21, 2022","image":null,"permalink":"/post/2022-8-10_%E8%81%9A%E7%B1%BB/","title":"【聚类1】——介绍"},{"categories":["shell"],"contents":"介绍kmeans分类。\n一. 聚类是什么 聚类属于无监督学习。聚类过程能够形成多个类，每个类对应的概念语义需要使用者来解释。\n二. 性能度量 聚类重要的两个问题：\n  确定聚类的个数\n  聚类结果稳定\n  对于聚类的结果，需要使用某种性能度量来评估好坏，另一方面，如果明确了最终要使用的性能度量，可以直接讲其作为聚类过程的优化目标，从而得到符合要求的聚类结果。\n聚类的结果应该满足：\n  类别之间相似度高（intra-cluster similarity）\n  类别之间相似度低（inter-cluster similarity）\n  聚类性能的度量大致有两类：\n  将聚类结果与某个“参考模型”进行比较，称为“外部指标”（external index）\n  直接考察聚类结果，不利用任何参考模型，称为“内部指标”（internal index）\n  2.1 外部指标 2.1.1 Jaccarad系数（Jaccarad Coefficient, JC）\n2.1.2 FM指数（Fowlkes and Mallows Index, FMI）\n2.1.3 Rand指数（Rand Index, RI）\n2.2 内部指标 2.2.1 DB指数\n2.2.2 Dunn指数\n来源  会议安排 ","date":"June 20, 2022","image":null,"permalink":"/post/2022-8-10_%E8%81%9A%E7%B1%BB-copy/","title":"【聚类2】——kmeans聚类"},{"categories":["shell"],"contents":"该部分会议内容如下：\n  一、Teresa Przytycka 突变模式可以作为一种工具来研究环境、细胞过程和疾病之间的关联，其中包含：\n  signature和分子通路之间的关联\n  突变过程和突变过程之间的关联\n  1. 突变过程和分子通路/突变过程的关联 研究的问题是吸烟和COVID-19易感。\n这部分暂时不是很感兴趣，有空再听一听补上笔记。\n2. 突变过程的可加性问题 存在两种形式的过程：\n  可加性的突变过程，单独的过程对应单个的signature。\n  非可加性的突变过程，产生的signature取决于主要的突变过程。\n  比如，在cosmic中，我们有至少7个signature和DNA错配修复有关，SBS14和SBS20都和错配修复有关，但是SBS14和错配修复+缺陷有关，SBS20和错配修复+D缺陷有关。（D Wojtowics st al., Cell System 2021）\n  为了进一步研究这个问题，该研究人员通过非可加性的模型来推测signature，建立DNA损伤和修复缺陷之间的关联。作者开发了一个工具：RepairSig，用来模拟主要和次要的突变过程。该算法包含三个功能：\n  可以模拟可加性的初级突变过程\n  模拟与主要过程相互作用的非可加性的次要过程\n  推断突变的机会，也就是基因组中容易被破坏或优先修复的位置的分布。（这里存在两个问题，一个是如何确定signature存在的位置，一个是如何推断优先级）\n  使用到的数学逻辑：\n 高阶张量代数，用到了tensorflow。  研究逻辑：\n  先针对模拟数据测试算法是否能够复原正确的分解结果\n  将算法应用在癌症数据中，并且发现通过该算法得到的dMMR signature和实验的结果更一致\n  研究者在乳腺癌中发现（D Wojtowicz et al., Cell System 2021）一个代表了DNA错配修复缺陷过程的次要突变过程。\n  以下是RepairSig的流程：\n  这里面的2个基因区域指的是？以下来自原文：\n  2.1 RepairSig基于的理论背景  NMF分解得到的signature理论上代表不同的突变过程，但是实际上存在多个signature对应一种突变过程的情况，这其中涉及到非单一的突变过程，该问题并未解决。  举例：COSMIC中，有至少8个不同的signature对应错配修复缺陷过程。\n   目前的假设：在基因组内，突变机会有所不同，但是这些突变在不同的癌症基因组中是相同的。不同的基因组中，突变机会也是不同的，因为存在DNA压力，双链断裂等等其他内源性的癌症过程。【不是特理解这部分假设，找个文献看看】\n  我的理解  通过常规的NMF方法可以分离出以下两种signature：    单独的过程\n  单独过程和其他过程的叠加发生过程\n  因此，目前的算法基础上，将叠加过程单独讨论即可。\n叠加过程也是一种重要的过程，具有其发生特殊性。  来源   会议安排\n  会议内容\n  会议公开视频\n  文献：\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-10_analytical/","title":"【2021NCI】signature讲座"},{"categories":["shell"],"contents":"该部分会议内容如下：\n  来源   会议安排\n  会议内容\n  会议公开视频\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-10_normal_tumor/","title":"【2021NCI】signature讲座"},{"categories":["shell"],"contents":"该部分会议内容如下：\n  【Steven Rozen】 Two signatures: one important for public health, one important for DNA damage and repair\n一、AA和肝癌发生 1. 背景 AA指的是aristolochic acid，存在于天然的植物中，广泛应用于中药中。在2013年在尿路上皮癌(upper tract urothelial carcinomas)病人中发现AA 突变模式，2016年发现多种其他的癌症类型中也存在。\n2. 目前研究 目前已经有很多证据证明AA和肝癌发生之间的因果/关联。\n3. 科学问题   AA暴露和肝癌发生之间的剂量关系是什么？是否和肝炎存在相关性？\n  病人发生的暴露是什么形式的暴露？哪一种类型的草本植物？如何准备的？\n  在非洲和拉丁美洲发生了什么样的暴露？\n  通过什么样的途径去进行教育和调控\n  是否存在非侵入性的检测方法（比如血液检测），来检测AA暴露？\n  通过检测可以进行二次预防，已经暴露的个体可以通过检测得知。\n  二、DNA损伤和修复 A hotspot mutation in toploisomerase II alpha (TOP2A) that causes a mutator phenotype in human cancer.\n   topoisomerases切除和重新连接DNA链，使得它们解开或者让他们的超螺旋双链解开。 在yeast topoisomerases II基因上发生的一个突变会造成2-4个碱基的复制，比如：  CGATAC ——C GATAGATA G ACATG——ACATCATG\nCATG——CATATG\n这个研究发现这个过程产生的变异和ID17的模式匹配。ID17的模式中包含以上三种形式的突变。\n  在4个胃癌病人中存在，以及另外3个肿瘤病人中存在，这7个病人的突变情况存在一致性：\n TOP2A p.K743N 4个胃癌病人存在K743N K743N突变和signature ID17存在强烈的相关性（p\u0026lt;10-14）    但是除了ID17的复制，K743N的肿瘤还存在没有在ID17中存在的删除事件。\n  发现这种删除事件和ID8的图谱一致，ID8是一个比较常见的，则产生一个科学问题：\n 是否造成K743N的删除的过程，也同样造成了ID8的产生，又或者是这些删除导致了K743N的突变。  研究发现：\n  存在K743N突变的肿瘤患者和主要是ID8的肿瘤患者存在不同的删除大小分布，通过比较，结论是：在K743突变的肿瘤患者中，2-4个碱基对删除事件更加常见。\n  在更大范围的删除事件中，K743N突变病人和ID8肿瘤病人有不同的删除大小分布，\n    以上这些大小分布的差异表明，很多或大多数的K743N肿瘤中的删除事件是由于K743N突变造成的，而不是ID8变异过程造成的。\n酵母和体外实验表明，在TOP2突变酵母中发现复制的signature，复制的原因是TOP2突变后则不能够重新连接裂口。细胞存在不同的方法去补救这种情况，\n  作者推测由于人类TOP2A K743N突变产生的复制，和酵母TOP2突变产生的复制，两者是来自于一样的过程。\n  作者发现两个序列的motif在K743N肿瘤患者中和复制以及删除有关，并且发现K743N突变的病人中存在的motif在ID8主导的病人中不存在，也印证了该K743N突变和ID8过程不同。\n 作者进一步研究了K743N突变和oncogen的关系。\n 作者的另外一些总结：\n 【Allan Balmain】 The impact of carcinogens, obesity, and chronic inflammatory processes on mutational signatures and cancer risk in mouse tumor models\n这部分没有特别关注，主要是做肿瘤病因学，做了很多小鼠实验，也研究了一些关于拷贝数变异的结果。一个概念：很多已知的致癌基因（carcinogene）不是诱变因素。\n讨论部分  听完全部再回头思考这些问题。\ndata mining 数据挖掘\nPanel discussion 专题讨论会\n 来源   会议安排\n  会议内容\n  会议公开视频\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-10_process/","title":"【2021NCI】signature讲座"},{"categories":["shell"],"contents":"该部分会议内容如下：\n  来源   会议安排\n  会议内容\n  会议公开视频\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-10_translational/","title":"【2021NCI】signature讲座"},{"categories":["shell"],"contents":"该部分会议内容如下：\n  somatic mutations in ageing and disease 1. 背景 对良性组织测序（non-malignant tissue）比较困难，这里使用的是laser capture microdissection，把组织切得更小，这样每次测序大概是200个样本，不需要任何预先对DNA进行扩增，这样可以获得很高质量的基因组。\n主要介绍的是正常组织中的突变模式，以肝脏作为例子进行讲解。\n2. 目前研究 3. 科学问题 来源   会议安排\n  会议内容\n  会议公开视频\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-10_open_session/","title":"【2021NCI】signature讲座-Opening Session"},{"categories":["shell"],"contents":"很多实际问题可以归结为线性规划问题，其目标函数和约束条件都是自变量的一次函数。但是存在另外一些应用也很广泛的问题，目标函数和（或）约束条件很难用线性函数表达，如果目标函数或约束条件中含有非线性函数，就称这种规划问题为非线性规划问题。\n由于非线性函数的复杂性，解非线性规划问题要比解线性规划问题困难得多，而且也不能像线性规划有通用解法，目前还没有适用于解决各种问题的一般算法，各个方法都有自己特定的适用范围。\n一、基本概念 目标函数，或约束条件中含有非线性函数，就称这种规划问题为非线性规划问题。\n1. 非线性规划问题的数学模型 https://blog-1310600458.cos.ap-shanghai.myqcloud.com/10001654751778_.pic.jpg\n二、 无约束问题 1. 极值问题 1.1 局部极值和全局极值 由于线性规划的目标函数为线性函数，可行域为凸集（没有空洞和凹入部分的集合叫做凸集），因而求出的最优解就是在整个可行域上的全局最优解。\n非线性规划却不是这样，有时求出的某个解虽是一部分可行域上的极值点，但是却不一定是整个可行域上的全局最优解。\n1.1.1 极值点存在的条件\n极值点存在的必要条件和充分条件。\n定理1：a\n定理2：\n2. 凸函数和凹函数 在研究非线性规划问题中，凸集、凸函数和凸函数的极值性质必不可缺少。\n2.1 凸集 简单来说，没有空洞和凹入部分的集合叫做凸集。\n2.2 凸函数和凹函数 凸函数和凹函数的几何意义十分明显。\n2.2.1 凸函数的性质\n2.2.2 判断函数是否为凸函数\n定理1：一阶条件\n定理2：二阶条件\n2.2.3 凸函数的极值\n优化的目的，往往是要求函数在整个域中的最小值（或最大值）和最小点（或最大点）。为此，必须将所得的全部极小值进行比较（有时还需要比较边界值）来从中选取最小值。但是定义在凸集上的凸函数来说，则不需要这么麻烦，它的极小值就等于其最小值。\n定理1：\n定理2：\n2.3 凸规则 2.4 下降迭代算法 【一维搜索】\n3. 无约束极值问题的解法 3.1 梯度法 3.2 共轭梯度法 3.3 变尺度法 3.4 步长加速法 三、约束极值问题 实际工作中遇到的大多数极值问题，其变量的取值可能收到一定的限制，这种限制由约束条件体现，带有约束条件的极值问题成为约束极值问题，也叫规划问题。\n非线性规划的一般形式为：\n对于约束的极小化问题来说，除了要使目标函数在每次迭代有所下降之外，还要时刻注意解的可行性问题，寻优工作比较困难。为了实际求解和（或）简化其优化工作，可采用以下方法：将约束问题化为无约束问题；将非线性规划问题化为线性规划问题，，以及其他能够将复杂问题化为简单问题的各种方法。\n1. 最优性条件 2. 二次规划 某线性规划的目标函数为自变量X的二次函数（X表示n维欧氏空间中的向量（点）），约束条件又全是线性的，称这种规划为二次规划。二次规划的数学模型可以表述为以下：\n【】\n以matlab中的quadprog为例，提出示例：\n在quadprog语法中，问题的表示中，此处引入概念，H为海赛（Hesse）矩阵 ：\n按照海赛矩阵的公式得到H矩阵，\n检查H是正定矩阵。\n3. 可行方向法 4. 制约函数法 4.1 外点法 4.2 内点法 四、概念 1. 可微函数 在微积分学中，可微函数是指那些在定义域中所有点都存在导数的函数。可微函数的图像在定义域内的每一点上必存在非垂直切线。因此，可微函数的图像是相对光滑的，没有间断点、尖点或任何有垂直切线的点。\n2. 范数 线性代数中，涉及到相似矩阵及二次型一章节。\n列向量：在线性代数中，列向量是一个 n×1 的矩阵\n参考资料   封面图片来源\n  老司机用python脚本刷微信读书的时长\n  优化理论——二次规划\n  MathWorks 二次规划\n  凸集、凸函数、凸优化的简介与联系\n  凸优化(一): 凸集\n  ","date":"June 9, 2022","image":null,"permalink":"/post/2022-6-09_quangrop/","title":"非线性规划"},{"categories":["shell"],"contents":"NMF（非负矩阵分解）  NMF in mutational signature\n 当得到大量样本时，根据k个分类，可以创建一个计数矩阵M，包含每个样本在每个分类的计数，该矩阵可以通过非负矩阵分解为两个矩阵，这两个矩阵一个代表signature的分类图谱，一个代表每个样本的signature数值。\n而当我们只有一个样本时，则不能通过NMF分解来获得以上提到的两个矩阵，但是可以通过确定的signature图谱矩阵，来推测该样本的signature的exposure。这个步骤就是fitting。\n fitting方法 $$ M \\approx P\\times E $$\n问题简化：\n已知矩阵M和矩阵P，推测非负矩阵E，需要满足Frobenius norm最小化：\n$$ min_{E}\\left|| M-PE\\right||_{F} $$\n该函数的值表示分解的误差。对每个病人g，目标函数可以写为：\nmin⁡∑k=1K(mkg−∑n=1N(engpkn))  s.t.:{∑n=1Neng=1eng≥0.\n  quadratic programming (QP)  可以用R包【quadprog】来实现。QP算法非常快和稳定，但是需要一个预定义的signature矩阵P为全列秩【？？？】，并依赖于问题公式作为Frobenius范数的最小化。\nsimulated annealing (SA)  可以用R包【GenSA】来实现。SA可以广泛用于没有很好预定义的signature矩阵，以及更广泛的错误估计，但是和QP相比，SA收敛到合适的解更慢。重要的是，SA还可以用来随机探索接近最优分解的次优解。\n 稳定性 稳定性分为两个部分：decomposition stability with respect to the input data (输入数据的稳定性)和the stability of the optimal solution (优化解决方法本身)\n 输入数据的稳定性  量化由于不确定性造成的生物学数据的噪音，需要对测量进行重复，bootstrap是一种实用的替代方法。为了确定signature的贡献是如何被分配的，以及评估他们的可信度和稳定性，作者（Teresa M Przytycka et al., 2018）使用QP方法对每个患者的原始突变分类进行1000次可替换的随机重采样，并估计每个bootstrap样本的signature贡献。基于signature贡献的分布，可以对每个signature贡献估计bootstrap的置信区间，以及signature贡献高于特定阈值的经验概率。为了评估不同的signature的贡献稳定性（bias），比如：为了检验bootstrap实验的贡献与原始贡献之间的差异，可以计算bootstrap估计与原始数据中最优贡献之间的差异的均方误差(MSE)。\n优化解决方法本身的稳定性  通过测试优化算法本身的稳定性可以探索signature之间的隐藏的依赖关系。我们对次优分解的空间进行采样，比如：近似分解误差略高于最优解，使用模拟退火方法——修改用于寻找最优解的SA方法，通过强制停止规则，从而当达到目标函数的给定值(分解误差)时报告次优解。把分解误差的阈值设置的比最优方法的要高1、3或5%。该计算对每个病人重复计算1000次，按给定的分解误差阈值随机抽取次优分解空间。随后使用获得的signature分布，和bootstrap分析一样，评估signature贡献的置信和稳定性。\n【how】\n二次规划（QP） 二次规划(QP, Quadratic Programming)定义：目标函数为二次函数，约束条件为线性约束，属于最简单的一种非线性规划。\n参考资料   封面图片来源\n  老司机用python脚本刷微信读书的时长\n  优化理论——二次规划\n  MathWorks 二次规划\n  凸集、凸函数、凸优化的简介与联系\n  凸优化(一): 凸集\n  ","date":"June 7, 2022","image":null,"permalink":"/post/2022-6-07_fit_algorithm/","title":"fit算法"},{"categories":["shell"],"contents":"步骤   安装网易mumu, 通过应用中心安装微信读书\n  安装python包：uiautomator\n  安装adb连接mumu\n  运行python脚本\n   安装网易mumu 网页mumu下载地址: http://mumu.163.com/\n安装完毕后，在应用中安装微信读书，并且登陆自己的账号\n 准备python环境 由于我已经安装过python，因此我只需要通过python来安装新的包。\npip3 install uiautomator  安装adb连接mumu  安装adb  adb相当于mumu的驱动，由于我是macOS系统，因此我只需要在终端使用brew来安装\nbrew install --cask android-platform-tools #安卓设备 写给小雨：windows的用户可以通过以下方式进行安装：\n Windows版本：https://dl.google.com/android/repository/platform-tools-latest-windows.zip\nMac版本：https://dl.google.com/android/repository/platform-tools-latest-windows.zip\nLinux版本：https://dl.google.com/android/repository/platform-tools-latest-linux.zip\n 下载后进行解压，按键windows+r打开运行，输入sysdm.cpl，回车。高级》环境变量》系统变量》path，将adb的存放路径新建路径，然后添加进path中。\n打开命令行测试是否安装成功：\nadb --verision 连接mumu  完成adb安装后，打开微信读书的某一页，记得看一下刚才安装过程中的默认端口提示，我的默认端口是5555，所以我输入以下内容来连接mumu：\nadb connect 127.0.0.1:5555 运行python脚本 python脚本如下，保存并自定义命名：\nfrom uiautomator import device as d  import time  import datetime  import random  #点亮屏幕  def lightScreen():  d.screen.on()  # 自动翻页，翻页后休息5-10秒钟  def autoSwipe():  # 假装看书45-55秒钟(假装是人类在看书。。。)   read_time = random.randint(45,50)   time.sleep(read_time)   print(\u0026#34;阅读花费：\u0026#34;,read_time,\u0026#34;秒\u0026#34;)  # 从（1000,500）到（30,500）   d.swipe(1000, 500, 30, 500) #这里需要根据你的模拟器的具体坐标测试  # 休息一段时间(休息的时间=60秒-看书的秒数)   time.sleep(60-read_time)   print(\u0026#34;休息\u0026#34;,60-read_time,\u0026#34;秒,放松下眼睛~\u0026#34;)  # 执行5小时(300分钟) if __name__ == \u0026#39;__main__\u0026#39;:   all_time = 300   user_input_time = input(\u0026#34;请输入需要阅读的分钟数(请输入正整数):\u0026#34;)   try:   user_input_time = int(user_input_time)   if (user_input_time \u0026gt; 0):   print(\u0026#34;程序将会执行\u0026#34;,user_input_time,\u0026#34;分钟\u0026#34;)   all_time = user_input_time   except:   print(\u0026#34;您输入的值不合法， 将使用默认参数300， 程序将会自动执行5小时\u0026#34;)   pass   for i in range(all_time):   lightScreen()   print(\u0026#34;自动点亮屏幕, 开始阅读。。。\u0026#34;)   autoSwipe()   print(\u0026#34;==\u0026gt;已经阅读\u0026#34;, i+1 ,\u0026#34;分钟\u0026#34;, \u0026#34;还差\u0026#34;, all_time-i-1,\u0026#34;分钟完成阅读\u0026#34;) 我的脚本名为wechat_read.py，运行该脚本。\n bug处理 运行python脚本的步骤中，我发现出现这样的报错：\nandroid - adb more than one device/emulator 检查我所使用的设备：\n$ adb devices List of devices attached emulator-5554 device 127.0.0.1:5555 offline 看来需要终止其中一个：\nadb kill-server emulator-5554 再次运行python脚本就可以了。\n参考资料   封面图片来源\n  老司机用python脚本刷微信读书的时长\n  windows下载安装adb\n  雷电模拟器adb devices返回127.0.0.1:5555 offline分析和解决办法【转】\n  ","date":"April 23, 2022","image":null,"permalink":"/post/2022-4-12_read/","title":"参加读书活动"},{"categories":["shell"],"contents":" \n    cna burden\n    参考资料    ","date":"April 11, 2022","image":null,"permalink":"/post/2022-4-12_qunatify_cnv/","title":"定量拷贝数变异"},{"categories":["shell"],"contents":" \n 本文主要参考：Survival Analysis Part I: Basic concepts and first analyses\n生存分析基本介绍 大多数的癌症研究中使用：\n  Kaplan-Meier plots\n  logrank tests\n  Cox (proportional hazards) regression\n  接下来将会介绍每种方法以及它们的应用。\n生存分析研究的内容   描述生存过程\n  比较生存过程\n  分析危险因素\n  建立数学模型\n  比如cox回归模型。\n生存分析的目的   估计\n  比较\n  影响因素分析\n  预测\n   死亡的时间、治疗到产生治疗反应时间、复发/无复发生存时间（relapse-free survival time; desease-free survival time）。\n删失 只有一部分的个体有生存时间，而另外一部分没有，这个现象叫做删失（censoring），有可能通过以下方式产生：\n  在研究终止的时候，还没有经历这个事件，比如死亡，复发等等\n  在研究过程中失踪了\n  一个病人经历了不同的事件，无法进行进一步随访\n  无法知道起始时间\n  这种删失情况，低估了事件发生的真实(但未知)时间。\n生存和风险 生存数据通常用两种相关的概率来描述和建模，即生存（survival）和危险（hazard）。\n生存概率（survival probability）S(t)（又被称作survivor function）是一个个体从时间起点（如：诊断出癌症）到一个特殊节点的时间t的生存概率。不同t值的生存概率提供了从时间到事件数据的关键汇总信息，非常重要。\n风险通常表示为 h(t) 或 \\( \\lambda(t) \\)，是一个人在一个时间t，被观察到发生事件的概率。换句话说，它表示已经存活到时间t的个体的瞬时事件率。和生存概率不同，生存概率侧重于不发生事件，风险函数侧重于发生事件。\n总而言之，风险与事件(当前)事件率有关，而生存反映了累积的不发生事件。\n KM生存分析 可以对观察到的生存时间（不管是删失还是非删失），通过KM或product-limit方法使用非参数估计来生存概率。假设有k名患者在不同时间的随访期间发生了事件：\n$$ t_1\u0026lt;t_2\u0026lt;t_3\u0026lt;t_4\u0026lt;t_5\u0026lt;\u0026hellip;\u0026lt;t_k $$\n由于事件被假定为独立地发生，从一个区间到下一个区间的生存概率可以相乘从而得到累积生存概率。在时间\\( t(j) \\)时活着的概率\\( S(t_j) \\)，是通过以下公式计算得到的，\\( d_j \\)表示在\\( t_j \\)之前活着的病人数量，\\( d_j \\)是在\\( t_j \\)时发生的事件数量：\n \n 这里\\( t_0=0 \\)且\\( S_0=1 \\)。在事件发生的时间之间 S(t) 值是恒定不变的，估算的概率值只在每个事件发生的时刻发生改变，这个估计公式允许每个病人在已知没有发生事件时为计算贡献信息。如果每个个体都经历了事件(没有发生删失)，这个估计器将简单地减少到t时刻自由事件的个体数量除以参与研究的人数的比率。\n生存概率的置信区间也可以计算出来。KM生存曲线是KM生存概率随时间变化的曲线，它提供了有用的数据总结，可用于估计中位生存时间等指标。大多数生存数据的分布存在较大的偏差，这是不经常使用平均值的原因。\n 风险和累积风险 S(t) 和 h(t) 之间存在一个关系：\n \n 这个公式对于常规的生存分析并不重要，因为它被纳入了大多数统计计算机软件包。这里表明的是，不管是 S(t) 和 h(t) 其中哪一个已知，另外一个会自动地得到。\n不过和 S(t)不同的是，h(t) 没有简单的方法来估计，因此经常使用累积风险 H(t) ，它被定义为风险的积分，或者风险函数下的面积在0和t之间的积分，与log-生存曲线的区别只是符号：\n$$ H(t)=-log[S(t)] $$\n对 H(t) 的解释很难，但也许把 H(t) 理解为死亡率的累积是最简单的方法，或者如果事件是一个可重复的过程，每个个体在时间t时预期的事件数。 H(t) 被作为估算 h(t) 的中间度量，并作为评估模型有效性的诊断工具。简单的非参数估计方法来估计 H(t) 是Nelson-Aalen估计。\n另外一种估计风险的方法是假设生存时间遵循一个特殊的数学分布，下图展示的是四个参数指定的风险和相应的生存概率之间的关系。它说明了随着时间的推移风险率是恒定的(类似于生存时间的指数分布)，根据威布尔模型严格增加/减少危险率，以及使用对数正态模型减少和增加风险率的组合。\n Relationships between (parametric) hazard and survival curves: (a) constant hazard (e.g. healthy persons), (b) increasing Weibull (e.g. leukaemia patients), (c) decreasing Weibull (e.g. patients recovering from surgery), (d) increasing and then decreasing log-normal (e.g. tuberculosis patients).\n  非参数检验比较生存 两个或更多组病人的生存可以使用非参数检验来比较，logrank test是比较两个或更多个生存曲线最广泛使用的方法，这些组可以是治疗组或预后组。如果组与组之间没有差异，则该方法在每个事件时间为每个组计算：自上一个事件以来预期的事件数。然后将这些值相加，得出组i中预期的事件总数 \\( E_i \\) ，用logrank test来比较治疗组i观察到的事件数量 O_i 与预期事件数量进行比较：\n \n 这个值与具有(g−1)自由度的χ2分布进行比较，其中g是组的数量。这样可以计算p值来计算完全生存曲线之间差异的统计意义。\n如果这些组是自然排序的，一个更合适的测试是考虑它们之间存在生存趋势的可能性，例如，年龄组或癌症阶段。在存活率可能增加或减少的基础上计算每一组的Oi和Ei，结果是一个更强大的测试。对于新的Oi和Ei，比较趋势检验统计量与单自由度的χ2分布。\n当只有两组进行比较时，logrank test是检验零假设，即两组风险率的比值等于1。危险比(HR)是衡量两组患者相对生存经验的指标，可通过下面公式进行估计：\n \n 其中\\( O_i/E_i \\)是估计组i的相对风险(relative hazard)，HR的置信区间是可以被计算的，HR对效应强度的解释与风险比类似。HR为1指的是生存没有差异。在实践中，最好使用回归建模技术来估计HR，比如Cox回归，下一篇文章将对此进行描述。\n 截至目前为止，上述内容描述的是对不同的病人组建立KM生存曲线，并通过logrank检验来判断不同组之间是否存在差异。这些方法都是单因素分析。 多因素生存分析 建立多因素统计模型的必要性在于：单因素分析，必然忽略了其他因素的影响。在临床中的实际情况是，有几个(已知)量或协变量，可能影响患者预后。比如对两组患者进行比较：一组是有特殊的基因型，另一组没有，其中一组的个体更老，存活的差异可能由于基因型或年龄，或两者皆有。因此，当调查与任何一个因素相关的生存时，通常需要根据其他因素的影响进行调整。\n另外，虽然logrank检验提供了组间差异的p值，但它没有提供实际效应量的估计;换句话说，它提供了该因素的影响的统计评估而非临床评估。\nezcox包 可视化函数show_forest可以批量完成生存分析并进行绘图，内部调用ezcox包，进而调用coxph函数：\nshow_forest(data,  covariates = ,  controls = ,  vars_to_show = ,  merge_models = TRUE,  add_caption = FALSE,  point_size = 2 )  单因素生存分析  covariate列出所有需要分析的因素，controls是需要控制的变量（不控制变量的话controls不填写即可），vars_to_show为需要展示的因素，结果会按照对covariate中的每个因素进行单因素控制变量分析。举例，假设我们需要对SBS1和SBS2单因素分析，且控制癌症类型变量，则：\nshow_forest(dt,  covariates = c(\u0026#34;SBS1\u0026#34;, \u0026#34;SBS2\u0026#34;),  controls = \u0026#34;cancer\u0026#34;,  vars_to_show = c(\u0026#34;SBS1\u0026#34;, \u0026#34;SBS2\u0026#34;) ) 结果会同时展示这两个函数计算的结果：\ncoxph(Surv(time, status) ~ SBS1 + cancer, data=dt) coxph(Surv(time, status) ~ SBS2 + cancer, data=dt) 关于控制变量cancer，这是一个字符型变量，每种癌症类型都有结果数值，其中会有1个癌症类型作为reference，\n 多因素生存分析  假设我们需要对SBS1和SBS2进行多变量分析，且需要控制变量cancer（不控制变量的话controls不填写即可），函数写为：\nshow_forest(dt,  covariates = c(\u0026#34;SBS1\u0026#34;),  controls = c(\u0026#34;SBS2\u0026#34;,\u0026#34;cancer\u0026#34;),  vars_to_show = c(\u0026#34;SBS1\u0026#34;, \u0026#34;SBS2\u0026#34;) ) 这里可能有点难理解，因为covariates只写了SBS1，是因为多因素生存分析只需要计算一次，就可以展示所有的变量的结果，因此这个函数真实的调用函数如下：\ncoxph(Surv(time, status) ~ SBS1 + SBS2 + cancer, data=dt) 样本量多少足够？ 参考资料   Survival Analysis Part I: Basic concepts and first analyses\n  Survival Analysis Part II\n  https://www.plob.org/article/16141.html\n  latex 工具\n  ","date":"April 11, 2022","image":null,"permalink":"/post/2022-4-10_survival_analysis/","title":"生存分析"},{"categories":["shell"],"contents":" C index介绍 C index 是AUC的扩展，AUC是C index的特殊。\n \n 参考资料  已经猴年马月了，你还不知道C-index？！  ","date":"April 10, 2022","image":null,"permalink":"/post/2022-4-10_c_index/","title":"C index"},{"categories":["shell"],"contents":"Multimodal data 首先我们应该明确什么是multimodal data?\n事物发生或经历时，或者是研究问题包含了多模态。我们通过看、听、闻来体会世界时，就是一种多模态。\n Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities.\n  Multimodal Deep Learning 整合具有不同的层次噪音和存在争议的模态是具有挑战性的。实践中最常见的方法是将不同输入的高级嵌入连接起来，然后应用softmax。\n Example of Multimodal deep learning where different types of NN are used to extract features\n 先建立一个博客，内容后续需要用到的时候再补上。\n详细了解生物信息学中用到的multimodal DL可以看最近的这篇文章：Deep learning with multimodal representation for pancancer prognosis prediction\n参考资料   Multimodal Deep Learning\n  \n  ","date":"April 10, 2022","image":null,"permalink":"/post/2022-4-10_multimodel_dl/","title":"Multimodal deep learning"},{"categories":["shell"],"contents":"注释类型 首先我们应该明确注释是在什么样的文件中？\n1．代码中的注释\n2．文本中的注释\n其次我们应该明确，我们需要去除的注释都是什么样子的？\n1．注释行前有特定字符，比如R代码中的注释#\n2．注释被包含在特定字符之间，比如Python代码中的 ```，以及SRIM软件输出结果开头的注释包含的两行===\n根据注释的类型不同，去除注释的方法也不同，接下来介绍一下如何去除注释。\n 特定字符的注释 使用read.table()读入数据时，设置参数comment.char = '#'，就可以把开头为#的行跳过。\n comment.char： 字符型，注释字符，以此字符开头的行将被忽略   特定字符间的注释【grep跨行匹配】 理想情况是一批文件的注释都在文本的前n行，这样我们只需要通过对read.table()中的参数设置为skip=n，跳过前n行进行读取即可。另外一种情况是，注释包含在两行字符之间，每个文件中注释的行数不确定。举个例子，cat input.txt\n======annotation====== this is annotation ====================== col1 col2 col3 1 2 3 4 5 6 使用shell处理方法如下：\ngrep -Pzao \u0026#34;=(\\n|.)*=\u0026#34; input.txt \u0026gt;mid.txt comm -3 input.txt mid.txt | grep -v \u0026#39;=\u0026#39; \u0026gt;out.txt grep是行过滤工具，用于根据关键字进行行过滤，常规使用方法是不支持跨行匹配的，这里通过正则和-P实现，参数：\n  -P：匹配的pattern是perl兼容的正则表达式。\n  z：一个 0 字节的数据行，但不是空行，将输入和输出数据视为行序列，每个行序列以一个零字节(ASCII NUL字符)而不是换行符结束。像-Z或——null选项一样，这个选项可以与sort -Z这样的命令一起使用，以处理任意文件名。\n  a：像处理文本一样处理二进制文件。这里我的文件显示的是txt的形式，但是匹配时却提醒是二进制，因此加上了-a。这个后面继续说。\n  o：只打印匹配行中匹配的(非空的)部分，每个部分输出单独行。\n  pattern：\u0026quot;=(\\n|.)*=\u0026quot;，跨行\\n匹配包含在=之间的所有内容。\n  更详细的参数直接在命令行输入man grep来查看，如果发现没有-P的话通过grep -V检查一下自己的grep是哪个版本，我最开始使用2.7版本没有-P，更新版本才可使用。\n另外我尝试使用-v来取反来直接一步到位，但是没有成功，原因还没搞清楚，可能和跨行匹配有关。\ncomm用于比较两个有序文件，使用comm是发现diff虽然能够找到文件不同，但是有\u0026gt;等提示两个文件不同的符号，无法一步到位，另外diff需要加上-a，因为文件是二进制的，comm则不管二进制还是普通文本格式，直接可以比较我的两个文件。\n管道操作符后面的grep，是因为结果出现了奇怪的多余的一行===，因此通过grep -v取反输出该行之外的所有内容。但是比较两个文件并没有发现该行来源，需要进一步仔细检查一下。\ncomm -1 A B 不显示在A文件中独有内容(显示B文件独有内容+两个文件共有) comm -2 A B 不显示在B文件中独有内容 comm -3 A B 不显示同时在两个文件中都存在的内容 comm -12 A B 显示A与B公共的部分 comm -23 A B 显示A独有的 comm -13 A B 显示B独有的  参考资料   R数据读取写入\n  vim删除多行注释与添加多行注释 好用\n  grep用法\n  grep\n  两个文件内容比较comm、diff、grep\n  grep跨行匹配\n  ","date":"April 7, 2022","image":null,"permalink":"/post/2022-4-06_%E6%B3%A8%E9%87%8A/","title":"如何去除文本注释"},{"categories":["癌症基因组学","生物学"],"contents":"基本框架（按照从大到细）   预后标志物背景介绍\n  预后标志物的类型（分类标准有作用；存在性质等等），主要关注基因组预后标志物\n  泛癌、特定癌种的预后标志物，这部分聚焦于特定的例子\n  如何开发一个预后标志物，好的预后标志物应该包含哪些特征，预后标志物目前存在哪些问题\n  生物信息学在研究预后标志物中的作用，聚焦于分析手段\n   预后标志物背景介绍 预后是XXXX【概念介绍】\n 从愿景到现实\n TNM分期系统： T(肿瘤大小和深度)N（淋巴结扩散）M（转移的发生与否）恶性肿瘤分类是用于分类癌症扩散程度的全球公认标准，它已为许多实体瘤癌症赢得了广泛的国际认可，但不适用于白血病和中枢神经系统肿瘤。最常见的肿瘤具有其自己的TNM分类。有时也称为AJCC系统。后来，病人的年龄、肿瘤分期、组织学亚型信息也纳入该标准，能够提高预后和预测对治疗反应的能力。[wiki]\n 当新的治疗手段出来时，病人情况变得更加复杂，TNM分期系统面临的问题：\n  个体分子标志物将传统的癌症类型分成更多的亚型，亚型之间存在不同的行为。\n  化疗和生物制剂更有效和广泛的使用。\n  很多新的靶向制剂面向的是特定突变或表达水平的病人。\n   预后标志物的类型 按照标志物性质分类   DNA标志物：单核苷酸多态性（SNPs），染色体异常（比如BCR-ABL转座），拷贝数变异，微卫星不稳定，不同的启动子区域甲基化。\n  RNA标志物：转录因子过表达或低表达，调控RNA（比如microRNA）。\n  蛋白质标志物：细胞表面受体（比如CD20），肿瘤抗原（比如前列腺癌特异性抗原PSA），磷酸化状态，碳水化合物鉴定【翻译可能有误，原有词汇：carbohydrate determinant】，肿瘤释放到血清、尿液、唾液等其他体液中的多肽。\n  如果按照标志物的存在位置，则除了从组织取样的方法，还有目前很热门的血液中cfDNA取样检测，（应该还有pet-ct等等物理性的手段，不过这个应该属于检测）。\n按照标志物作用分类  临床应用中的肿瘤测序\n 可以分为：分类、分程度、分期、预后和治疗选择、种系变异癌症遗传风险等等，通过上图可以发现，部分标志物身兼数职。\n 分类(classification)：比如分癌症亚型，来找到能够对特定治疗/药物受益的人。  通过组织的起源对恶性肿瘤进行分类是最基本的。组织学上通常可以诊断以及定义肿瘤的亚型，但是新的分子标志物可能有不同的诊断，比如\n  针对原发位置不确定的腹部肿瘤，可以结合高通量RNA、蛋白质和组织微阵列芯片技术来有效的区分起源于结肠还是卵巢[6]。\n  区分原发头颈部鳞状细胞癌（HNSCC）和转移肺鳞状细胞癌（SCC）[7]，判断HNSCC原发肿瘤的位置[8]。\n  以及追踪肿瘤发展过程中产生的基因组变异[9]\n   分程度(grade)  每个解剖位置都有自己的组织学程度系统，通过分化程度来区分恶性程度【？】，相比高等级的肿瘤，低等级、高度分化的肿瘤通常严重程度低且预后更良好，高等级的肿瘤生长更快速而且更容易转移。等级分配通常依赖于病理学家的经验，比较主观，但是也有根据病理学家定义的评分【10】。\n 分期(stage)  肿瘤的分类、分期以及程度会用来评估病人预后。但是为了提升预后而去汇总众多指标比较费时费力。\n再考虑使用靶向治疗的情况下，表达标志物通常会代替或完善肿瘤分类、分期、分程度等等。比如：\n  CD20阳性，使用利妥昔单抗(Rituximab)治疗淋巴癌，利妥昔单抗是一种对B细胞上特有的CD20抗原具有高亲合力的单克隆抗体。\n  HER2/NEU阳性，使用曲妥单抗治疗乳腺癌[27]。\n  BCR-ABL转座，使用伊马替尼(imatinib)治疗慢性髓细胞性白血病。\n  \u0026hellip;\u0026hellip;\n部分已经获得FDA批准，部分仍在研究阶段。\n 预后和治疗选择(treatment)      扫描种系变异来找到可能会遗传的癌症风险[2021, David B, Solit]  比如BRCA1和BRCA2种系突变。\n 分析方法角度 如何证明某特征或标志物具有预后作用？通常通过什么样的分析方法来得到这一结果？\n 预测他莫西芬治疗的淋巴结阴性乳腺癌复发的多基因试验[12]  问题：选择的21个基因是否和淋巴结阴性患者复发的可能性相关？\n主要验证手段：\n（1）根据21个基因的表达水平为基础定义一个复发评分，根据复发评分将目标病人分为低、中、高风险三个类别，通过KM生存分析来评估三个类别中10年复发率。发现三组中的复发率高低确实和风险水平高低呈现对应关系。\n（2）多变量cox分析中，复发评分是独立于年龄和肿瘤大小，具有显著预测能力的指标。\n多模态深度学习建立预后模型  详细看这篇文章：Deep learning with multimodal representation for pancancer prognosis prediction，使用了卷积神经网络来对泛癌进行建模，使用c index来判断预测结果好坏。\n三阴性乳腺癌的标志物  详细参考这篇文章：Triple-negative breast cancer: promising prognostic biomarkers currently in development\n预后模型预测患者的生存  详细参考这篇文章：A novel prognostic model predicts overall survival in patients with nasopharyngeal carcinoma based on clinical features and blood biomarkers\n使用lasso回归建立模型，并且使用c index来进行预测\n prognostic and Predictive Biomarkers in Triple-Negative Breast Cancer\n  名词解释 : tumor agnostic   A tumor-agnostic treatment is a drug treatment that is used to treat any kind of cancer, regardless of where in the body it started or the type of tissue from which it developed. This type of treatment can be used when the tumor has a very specific molecular alteration that is targeted by the drug or predicts that the drug is likely to work.\nMost cancer treatments are developed to treat a cancer that has developed in a specific organ or tissue, like breast cancer or lung cancer. A tumor-agnostic treatment treats any kind of cancer as long as the cancer has the specific molecular alteration targeted by the drug.\n  OncoKB   OncoKB is a precision oncology knowledge base developed at Memorial Sloan Kettering (MSK) that collects and stores information on somatic cancer gene alterations. Alterations included in OncoKB are DNA-based, nonsynonymous mutations, rearrangements, insertions and deletions in cancer. The document uses “alterations”, “mutations” and “variants” interchangeably.   参考资料   维基百科\n  Ludwig, Joseph A., and John N. Weinstein. \u0026ldquo;Biomarkers in cancer staging, prognosis and treatment selection.\u0026rdquo; Nature Reviews Cancer 5.11 (2005): 845-856.\n  12:Paik S, Shak S, Tang G, et al. A multigene assay to predict recurrence of tamoxifen-treated, node-negative breast cancer. N Engl J Med 2004;351:2817-26. [PubMed]\n  《精准医疗：从愿景到现实》\n  What is a tumor-agnostic treatment?\n  OncoKB\n   补充 看了一下自然基金的架构，以后我写文献调研类也按照这个模板写，有条理一些。\n  项目的立项依据（研究意义、国内外研究现状及发展动态分析，需结合科学研究发展趋势来论述科学意义；或结合国民经济和社会发展中迫切需要解决的关键科技问题来论述其应用前景。附主要参考文献目录）\n  项目的研究内容、研究目标,以及拟解决的关键科学问题。（此部分为重点阐述内容）\n  拟采取的研究方案及可行性分析。（包括有关方法、技术路线、实验手段、关键技术等说明）\n  本项目的特色与创新之处。\n  年度研究计划及预期研究结果。（包括拟组织的重要学术交流活动、国际合作与交流计划等）\n  ","date":"April 3, 2022","image":null,"permalink":"/post/2022-3-21_prognosis/","title":"癌症预后"},{"categories":["癌症基因组学"],"contents":"一、 拷贝数变异的介绍 1.1 结构变异和拷贝数变异 结构变异（structure variation, SV）是指基因组上大片段碱基的缺失、插入、重复、倒位和易位。这部分变异的频率，和疾病或者表型的关系等等都不明确，另外还有一部分的变异也属于结构变异，比如异态性（heteromorphisms），脆性位点（fragile sites），marker染色体，等臂染色体（isochromosomes），双微体（double minutes）等等，这部分的结构不正常和疾病有关。\n 等臂染色体，是两条基因和形态都一致的染色体臂。\n双微体，是无着丝粒的，染色体外扩增的核染色质，通常包含特定的染色体片段或基因，在癌细胞中常出现。\nmarker染色体，也被称为结构外异常染色体或“多余”染色体。在荧光原位杂交实验中，除了正常的染色体补体外的染色体。\n 拷贝数变异可以看成是特殊的结构变异。鉴定SV对DNA测序深度有较高的要求（一般要\u0026gt;30x），而较低深度的（\u0026gt;5x）覆盖均一的DNA测序就可以鉴定到CNV。对于肿瘤样本中的体细胞CNV和SV，需要比较肿瘤样本和正常样本来得到肿瘤样本特有的变异。基因芯片技术分辨率较低，能检测到的片段都是大片段的（比如1kb以上）。二代测序对于比较大片段的SV，比如是超过150bp长的变异，一个reads是无法涵盖这个变异的，这种情况下会有不同的策略进行间接地推断。\n1.2 结构变异和拷贝数变异造成的影响   Cancer\n  精神类疾病\n  免疫类疾病（红斑狼疮）\n  21三体综合症：做缺陷新生儿产前诊断\n……等等\n  二、拷贝数变异的检测  检测人类基因组中结构变异的方法\n  CNV, copy-number variation ：拷贝数变异\nCGH, comparative genome hybridization：比较基因组杂交\nLCV, large-scale CNV ：大范围拷贝数变异\nFISH, fluorescence in situ hybridization ：荧光原位杂交(\nIndel, insertion and deletion：插入和删除\nMAPH, multiplex amplifiable probe hybridization：多重可扩增探针杂交\nMLPA, mutiplex ligation-dependent probe amplification：多重连接探针扩增技术\nQMPSF, quantitaive multiplex PCR of short fluorescent fragments：短荧光片段的多重定量PCR\nqPCR, quantitative PCR：定量PCR\n 2.1 传统技术 2.1.1 细胞遗传学中常用染色体核型分析：  利用PHA（植物血凝素）刺激成熟的淋巴细胞再次分裂 利用秋水仙素在细胞分裂中期破坏纺锤丝，抑制细胞分裂，形成染色体的形态 通过胰酶消化或缓冲液作用，将染色体显带，通过带纹和数目的分析判断染色体数目和结构的情况  2.1.2 荧光原位杂交（FISH） ……\n2.2 aCGH芯片（array-based Comparative genomic hybridization） 2.2.1 aCGH和CGH的异同 1992年，Kallioniemi发明了无需细胞培养的比较基因组杂交技术（CGH）：\n 用不同荧光分别标记肿瘤和正常对照样本，把两者DNA等量混合 与中期细胞染色体进行杂交 通过比较肿瘤和正常对照荧光信号的相对剂量来检测拷贝数异常，DNA局部扩增或缺失导致单个染色体上的荧光强度增加或减少  该方法的缺点是无法检测整个染色体组数目增加的多倍体变化，也无法检测DNA总数量不变的染色体畸变。\n1995年。Schena等利用CGH技术结合微阵列基因探针（aCGH）定量检测多个基因的表达，随后aCGH应用于检测拷贝数异常。和CGH相比，aCGH的优点是：该技术不需要染色体培养，一次杂交实验即可在整条染色体或染色体区带水平对不同基因组间DNA序列拷贝数的差异进行检测并定位，aCGH将两个样本的DNA用红绿两种荧光进行标记，然后与芯片进行杂交，用据分析软件检测红绿两种荧光的比值，分析相对对照样本，实验样本的DNA拷贝数是增加还是减少。\n 比较基因组杂交\n \u0026ldquo;dye-swap\u0026quot;检测了假信号，该方法进行了两次检测， 在第二次检测时颠倒肿瘤和正常样本的标签，这样，如果两次检测的信号有不对称的情况时，认为是 假信号。\n 基于aCGH芯片技术的全基因组拷贝数变异检测\n 对于芯片技术来说，探针的本质是一段很短的DNA序列，是提前设计好的，对于倒位（inversion）和易位（translocation）来说没有改变DNA的剂量，在芯片上看不出来，另外由于插入（insertion）的片段可能是原先基因组上没有的，而探针是根据已知序列进行设计的，因此插入也无法进行检测。\n2.2.2 数据拟合 软件最原始的输入数据为荧光的信号值，信号值是有波动的，而拷贝数一定是一个整数，算法通过对原始的荧光信号值进行拟合，以确定对应染色体片段的拷贝数水平。\n 离散的拷贝数数值\n  去除噪音 检测正常、增加、减少的拷贝数 断点的检测  2.2.3 断点检测之CBS（circular binary segmentation） 是一种基于染色体芯片数据检测拷贝数变异的方法，目前也适用于二代测序数据的拷贝数变异检测的方法。\nA. CBS的背景 芯片数据存在噪音，发生在连续区域的变异通常会覆盖多个标记(marker)，因此标记不能够完全反映test样本中真实的拷贝数变异情况，因此需要一个方法来将染色体划分成多个片段，保证每个片段中的拷贝数是一致的。循环二元分割算法是从1975年的二元分割算法改进来的，它提供了一种自然的方法将染色体分割成相邻的区域，并利用permutation reference distribution避免了数据的参数化建模。\nB. CBS算法 循环二元分割算法(circular binary segmentation, CBS)是目前常用的芯片数据分段算法, 其优势在于利用相邻待测区间的数据均值差构建 t 统计量, 进而精确检测不同变异区域间的分段点。介绍一下估计拷贝数变异区域位置与变点检测（change point detection）的关联。\n原始的变点检测策略是1975年Sen和Srivastava提出来的，假设\\( D \\) 是芯片数据，\\( n \\)是长度，设\\( \\mu_{i} \\)和\\( \\mu_{i}^{\u0026rsquo;} \\)分别为\\( D \\)第一个元素\\( i \\)的log2ratio均值和后一个元素\\( n-i \\)的log2ratio均值，把\\( i \\)定位在使得\\( \\left | \\mu_{i}-\\mu_{i}^{\u0026rsquo;} \\right | \\)最大化的位置上，并且使用\\( t \\)检验来判断两段信号差异是否显著，如果显著，则标记此处为一个变点。原始的方法是检验1个单个变点的情况，不能够在大片段中检验出小片段的拷贝数变化，CBS算法所做的改进在于：\n并非考虑单个的变点，而是假设片段环绕成一个圆圈，第一次运行时将数据划分为两个弧，从\\( i \\)到 \\( j \\)为第一个弧，从\\( j \\)经过\\( n \\)和\\( 0 \\)再到\\( i \\)为第二个弧，找到使得两段均值最大的位置，并使用\\( t \\)检验判断两段的均值是否显著有差异，差异显著则标记为一个变化片段（2个变点），接下来，递归地将算法应用于三个结果片段：从0到\\( i \\);从\\( i \\)到\\( j \\)，从\\( j \\)到\\( n \\)，直到发现不了新的变化片段。\nBS算法图解\nCBS算法图解\n CBS算法存在的问题是存在边界效应（edge effect），即假设\\( i \\) 和\\( j \\)符合\\( \\left | \\mu_{i}-\\mu_{i}^{\u0026rsquo;} \\right | \\)最大化，不管是\\( i \\)贴近1还是\\( j \\)贴近n，有可能只存在一个真正的变点而非预想的两个变点的情况。可以通过以下办法来避免：\n 首先使用BS测试（CBS的前身）是否数据支持\\( i \\)作为一个变点，如果不是则撤销对\\( i \\)的标记 同样对\\( j \\)也进行BS测试，是否数据支持\\( j \\)作为一个变点，如果不是则撤销对\\( j \\)的标记  C. 使用CBS算法进行分段的工具  R包：DNAcopy CNVkit：适用于全外显子，目的区域靶向测序等数据的CNV检测 ……  2.3 SNP芯片 SNP芯片本质是基于染色体区域内的SNP分型结果来判断对应拷贝数，SNP芯片的分型是通过比较A/B两种allel对应的荧光信号强度的比值来确定的。除了提供 拷贝数信息，SNP array还提供了表型信息比如：杂合性位点丢失，证明了 删除 以及 单亲二体（Uniparental disomy, UPD)的存在。\n 单亲二体： 是指子代的一对同源染色体全部或者 部分 来自父亲或母亲中的一方\n CCLE和TCGA里的拷贝数变异文件就是由芯片数据——Affymetrix SNP 6.0 array数据处理得到的，该芯片包括单核苷酸多态性（SNPs）和检测拷贝数变异的探针，是全基因组水平的探针，包含超过906,600 SNPs和超过946,000检测拷贝数变异的探针，和SNP 5.0 Arrays的482000 SNPs相比较，新增的424,000 SNPs是HapMap项目（在全基因组规模上，确立SNP在人群中的常见分布和传递模式）中得到的，新的标记在染色体X和Y以及线粒体SNP等表现更好。946,000个非多态性的拷贝数探针中744,000个探针是通过空间分布选择得到的，其他202,000是基于Toronto Database of Genomic Variants（DGV）已知的拷贝数变化，这样的数据可以用来从头检测拷贝数变化，以及通过对SNP和已知的拷贝数多态性位点（copy number polymorphism loci）进行关联研究。\n SNP array 5.0/6.0的检测数据来源\n 使用Affymetrix SNP 6.0 数据来定义重复的基因组区域以及该重复区域的拷贝数，这个pipeline使用已有的TCGA 第2水平的数据（normalized data） 和R包 DNAcopy 来实现CBS算法进行数据的分割。级别1的数据是原始的芯片强度数据，浏览了一下GDC上的数据，数据格式为CEL的，测序平台为SNP 6.0，这批原始数据是私密的。\n 级别1的数据\n 通常是用PICNIC等软件来处理原始的数据，得到分段记录的文件。TCGA级别2的Tangent Copy Number文件，通过对芯片强度数值进行归一化，估计原始的拷贝数，并且做切线归一化（tangent normalization）即减去在正常样本中发现的变异。Tangent Copy Number数据包含下面5列数据：\nChromosome Start End Num_Probes Segment_Mean 用DNAcopy包将TCGA级别2的 Tangent Copy Number 数据转换成 Copy Number Segment 数据，该文件以制表符分隔的格式将相邻染色体区域与log2比率段关联，与每个染色体区域相关联的带有强度值的探针的数量包括在该文件中(没有强度值的探针不包括在计数中)，在拷贝数分割过程中，从男性中去除伪常染色体区探针集，从女性中去除Y染色体片段。这里还有一个 masked copy number segments ，使用的是和上述一样的方法，不同之处在于过滤步骤，去除了Y染色体和探针集中种系拷贝数变异，针对该数据使用GISTIC2来获得拷贝数变异数值文件Copy number Estimate，最后只保留编码蛋白质的基因，以及这些基因的拷贝数变异数值，噪音阈值设定在0.3，认为:\n CNV数值\u0026lt;-0.3则分类为删除事件（-1） CNV数值\u0026gt;0.3则分类为扩增事件（+1） -0.3\u0026lt;CNV数值\u0026lt;0.3则分类为中性事件（0）   数据级别对应的数据类型\n  示例：个体15号染色体q臂的log R Ratio (LRR)和B Allele Freq (BAF) values\n 2.3.1 SNP芯片数据工具   PennCNV\n  DNAcopy\n  ……\n  2.3.2 隐马尔可夫模型 2.4 全基因组（WGS） 全基因组的成本很高，针对全基因组CNV的检测，开发了一种CNV_seq的测序策略，指的是低深度全基因组测序，只需要5X的测序深度，就可以有效的检测CNV，全基因组CNV分析的算法/策略（以删除和新序列插入为例）：\n Genome structural variation discovery and genotyping\n 二代测序技术采用双端测序（图中绿色部分就是所测序列两端），得到双端的序列比对到参考基因组上，可以根据比对到参考基因组上的两端序列之间的间隔推测变异是否存在，大家都以参考基因组为基准，推断方法有四种以上主要的策略：\n  Read Pair（RP） ： 是根据双端测序插入片段的长度分布来检测CNV的，以1为例，假设测得的基因序列是500bp的reads，但是匹配到参考基因组上发现，两端的序列匹配上之后序列比500bp还要长，那么认为两端序列之间发生删除后才是我们测到的序列。\n软件工具：BreakDancer；PEMer；Ulysses\n  Read Depth（RD） : 通过测序堆在参考基因组上，某处reads很多堆在一起(可能是扩增)，某处reads缺失（可能是插入），扩增的序列往往指在参考基因组上存在（或者相似度很高），而novel sequence insertion中novel意思是这段序列通常比较特殊，在参考基因组上找不到。\n软件工具：CNVnator；ERDs\n  Split Read（SR） : 把reads mapping到基因组上，刚好reads被切开，切开的两端可以很好的mapping到参考基因组上。\n软件工具：Pindel；SVseq2\n  Assmbly : 从头组装，不是和参考基因组去比对。通过测序得到的序列从头组装后得到的contig直接和参考基因组做序列比对，就可以知道发生变异的情况。\n  现在这四种方法都用，做得较好的软件：manta sv同时用到了这四种策略。\n2.5 全外显子组（WES） 由于全基因组的成本挺高的，又考虑到外显子上的变异可能更具有致病性，因此可以基于WES做CNV分析，由于CNV区域的长度可能横跨了多个外显子或者基因，断点可能位于外显子以外的位置，所以PR，SR的策略无法应用到WES的CNV分析中，只能通过RD的策略进行分析。\n可以使用CNVkit、XHMM来鉴定WES的CNV。\n2.6 靶向测序 https://cloud.tencent.com/developer/article/1556103\nhttp://www.njnad.com/marketing/421.html\nhttps://cloud.tencent.com/developer/article/1556107\n \n  \n 下期内容：  相对和绝对拷贝数数值的获取 示例：ABSOLUTE 使用隐马尔可夫模型检测CNV 主流获取CNV的工具比较 靶向测序检测CNV GATK所使用的拷贝数calling算法 CBS算法除了判断断点，如何判断拷贝数数值？ PennCNV能够同时判断断点和拷贝数数值咩？总强度用来判断拷贝数数值，相对强度BAF用来判断LOH咩？ 我认为PennCNV有一个缺点就是，但是实际上可能拷贝数扩增不止4，那么其他的软件是怎么解决这个问题的？为了优化HMM的参数，使用Baum-Welch算法去优化，使用Vitebi算法来推断最可能的状态路径。排除了所有包含SNP小于等于2的CNV，因为这些CNV很肯定是假阳性的结果。 马尔可夫链依赖于初值的选择，初值的选择影响大吗？ ……  参考资料   染色体核型分析介绍\n  PennCNV官方文档\n  拷贝数变异检测算法之CBS算法详解\n  Kallioniemi, Anne, et al. \u0026ldquo;Comparative genomic hybridization for molecular cytogenetic analysis of solid tumors.\u0026rdquo; Science 258.5083 (1992): 818-821.\n  Schena, Mark, et al. \u0026ldquo;Quantitative monitoring of gene expression patterns with a complementary DNA microarray.\u0026rdquo; Science 270.5235 (1995): 467-470.\n  Pinkel, Daniel, et al. \u0026ldquo;High resolution analysis of DNA copy number variation using comparative genomic hybridization to microarrays.\u0026rdquo; Nature genetics 20.2 (1998): 207-211.\n  Alkan, Can, Bradley P. Coe, and Evan E. Eichler. \u0026ldquo;Genome structural variation discovery and genotyping.\u0026rdquo; Nature Reviews Genetics 12.5 (2011): 363-376.\n  Wang, Kai, et al. \u0026ldquo;PennCNV: an integrated hidden Markov model designed for high-resolution copy number variation detection in whole-genome SNP genotyping data.\u0026rdquo; Genome research 17.11 (2007): 1665-1674.\n  图：Kai Wang et al. Genome Res. 2007;17:1665-1674\n  Circular Binary Segmentation from Jeremy Teibelbaum\u0026amp;rsquo;s blog\n  CKVkit：https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004873\n  Ming-Lian\u0026amp;rsquo;s blog\n  隐马尔可夫模型：https://www.cnblogs.com/skyme/p/4651331.html\n  coursera生物信息学：导论与方法第四周\n  动态贝叶斯网络\n  封面图来源\n  ","date":"March 19, 2022","image":null,"permalink":"/post/2022-3-29_cnv/","title":"CNV"},{"categories":["如何科研"],"contents":"最近在修改文章，感觉自己的英文表达能力和学术写作能力太弱，刚巧看到施一公教授分享的公众号文章，觉得收获很大，记录一下。\n如何写论文 1．要写好科研论文，必须先养成读英文文章的习惯，争取每天30-60分钟。刚开始可以选择以读英文报纸、英文新闻为主，逐渐转为读专业杂志。我会在近期专门写一篇博客文章介绍一套行之有效的增强读专业杂志能力的办法。\n2．写科研论文，最重要的是逻辑。逻辑的形成来自对实验数据的总体分析。必须先讨论出一套清晰的思路，然后按照思路来做图(Figures)，最后才能执笔。\n3．具体写作时，先按照思路（即Figures）写一个以subheading为主的框架，然后开始具体写作。第一稿，切忌追求每一句话的完美，更不要追求词语的华丽，而主要留心逻辑（logic flow），注意前后句的逻辑关系、相邻两段的逻辑关系。写作时，全力以赴，尽可能不受外界事情干扰（关闭手机、座机），争取在最短时间内拿出第一稿。还要注意：一句话不可太长。\n4．学会照葫芦画瓢。没有人天生会写优秀的科研论文，都是从别人那里学来的。学习别人的文章要注意专业领域的不同，有些领域（包括我所在的结构生物学）有它内在的写作规律。科研文章里的一些话是定式，比如 “To investigate the mechanism of …, we performed …”, “These results support the former, but not the latter, hypothesis …”, “Despite recent progress, how … remains to be elucidated …” 等等。用两次以后，就逐渐学会灵活运用了。在向别人学习时，切忌抄袭。在美国一些机构，连续7个英文单词在一起和别人的完全一样，原则上就被认为抄袭（plagiarism）。\n5．第一稿写完后，给自己不要超过一天的休息时间，开始修改第二稿。修改时，还是以逻辑为主，但对每一句话都要推敲一下，对abstract和正文中的关键语句要字斟句酌。学会用“Thesaurus”（同义词替换）以避免过多重复。第二稿的修改极为关键，再往后就不会大改了。\n6．第二稿以后的修改，主要注重具体的字句，不会改变整体逻辑了。投稿前，一定要整体读一遍，对个别词句略作改动。记住：学术期刊一般不会因为具体的语法错误拒绝一篇文章，但一定会因为逻辑混乱而拒绝一篇文章。\n用最简单的话表达最明白的意思，但一定要逻辑严谨！\n 一些细节  odds ratio而不是odd ratio。   参考资料   施一公：如何一个通宵写出一篇Nature？ 点击原文\n  Nature:How to write a good research paper title\n  ","date":"March 19, 2022","image":null,"permalink":"/post/2022-3-28_%E7%A7%91%E7%A0%94%E5%86%99%E4%BD%9C/","title":"如何进行科研写作——【转载】"},{"categories":["R","生信"],"contents":"自问   是否有方法的比较\n  比较的标准是什么，如何判断应该使用哪种\n  聚类的共性如何展示\n  核心的算法是什么\n  仔细分析这个包的目的是什么？【一个包为什么可以被开发出来的，开发出来使用的优势在于？一个包可以包含的内容，一个包为了一种类型的分析可以提供哪些？】\n  基本框架   聚类分析介绍\n  理论：聚类算法\n  实操：聚类工具\n   【R包】cola 为了检验子群分类的稳定性，通常采用一致性聚类（consensus clustering/consensus partitioning）。通过对数据随机选取子集重复聚类，并对聚类的鲁棒性进行汇总，最终给出所有样本的聚类结果。cola包就是用来做一致性聚类的。\ncola包的优点在于：\n  它将共识聚类过程模块化，可以在不同的分析步骤中轻松地集成各种方法。\n  它为解释结果提供了丰富的可视化。\n  它允许同时运行多个方法，并提供功能以直接的方式比较结果。\n  它提供了一种新的特征提取方法，该方法可以更有效地分离子组。\n   cola包的流程  flowchart\n （1）清洗输入矩阵（可选步骤）。这一步骤后续会进一步说明。\n（2）通过最高分数提取行的子集。这里的分数是通过特定方法（the top-value method）计算得到的。针对基因表达分析或甲基化数据分析，大多数研究中会使用到方差最高的行。这个方法可以是绝对中位差（MAD）或其他。\n（3）对处理后的矩阵进行归一化（可选步骤）。比如基因表达可能会用到，而甲基化数据则不一定会用到。\n（4）对处理后的矩阵根据一定概率，对行或列进行随机采样，并对矩阵的列使用特定的聚类方法进行聚类，同时尝试不同的分类个数。\n（5）重复第4步并汇总所有的分类。\n（6）进行一致性聚类分析并给出结果最稳定的分类数目。\n（7）在预测的类别之间进行统计学检验，找出显著不同的的行。比如：找出类与类之间最特殊的基因。\n（8）如果矩阵的行和基因有关，可以进一步进行下游分析比如富集分析等等。\n cola包的使用步骤 假设矩阵存储在名为mat的对象中（列为样本，行为特征），使用cola进行一致性聚类，\n主要分为三个步骤：\n（1）调整矩阵。\nmat = adjust_matrix(mat) # optional 移除有很多NA值的行；移除方差太低的行；当每行中NA值少于50%时，推算处理NA值；处理每行中的离群值。\n（2）使用多种方法运行一致性聚类。\nrl = run_all_consensus_partition_methods(mat, cores = ...)  聚类方法：参数partition_method = ...  包括以下几种：\nhclust\nkmeans\nskmeans:skmeans`\ncluster::pam\nMclust::mclust\n 提取行的方法：参数top_value_method = ...  默认提取顶层行的方法包括：SD（标准差）、CV（变异系数）、MAD（绝对差中位数，原数据减去中位数后得到的新数据的绝对值的中位数）、ATC（cola中特有的）。在该步骤中，可以选择运行所有方法，也可以通过设定参数来运行其中某个方法，甚至可以通过register_top_value_methods()使用自定义函数来进行提取。\n如果数据量非常大的时候，可以随机采样一个样本子集来分类（自定义），随后进行聚类。\n另外，2.0.0版本提供了新的函数:\nhierarchical_partition(mat, cores = ...) 其通过层次方法进行一致性聚类。矩阵很大时，设置参数subset以便对样本降采样来聚类。\n（3）生成具体的HTML文件来展示完整的分析过程。\ncola_report(rl, output_dir = ..., cores = ...) （4）基于分类结果预测新样本的类别\npredict_classes() 主要应用在两个场景中：\n  在新的数据中预测样本的类别。\n  在大样本中，用户使用随机的样本子集进行分类，随后需要对剩余的样本同样进行分类。\n  为了使用cola的分类结果，需要对结果进行选择，比如：\ndata(golub_cola) res = golub_cola[\u0026#34;ATC:skmeans\u0026#34;] res predict_classes()需要至少三个重要参数：\n  ConsensusPartition对象\n  子集的数量\n  新的矩阵\n  新的矩阵需要有和最初分析使用的矩阵一样的行数，即拥有同样的特征，且特征排列必须一致。对新矩阵scaling的方法也要和最开始使用的一致。\n步骤如下：\n（4.1）对于所提供的ConsensusPartition对象和选定的k，由get_signatures()提取区分类的特征(signature)。当特征(signature)数大于2000时，只随机抽取2000个特征(signature)。\n（4.2）特征质心矩阵为k列矩阵，其中每一列为对应类样本的质心，即样本间均值。如果在cola分析中对行进行缩放，则特征质心矩阵是缩放值的均值，反之亦然。请注意，在计算质心时，silhouette分数小于silhouette_cutoff(默认值0.5)的样本将被删除。\n（4.3）拥有特征质心矩阵和新的矩阵，能够进行预测。\n对于新的矩阵中的每个样本，需要找到其最近的质心，有三种方法：欧氏距离(Euclidean distance)，相似性距离(cosine distance)，斯皮尔曼距离(Spearman distance)。\n简单来说步骤如下：\n 获取res矩阵  mat = get_matrix(res)  对矩阵进行归一化处理  mat2 = t(scale(t(mat)))  通过res得到三个分类，进而预测mat2的类别  cl = predict_classes(res, k = 3, mat2) cl  ATC \n  参考资料   cola包文档\n  Picture\n  ","date":"March 19, 2022","image":null,"permalink":"/post/2022-3-24_cluster/","title":"聚类分析之cola包【没写完】"},{"categories":[" "," "],"contents":"安装包   pyexecjs：这个库主要是将 JS 代码运行在本地的 JS 环境中\n  lxml\n  参考资料：   https://juejin.cn/post/6844903798935126030\n  封面图来源\n  ","date":"March 16, 2022","image":null,"permalink":"/post/2022-3-17_sport/","title":"Python实现自动定场"},{"categories":["癌症基因组学","生物学"],"contents":"基本框架 拷贝数变异的博客框架：\n  介绍拷贝数变异的从属关系，和结构变异的关系；克隆性拷贝数变异和亚克隆拷贝数变异\n  拷贝数变异检测手段，拷贝数变异检测方法/算法（这个比较多，可以分多点讲）\n  结构变异的检测手段等等\n  引发拷贝数变异的机制\n  拷贝数变异的应用价值\n  拷贝数变异的最新研究动向\n  该博客的框架：\n  简单过一下拷贝数变异的介绍。\n  详细阐述每一种拷贝数变异的机制和机制发现的过程。\n  癌细胞中的基因组变异可以分为两种主要的类别：\n（1）小的变异：单核苷酸变异，双核苷酸变异，以及小的插入和缺失等等。\n（2）大的变异：结构变异。\n这里的大小是人为界定的，界定范围一般是50bp。（patterns and mechanisms of structural variation in human cancer）相比较结构变异，拷贝数变异的检测相对容易。\n Yi and Ju 2018\n 传统的细胞生成技术把结构变异分成了简单的四类：大的删除、扩增、易位和倒位。全基因组测序分析表明，很多结构变异不是独立的事件造成的，而是通过“单次命中（single-hit）”获得的，因此是复杂的基因组重排。下面介绍一些肿瘤中发现的复杂的重排模式。\n 染色体碎裂（chromothripsis） 1. 概念介绍 染色体碎裂于2011年首次被定义，命名含义：染色体被切成碎片（2011）。它是受到大量的（可以超过100个）结构变异断点影响的复杂的重排模式，主要集中在一个或几个染色体臂上(Korbel,J.O. \u0026amp; Campbell, P.J.2013)。通常情况下，染色体碎裂在约3%的肿瘤中出现，尤其是骨癌（25%）和脑癌（10%）中较为常见。（2011）但是对于其患病率和癌症类型特异性仍比较模糊。染色体碎裂涉及到的染色体臂的拷贝数变异只在两种状态间震荡变化(1个拷贝或2个拷贝，偶尔出现3个拷贝的情况)，在删除和正常的拷贝数状态之间变化，另外，杂合性缺失（LOH）在DNA拷贝数区域出现频繁。\n2. 染色体碎裂模式 最简单的解释染色体碎裂发生的模式是：在肿瘤细胞中的单次灾难性打击，同时将一个或几个染色体臂粉碎成数百个DNA片段，随后DNA修复通路（可能是NHEJ，这个后续会提到）将片段重新组装，但是是以错误的排序和方向进行的组装，在修复过程中没有参与重新组合的片段发生了丢失从而发生了删除事件，最终结果形成了染色体碎裂。\n上述解释了染色体碎裂在基因组上的特征，但是对于灾难性打击事件并不了解。目前有两种互斥的机制：\n（1）端粒危机:端粒短化，末端和末端的染色体融合，形成染色质桥（41）\n端粒（telomere）是染色体末端保护染色体的DNA区域，当端粒缩短，染色体末端（chromatid）可能会融合，形成具有双着丝粒（dicentric）的染色体，从而在有丝分裂中不能够分开进行子细胞，融合位点在有丝分裂后期伸展，形成染色质桥。在某些情况下，该桥诱导核膜在后期部分破裂，3 \u0026lsquo;修复外切酶1 (TREX1)的核酸酶活性产生大量的单链DNA和桥断裂[42]。在子细胞中观察其结构变异情况，容易发现其存在已知染色体碎裂存在的特征，以及局部超点突变(kataegis)。这个机制可以解释为什么染色体碎裂容易发生在端粒附近。\n（2）有丝分裂时染色体错分离，形成微核（18）\n在染色体上异常的核结构（微核）造成的物理隔离可能是染色体碎裂产生的机制。微核通常由于细胞分裂错误造成，比如在有丝分裂期间完整染色体的错误分离(43)以及异常DNA复制或修复过程产生的无着丝粒的基因组片段。微核中的分子过程容易出错，被分离出来的遗传物质被大量的分解成碎片，并重新组合，重新连接的DNA片段，显示出染色体碎裂的特征，并且被固定在子细胞中。\n Yi and Ju 2018\n 3. 染色体碎裂的定量/界定  chromoplexy 1. 概念介绍 chromoplexy是扩展了的平衡易位（balanced trandslocation），重新洗牌了很多染色体（超过两个染色体），结构变异在chromoplexy事件中通常涉及到超过3个染色体，并且是一个闭环“closed chain”的重排模式，尽管有小的删除会偶然的结合在断点周围形成“删除桥”，在chromoplexy时间中大部分的结构变异是拷贝数中性的。chromoplexy有很多相互依赖的结构变异断点（大部分是染色体易位），但是数量比染色体碎裂要少，这种现象在前列腺癌中发现（46），其发生的比例约90%，并且未在其他癌症类型中发现（截止文章发表的2018年）。chromoplexy机制通常是会破坏肿瘤抑制基因（比如：PTEN、TP53和CHEK2等等），并通过融合基因的形成来激活致癌基因（oncogene）（比如：TMPRSS2-ERG），\n2. chromoplexy和染色体碎裂的异同点   同：都是由于一次灾难性打击后（不是相同的灾难性打击），产生了很多DNA双链断点。\n  同：染色体碎裂和chromoplexy都是：破碎+缝合，因此几乎不涉及到扩增，尽管chromoplexy有小的删除，但绝大部分chromoplexy是拷贝数中性的。\n  异：染色体碎裂只限于染色体臂，chromoplexy则分布在染色体上，不过双链断点的分布并不是随机的，而是富集在转录活跃的和开放的核染色质区域（48-50）。这表明了核转录中心，这个在空间上聚集了很多共同调控的基因组区域的部分，由于一次灾难性打击被片段化了。但是这个灾难性打击并不清楚是什么。\n   Yi and Ju 2018\n  微同源介导的双链断裂修复（microhomology-mediated break-induced replication, MMBIR） 1. 概念介绍 一个父位点导致的大量散布的拷贝数扩增，这些扩增子直接和常见的微同源（2-15bp）以及模版插入在断点连接处相互连接，该模式最开始是用来解释种系拷贝数变异的，研究假设：跨损伤DNA聚合酶比如Rev1等等参与产生了MMBIR（53）。\n细胞条件下诱导MMBIR产生的机制仍不明确。研究假设：由于大量的DNA加合物或/和单链DNA断裂干涉了正常DNA的复制，并刺激了模板的转换（？），形成破损复制叉（replication fork），通常为了修复破损的复制叉，模板的转换使用了姐妹染色单体，但是这个过程有利有弊，该修复过程在选择非等位的（non-allelic）染色体区域作为模板时，会导致染色体重排。\n总结：扩增；复制叉\n Yi and Ju 2018\n  断裂融合桥（breakage-fusion-bridge cycle, BFB cycle） 1. 概念介绍 断裂融合桥首次在1939年被发现（57），当两个着丝粒在细胞后期被撕开时，端粒融合和断裂形成了具有双着丝粒的环，在部分细胞周期中，两个着丝粒中间随机形成了随机的大量双链断裂点，断裂融合桥环的典型特征就是重排，包括：\n  在亚端粒区域的stair-like增加（？）[41]\n  在断点处的fold-back inversion的富集\n  BFB环调控的结构变异在急性淋巴细胞白血病的亚型中展现出来，涉及到21号染色体的染色体内扩增，以及RUNX1基因的扩增。\n Yi and Ju 2018\n  同源重组修复缺陷（homologous recombination repair defect， HRD） 1. 概念介绍 同源重组（HR）是使用一致的或相似的DNA序列来修复双链断裂的基本细胞机制，同源重组的步骤通常是：\n（1）切除双链断裂的5‘【这段检查一下】\n（2）把3’端悬挂的部分匹配到一致的或类似的DNA片段上【是不是避免发生而不是推动发生】\n（3）DNA修复使用1个或2个通路——double-holliday junction[62]或synthesis-dependent strand annealing\n双链断裂修复缺陷（比如：BRCA1和BRCA2的失活）造成了基因组的不稳定【这个增加风险是如何得到该结论的？？？见下图64，65】，并且增加了乳腺癌和卵巢癌的风险。通常，在同源重组通路中，BRCA1伴随ATM，TP53和CHEK2来识别DNA双链断裂，BRCA2在加载(loading of)RAD51中起到重要作用，RAD51是5‘末端切割后链匹配必须的基因，在7%的乳腺癌患者中发现BRCA1和/或BRCA2完全失活，尤其是三阴性乳腺癌中。\n 1994\n BRCA基因突变的乳腺癌患者和其他乳腺癌患者相比存在更高的结构变异负荷，根据失活基因，发现了结构变异的特殊的模式。比如，BRCA1失活的肿瘤中主要是短的(\u0026lt;10kb)串联重复（tandem duplication），BRCA2失活的肿瘤主要是删除[68]，\n 2018\n HRD在临床上非常重要，因为其存在靶向药物（PARP抑制剂）来抑制碱基切除修复通路，通过靶向其他的基因组不稳定性作用于同源重组修复缺陷的细胞，只使得肿瘤细胞死亡。【如何进行靶向的如何把靶向HRD的肿瘤细胞呢】\n 双微体和新染色体（double-minute chromosome and neochromosome） 1. 概念介绍 双微体是缺少着丝粒的小环形的异常基因组片段，双微体通常在血液和实体瘤肿瘤细胞中大量扩增，在40%的恶性胶质瘤中存在，部分致癌基因比如CDK4,MDM2,EGFR等在双微体中一起扩增。双微体在肿瘤发生和肿瘤克隆性演化中[78,79]非常重要。\n新染色体（neochromosome）是异常的环形或线形的基因组片段。和双微体不同，新染色体有着丝粒结构和端粒区域（如果是线形的），新染色体在3%的癌症中出现，尤其是间叶性肿瘤【什么是间叶性肿瘤】[80]，脂肪肉瘤的基因组可以阐述新染色体的形成。[81]。和双微体一样，新染色体初始是环状DNA结构，中间的结构随后抓住着丝粒，最终通过双末端获取端粒形成线形。\n 2018\n  可移动原件的转座（transposition of mobile elements） 1. 概念介绍 转座子(transposable elements,TEs)是人类基因组上占据45%的重复的DNA序列[82]，这些元件通过“剪切-粘贴”（DNA转座）或“拷贝-粘贴”（逆转录转座子）的方法产生结构变异，在基因组演化过程中产生重要作用，人类基因组中大部分的转座子在种系和体细胞中是缩短和不活跃的（肿瘤细胞中存在比较频繁的现象，比如L1逆转录子），\n 2018\n  外源性DNA的插入（insertion of external DNA sequence） 1. 概念介绍 除了对核基因组进行重新洗牌的方法，肿瘤细胞可以从病毒、线粒体和细菌中获取崭新的核外DNA序列，比如超过95%的宫颈癌和12%的头颈癌患者的基因组中含有HPV的DNA序列，HPV基因组整合直接涉及到肿瘤发生（例：通过HPV的致癌基因E6来抑制P53通路）以及导致基因组不稳定性的发生。HPV插入的区域会通过“环调控机制”频繁扩增，插入的区域趋向于形成环结构，基因组DNA片段由于病毒插入会扩增超过50倍，导致病毒致癌基因表达上调，并且和附近的基因产物共同扩增。\n细胞内部核转移全部或部分的线粒体DNA序列在癌症基因组中也存在（在2%的肿瘤中，尤其是皮肤癌、肺癌、乳腺癌中），但是线粒体DNA如何移动和插入基因组DNA的分子机制并没有完全阐明（截止2018）。大部分的体细胞核基因组整合线粒体DNA并不是单独发生的，而是和重排事件组合发生，认为，线粒体染色体片段可能在体细胞中DNA修复过程中充当“补充材料”或线形编织破损的核DNA片段[106]。\n 2018\n  结构变异的功能作用 结构变异对肿瘤发生和克隆性演化中的功能性结果至少可以分为四个直接的机制：\n  截断基因（truncation of genes），比如基因删除或破坏\n  对整个基因扩增，通过“剂量效应”来提升表达水平\n  融合基因的形成，比如肺癌中的EML4-ALK和白血病中的BCR-ABL\n  前三种是传统的机制，第四种存在概念进展。\n 劫持增强子。比如：对肿瘤基因替换基因表达，包括IRS4、SMARCA1、TERT等等[119]，在乳腺癌中，乳腺癌组织特异性调控区域反复重复，表明正向选择压力的存在，很多非编码的结构变异可能会影响近端或远端的基因表达。   拷贝数变异具有的特征  focal和arm-level染色体变异  在整个基因组上，最常见的体细胞拷贝数变异（SCNA）是短的染色体变异（focal），和几乎和染色体臂或整条染色体等长的染色体变异（arm-level）。[3] arm-level的染色体变异发生的占比大约是focal的30倍，且几乎所有的癌症类型中都是这样。认为这两者发生概率不同，意味着是分别发生的。\n \ns  参考资料   维基百科\n  算法的鲁棒性\n  [2]context is everything:aneuploidy in cancer\n  [3]The landscape of somatic copy-number alteration across human cancers\n  ","date":"March 16, 2022","image":null,"permalink":"/post/2022-3-16_mechanism_of_cnv/","title":"拷贝数变异机制"},{"categories":["算法","机器学习"],"contents":"通过搜索发现，鲁棒性是一个应用很广泛的词汇，我们主要关注它在计算机科学的算法和统计学中代表的含义。\n维基百科上这样描述：稳健性（英语：Robustness）是指一个计算机系统在执行过程中处理错误，以及算法在遭遇输入、运算等异常时继续正常运行的能力。\n Robustness is the capacity of a method to remain unaffected by small, deliberate variations in method parameters.\n 在分析方法中，鲁棒性是在分析方法的验证研究中评估过的一个参数，它被定义为“在实验条件存在微小变化的情况下，分析方法产生无偏结果的能力”。\n总结：鲁棒性看起来并不是强调方法的普适性，而是描述方法在面对异常情况时的稳定能力，强调的是面对干扰的能力。\n描述一个方法/算法可以适用不同来源的数据，应该用什么词？\n   稳定性？\n  适应性？\n   参考资料   维基百科\n  算法的鲁棒性\n  ","date":"March 15, 2022","image":null,"permalink":"/post/2022-3-15_robustness/","title":"什么是鲁棒性(robustness)"},{"categories":["R","机器学习"],"contents":"介绍机器学习R包。\n1. 数据分割 基于输出变量的分割 在建模之前，需要对样本数据进行分割分为训练集和测试集。在之前建模过程中，我发现我使用sample进行分割存在一定的问题，比如我分割出来的数据中response的占比在训练集和测试集中差别很大等等。caret包中可以解决这个问题，通过设置p值，同时确定训练集和测试集的占比，并各个因子水平下取占比（表述的有点问题）。\ncreateDataPartition(y,  times = 1,  p = 0.5,  list = TRUE,  groups = min(5, length(y))) 2. 预处理 2.1 虚拟变量处理 数据预处理又包括：对因子型变量进行虚拟变量处理（比如response在数据中是yes或no的形式表示，那么可以在这一步转换为虚拟变量0,1，这一步骤尤其适合多个因子型变量都需要处理的情况，可以节约时间）；\ncaret包假定所有的数据都是数值型的，比如因子型的可以通过model.matrix和dummyVars转换为dummy的变量。 dummyVars：\ndummyVars(formula, data, sep = \u0026#34;.\u0026#34;,  levelsOnly = FALSE,  fullRank = FALSE, ...) predict(object, newdata, na.action = na.pass, ...) formula：y~x1+x2，公式右边需要处理为哑变量的因子型变量，不确定哪些是因子型变量的话，可以直接使用y~.来制定所有的列，自动把因子型变量的进行处理。\n2.2 近零方差变量的删除 比如有些变量存在一些很特殊的值，这些值的占比很少，假设response中yes的为99个，no的为1个，那么一些模型来说，模型会崩溃(crash)或者fit to be unstable. 当数据做交叉验证时进行分割以及bootstrap采样的时候，这些变量可能是零方差的，举个例子：\ndata(mdrr) data.frame(table(mdrrDescr$nR11))  ## Var1 Freq ## 1 0 501 ## 2 1 4 ## 3 2 23 当存在很多变量时，可以用过这个函数批量处理：\nnearZeroVar(x, freqCut = 95/5,  uniqueCut = 10,  saveMetrics = FALSE,  names = FALSE,  foreach = FALSE,  allowParallel = TRUE) nzv(x, freqCut = 95/5, uniqueCut = 10, saveMetrics = FALSE, names = FALSE) freqCut：阈值，默认值是最频繁的数值个数95/次频繁的数值个数5\n2.3 删除高相关的预测变量和完全线性关系的变量 2.4 数据标准化处理 2.5 缺失数据的处理 2.6 变量转换 3. 特征选择 4. 使用重采样的模型调整 5. 变量重要性的评估 参考资料：  教你使用caret包(一)\u0026amp;ndash;数据预处理  ","date":"February 24, 2022","image":null,"permalink":"/post/2022-2-24_caret1/","title":"R包caret基本功能【1】——六种基本功能"},{"categories":["R","机器学习"],"contents":"介绍机器学习R包mlr3，该包提供了分类、回归、生存分析以及其他的机器学习任务，包含超参的调节以及特征的选择，本地支持很多操作的并行化。\n1. mlr3 Quickstart install.packages(\u0026#34;mlr3\u0026#34;) 对iris数据集的前120行训练一个决策树，并且对后30行进行最终的预测，最后判断预测模型的准确性。\nlibrary(\u0026#34;mlr3\u0026#34;) task = tsk(\u0026#34;iris\u0026#34;) learner = lrn(\u0026#34;classif.rpart\u0026#34;)  # train a model of this learner for a subset of the task learner$train(task, row_ids = 1:120) # this is what the decision tree looks like learner$model 预测\npredictions = learner$predict(task, row_ids = 121:150) predictions 准确性\n# accuracy of our model on the test set of the final 30 rows predictions$score(msr(\u0026#34;classif.acc\u0026#34;))   2. R6 mlr3提供的所有基本构建块都是R6类，因此需要了解一下R6类。R6是R用于面向对象编程(OO)的语言之一，面向对象是以功能来划分问题，而不是步骤。\n  object是通过R6::R6Class()创建的，foo = Foo$new(bar = 1)就是创建了一个新的Foo类的object，设定bar这个参数值为1，在mlr3中很多object都是通过特殊的功能来创建的。e.g. lrn(\u0026quot;regr.rpart\u0026quot;)。\n  object有沉默状态，可以通过$来调用，比如我们可以通过 foo$bar调取foo变量中的bar参数值或者重新对参数值进行设定。\n  object还公开了一些方法来检查object的状态，检索信息或执行改变对象内部状态的操作，比如$train通过建立和存储一个训练过的模型来改变学习者的内部状态，以便在给定数据的情况下使用该模型进行预测。\n  object有公开和私密的field和方法，公开的field和方法定义了和object联系的API，私密的方法只和使用者有关。（？？？）\n  拷贝object可以使用$clone()方法以及对nested object使用deep=TRUE参数，比如foo2 = foo$clone(deep = TRUE)\n  更多关于R6的介绍可以看R6介绍，特别是其中introduction部分。\n2.1 R6基础介绍 创建一个简单的R6类，public参数是项目列表，可以是函数和filed（非函数），函数可以被当作方法使用。\nlibrary(R6)  Person \u0026lt;- R6Class(\u0026#34;Person\u0026#34;,  public = list(  name = NULL,  hair = NULL,  initialize = function(name = NA, hair = NA) {  self$name \u0026lt;- name  self$hair \u0026lt;- hair  self$greet()  },  set_hair = function(val) {  self$hair \u0026lt;- val  },  greet = function() {  cat(paste0(\u0026#34;Hello, my name is \u0026#34;, self$name, \u0026#34;.\\n\u0026#34;))  }  ) ) 要实例化这个类的一个对象，使用$new():\nann \u0026lt;- Person$new(\u0026#34;Ann\u0026#34;, \u0026#34;black\u0026#34;) ann $new()方法创建了对象并且使用了initialize()方法（如果存在的话）。在class方法里，self指的是对象，对象中面向公众的成员可以通过self$x来使用，赋值通过self$x \u0026lt;- y完成。一旦对象被实例化，就可以通过$来获取值和方法。\nann$hair ann$greet() ann$set_hair(\u0026#34;red\u0026#34;) ann$hair 2.2 R6的私密成员 在上面的例子中，所有的成员都是公开的，但也可以在对象中添加私密成员：\nQueue \u0026lt;- R6Class(\u0026#34;Queue\u0026#34;,  public = list(  initialize = function(...) {  for (item in list(...)) {  self$add(item)  }  },  add = function(x) {  private$queue \u0026lt;- c(private$queue, list(x))  invisible(self)  },  remove = function() {  if (private$length() == 0) return(NULL)  # Can use private$queue for explicit access  head \u0026lt;- private$queue[[1]]  private$queue \u0026lt;- private$queue[-1]  head  }  ),  private = list(  queue = list(),  length = function() base::length(private$queue)  ) )  q \u0026lt;- Queue$new(5, 6, \u0026#34;foo\u0026#34;) 公共的成员可以通过self来获取，比如self$add()，私密的成员可以通过private来获取，比如private$queue，公共的成员使用方法如下：\n# Add and remove items q$add(\u0026#34;something\u0026#34;) q$add(\u0026#34;another thing\u0026#34;) q$remove() #\u0026gt; [1] 5 q$remove() #\u0026gt; [1] 6 q$remove() #\u0026gt; [1] \u0026#34;foo\u0026#34; q$remove() #\u0026gt; [1] \u0026#34;something\u0026#34; q$remove() #\u0026gt; [1] \u0026#34;another thing\u0026#34; 但是私密的成员没办法直接获取。\nq$queue #\u0026gt; NULL q$length() #\u0026gt; Error: attempt to apply non-function 这样的设计可以使得方法连接使用，因为可以让方法在可能的情况下返回self（不可见的）。\nq$add(10)$add(11)$add(12) 2.3 Active bindings Active bindings看起来像fields，但是每次获取时，会调用一个函数，通常是公开可见的。\nNumbers \u0026lt;- R6Class(\u0026#34;Numbers\u0026#34;,  public = list(  x = 100  ),  active = list(  x2 = function(value) {  if (missing(value)) return(self$x * 2)  else self$x \u0026lt;- value/2  },  rand = function() rnorm(1)  ) )  n \u0026lt;- Numbers$new() n$x 当以读取值的方式访问active binding时，它会调用value的函数作为缺少的参数:\nn$x2 #\u0026gt; [1] 200 当它像赋值一样被访问时，它使用赋值值作为value参数:\nn$x2 \u0026lt;- 1000 n$x #\u0026gt; [1] 500 如果函数没有参数，那么不能使用赋值符号：\nn$rand #\u0026gt; [1] 0.2648 n$rand #\u0026gt; [1] 2.171 n$rand \u0026lt;- 3 #\u0026gt; Error: unused argument (quote(3)) 2.4 继承 一个R6类可以继承另外一个R6类，也就是有超类和亚类。亚类可以有另外的方法，而且可以有覆盖超类的方法，举个例子，queue保留它的历史，添加show()方法，覆盖remove()方法：\n# Note that this isn\u0026#39;t very efficient - it\u0026#39;s just for illustrating inheritance. HistoryQueue \u0026lt;- R6Class(\u0026#34;HistoryQueue\u0026#34;,  inherit = Queue,  public = list(  show = function() {  cat(\u0026#34;Next item is at index\u0026#34;, private$head_idx + 1, \u0026#34;\\n\u0026#34;)  for (i in seq_along(private$queue)) {  cat(i, \u0026#34;: \u0026#34;, private$queue[[i]], \u0026#34;\\n\u0026#34;, sep = \u0026#34;\u0026#34;)  }  },  remove = function() {  if (private$length() - private$head_idx == 0) return(NULL)  private$head_idx \u0026lt;- private$head_idx + 1  private$queue[[private$head_idx]]  }  ),  private = list(  head_idx = 0  ) )  hq \u0026lt;- HistoryQueue$new(5, 6, \u0026#34;foo\u0026#34;) hq$show() #\u0026gt; Next item is at index 1  #\u0026gt; 1: 5 #\u0026gt; 2: 6 #\u0026gt; 3: foo hq$remove() #\u0026gt; [1] 5 hq$show() #\u0026gt; Next item is at index 2  #\u0026gt; 1: 5 #\u0026gt; 2: 6 #\u0026gt; 3: foo hq$remove() #\u0026gt; [1] 6 亚类的方法可以称之为super$xx()，CountingQueue（接下来的例子）持续计算曾经加在queue里的object的总数，它覆盖了add()方法——它增加一个计数器，然后用超$add(x)调用超类的add()方法：\nCountingQueue \u0026lt;- R6Class(\u0026#34;CountingQueue\u0026#34;,  inherit = Queue,  public = list(  add = function(x) {  private$total \u0026lt;- private$total + 1  super$add(x)  },  get_total = function() private$total  ),  private = list(  total = 0  ) )  cq \u0026lt;- CountingQueue$new(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;) cq$get_total() #\u0026gt; [1] 2 cq$add(\u0026#34;z\u0026#34;) cq$remove() #\u0026gt; [1] \u0026#34;x\u0026#34; cq$remove() #\u0026gt; [1] \u0026#34;y\u0026#34; cq$get_total() #\u0026gt; [1] 3 2.5 包括参考object的field 如果你的R6类包含任何具有引用语义的字段(例如，其他R6对象，环境)，这些字段应该在initialize方法中填充。如果该字段在类定义中直接设置为引用对象，则该对象将在R6对象的所有实例中共享。这里有一个例子:\nSimpleClass \u0026lt;- R6Class(\u0026#34;SimpleClass\u0026#34;,  public = list(x = NULL) )  SharedField \u0026lt;- R6Class(\u0026#34;SharedField\u0026#34;,  public = list(  e = SimpleClass$new()  ) )  s1 \u0026lt;- SharedField$new() s1$e$x \u0026lt;- 1  s2 \u0026lt;- SharedField$new() s2$e$x \u0026lt;- 2  # 改变 s2$e$x 也会改变 s1$e$x 的值 s1$e$x #\u0026gt; [1] 2 为了避免上面出现的同时改变的情况，请填充initialize方法中的字段:\nNonSharedField \u0026lt;- R6Class(\u0026#34;NonSharedField\u0026#34;,  public = list(  e = NULL,  initialize = function() self$e \u0026lt;- SimpleClass$new()  ) )  n1 \u0026lt;- NonSharedField$new() n1$e$x \u0026lt;- 1  n2 \u0026lt;- NonSharedField$new() n2$e$x \u0026lt;- 2  # n2$e$x 不会影响 n1$e$x 的值 n1$e$x #\u0026gt; [1] 1 2.6 在已有的类中添加成员 在已有的类中添加成员是一个很有用的操作，可以在generator对象中使用$set()方法来完成。\nSimple \u0026lt;- R6Class(\u0026#34;Simple\u0026#34;,  public = list(  x = 1,  getx = function() self$x  ) )  Simple$set(\u0026#34;public\u0026#34;, \u0026#34;getx2\u0026#34;, function() self$x*2)  # To replace an existing member, use overwrite=TRUE Simple$set(\u0026#34;public\u0026#34;, \u0026#34;x\u0026#34;, 10, overwrite = TRUE)  s \u0026lt;- Simple$new() s$x #\u0026gt; [1] 10 s$getx2() #\u0026gt; [1] 20 $set()中第一个输入表示public的成员，第二个输入表示新增加的函数名称/成员为x，第三个是增加的函数/用新的值覆盖原先的x值。为了防止修改类，可以在创建类时使用lock_class=TRUE。也可以按照如下方式锁定和解锁一个类:\n# Create a locked class Simple \u0026lt;- R6Class(\u0026#34;Simple\u0026#34;,  public = list(  x = 1,  getx = function() self$x  ),  lock_class = TRUE )  # This would result in an error # Simple$set(\u0026#34;public\u0026#34;, \u0026#34;y\u0026#34;, 2)  # Unlock the class Simple$unlock()  # Now it works Simple$set(\u0026#34;public\u0026#34;, \u0026#34;y\u0026#34;, 2)  # Lock the class again Simple$lock() 2.7 克隆对象 默认的，R6对象存在clone方法来克隆对象。\nSimple \u0026lt;- R6Class(\u0026#34;Simple\u0026#34;,  public = list(  x = 1,  getx = function() self$x  ) )  s \u0026lt;- Simple$new()  # Create a clone s1 \u0026lt;- s$clone() # Modify it s1$x \u0026lt;- 2 s1$getx() #\u0026gt; [1] 2  # Original is unaffected by changes to the clone s$getx() #\u0026gt; [1] 1 如果不希望添加克隆方法，可以在创建类时使用cloneable=FALSE。如果任何加载的R6对象有一个克隆方法，该函数将使用83,552字节，但是对于每个额外的对象，克隆方法只消耗少量的空间(112字节)。\n2.8 深层克隆 如果有任何字段是具有引用语义的对象(environments、R6对象、引用类对象)，则副本将获得对同一对象的引用。这有时是可取的，但通常不是。\n例如，我们将创建一个对象c1，其中包含另一个R6对象s，然后克隆它。因为原始的和克隆的s字段都指向同一个对象，所以从一个字段修改它会导致另一个字段的变化。\nSimple \u0026lt;- R6Class(\u0026#34;Simple\u0026#34;, public = list(x = 1))  Cloneable \u0026lt;- R6Class(\u0026#34;Cloneable\u0026#34;,  public = list(  s = NULL,  initialize = function() self$s \u0026lt;- Simple$new()  ) )  c1 \u0026lt;- Cloneable$new() c2 \u0026lt;- c1$clone()  # Change c1\u0026#39;s `s` field c1$s$x \u0026lt;- 2  # c2\u0026#39;s `s` is the same object, so it reflects the change c2$s$x #\u0026gt; [1] 2 可以发现更改c1后c2也同时发生了改变，我们可以使用deep=TRUE选项来让克隆对象接收到s的拷贝:\nc3 \u0026lt;- c1$clone(deep = TRUE)  # Change c1\u0026#39;s `s` field c1$s$x \u0026lt;- 3  # c2\u0026#39;s `s` is different c3$s$x #\u0026gt; [1] 2 此时c3并不会改变，克隆的默认行为(deep=TRUE)是复制R6对象的字段，但不复制环境、引用类对象或其他包含其他引用类型对象的数据结构(例如，带有R6对象的列表)的字段。\n如果R6对象包含这些类型的对象，并且希望对它们进行深度克隆，则必须在名为deep_clone的私有方法中提供用于深度克隆的自己的函数。下面是一个R6对象的例子，它有两个字段，a和b，都是环境，都包含一个值x。它还有一个字段v，它是一个常规(非引用)值，和一个私有的deep_clone方法。\nCloneEnv \u0026lt;- R6Class(\u0026#34;CloneEnv\u0026#34;,  public = list(  a = NULL,  b = NULL,  v = 1,  initialize = function() {  self$a \u0026lt;- new.env(parent = emptyenv())  self$b \u0026lt;- new.env(parent = emptyenv())  self$a$x \u0026lt;- 1  self$b$x \u0026lt;- 1  }  ),  private = list(  deep_clone = function(name, value) {  # With x$clone(deep=TRUE) is called, the deep_clone gets invoked once for  # each field, with the name and value.  if (name == \u0026#34;a\u0026#34;) {  # `a` is an environment, so use this quick way of copying  list2env(as.list.environment(value, all.names = TRUE),  parent = emptyenv())  } else {  # For all other fields, just return the value  value  }  }  ) )  c1 \u0026lt;- CloneEnv$new() c2 \u0026lt;- c1$clone(deep = TRUE) 当c1$clone(deep=TRUE)被调用时，c1中的每个字段都会被调用deep_clone方法，并传递字段的名称和值。在我们的版本中，a环境被复制，但b没有，v也没有(但这没关系，因为v不是一个引用对象)。我们可以测试克隆:\n# Modifying c1$a doesn\u0026#39;t affect c2$a, because they\u0026#39;re separate objects c1$a$x \u0026lt;- 2 c2$a$x #\u0026gt; [1] 1  # Modifying c1$b does affect c2$b, because they\u0026#39;re the same object c1$b$x \u0026lt;- 3 c2$b$x #\u0026gt; [1] 3  # Modifying c1$v doesn\u0026#39;t affect c2$v, because they\u0026#39;re not reference objects c1$v \u0026lt;- 4 c2$v #\u0026gt; [1] 1 在上面的deep_clone方法示例中，我们检查了每个字段的名称，以确定如何处理它，但我们也可以通过使用继承(value，“R6”)或is.environment()等来检查值。\n2.9 在环境中打印R6对象 R6对象有一个默认的打印方法，该方法列出了对象的所有成员。如果类定义了打印方法，那么它将覆盖默认方法。\nPrettyCountingQueue \u0026lt;- R6Class(\u0026#34;PrettyCountingQueue\u0026#34;,  inherit = CountingQueue,  public = list(  print = function(...) {  cat(\u0026#34;\u0026lt;PrettyCountingQueue\u0026gt; of \u0026#34;, self$get_total(), \u0026#34; elements\\n\u0026#34;, sep = \u0026#34;\u0026#34;)  }  ) ) pq \u0026lt;- PrettyCountingQueue$new(1, 2, \u0026#34;foobar\u0026#34;) pq #\u0026gt; \u0026lt;PrettyCountingQueue\u0026gt; of 3 elements 2.10 终结器 有时候，在对象被垃圾回收时运行一个函数是很有用的。例如，您可能希望确保关闭文件或数据库连接。为此，您可以定义一个私有finalize()方法，当对象被垃圾收集时，将不带参数调用该方法。\nA \u0026lt;- R6Class(\u0026#34;A\u0026#34;, private = list(  finalize = function() {  print(\u0026#34;Finalizer has been called!\u0026#34;)  } ))  # Instantiate an object: obj \u0026lt;- A$new()  # Remove the single existing reference to it, and force garbage collection # (normally garbage collection will happen automatically from time # to time) rm(obj); gc() #\u0026gt; [1] \u0026#34;Finalizer has been called!\u0026#34; #\u0026gt; used (Mb) gc trigger (Mb) max used (Mb) #\u0026gt; Ncells 678820 36.3 1349101 72.1 1349101 72.1 #\u0026gt; Vcells 1273761 9.8 8388608 64.0 3247078 24.8 2.11 类方法和成员函数 当R6类定义包含函数时，这些函数都是class方法：可以通过self来调用，当R6对象被拷贝时，最终的对象将会有一个self来指代新的对象，这是通过改变克隆对象中方法的封闭环境来实现的。\n相比较class method，我们可以在R6对象中添加常规的函数做为成员，可以通过在initialize方法中把方法分配到filed，或者在对象被实例化之后，这些函数不是class methodm，也无法通过self，private或super来接触到。\n下面是一个简单的类，它有一个方法get_self()，它只返回self，还有一个空成员fn。在这个例子中，我们将赋予fn一个函数，它具有与get_self相同的函数体。然而，由于它是一个常规函数，self将引用R6对象以外的东西:\nFunctionWrapper \u0026lt;- R6Class(\u0026#34;FunctionWrapper\u0026#34;,  public = list(  get_self = function() {  self  },  fn = NULL  ) )  a \u0026lt;- FunctionWrapper$new()  # Create a function that accesses a variable named `self`. # Note that `self` in this function\u0026#39;s scope refers to 100, not to the R6 object. self \u0026lt;- 100 a$fn \u0026lt;- function() {  self }  a$get_self() #\u0026gt; \u0026lt;FunctionWrapper\u0026gt; #\u0026gt; Public: #\u0026gt; clone: function (deep = FALSE) #\u0026gt; fn: function () #\u0026gt; get_self: function ()  a$fn() #\u0026gt; [1] 100 从R6 2.3.0开始，如果对象被克隆，成员(非方法)函数的外部环境将不会改变，这是人们通常所期望的。它会这样表现:\nb \u0026lt;- a$clone()  b$get_self() #\u0026gt; \u0026lt;FunctionWrapper\u0026gt; #\u0026gt; Public: #\u0026gt; clone: function (deep = FALSE) #\u0026gt; fn: function () #\u0026gt; get_self: function ()  b$fn() #\u0026gt; [1] 100 3. S3 参考资料：   2分钟让你明白什么是面向对象编程\n  mlr3 book\n  ","date":"February 24, 2022","image":null,"permalink":"/post/2022-2-24_mlr31/","title":"R包mlr3基本功能【1】——R6类——需要修订"},{"categories":["R"],"contents":"R包功能的实现需要各种函数，函数应该如何编写？什么样的步骤可以被包括在函数里？函数需要哪些基本的功能？函数应该如何命名？函数可以通过调用函数并且给定参数来代替对代码的重复操作（当一个操作需要重复2次时），通过自定义的函数命名来使得功能易懂，并且方便后续的更新和更改，对功能进行更改只需要变更函数，而非在所有的代码中一一修改。\n1. 基本步骤 写函数的基本步骤如下：\n1.1 函数命名   命名要尽量短，而且指示函数的功能，长一点能够清楚的展示功能也无妨。\n  函数名尽可能是动词，不过相比使用get、compute、calculate、 determine这样宽泛的动词来说，名词相对更好，函数的参数尽可能是名词。想到好的名字随时去更改掉它。\n  当函数名包含很多单词时，可以统一选取一种使用，可以使用snake_case，或者camelCase的形式，两者不要混着用。\n  同类的功能名称保持一定的一致性。\n  # Good input_select() input_checkbox() input_text()  # Not so good select_input() checkbox_input()   尽量避免和已有的函数名称重复，尤其是基本包里的。\n  习惯使用注释把文件分割，快捷键是：cmd/ctrl + shift + R\n  # Load data --------------------------------------  # Plot data -------------------------------------- 1.2 列出输入数据或参数 1.3 函数内部功能 我觉得还需要加上一个步骤就是，确定函数的输出是什么。写完函数还需要对函数进行交互式测试，参考这本书的测试章节。通过下面的简化过程比较容易理解：\n初始未简化：\ndf \u0026lt;- tibble::tibble(  a = rnorm(10),  b = rnorm(10),  c = rnorm(10),  d = rnorm(10) )  df$a \u0026lt;- (df$a - min(df$a, na.rm = TRUE)) /  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$b \u0026lt;- (df$b - min(df$b, na.rm = TRUE)) /  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$c \u0026lt;- (df$c - min(df$c, na.rm = TRUE)) /  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE)) df$d \u0026lt;- (df$d - min(df$d, na.rm = TRUE)) /  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE)) 编写函数进行简化：\nrescale01 \u0026lt;- function(x) {  rng \u0026lt;- range(x, na.rm = TRUE)  (x - rng[1]) / (rng[2] - rng[1]) }  df$a \u0026lt;- rescale01(df$a) df$b \u0026lt;- rescale01(df$b) df$c \u0026lt;- rescale01(df$c) df$d \u0026lt;- rescale01(df$d) 目前仍存在重复的步骤，可以通过向量和迭代的使用来简化，并进一步修改函数：\nx \u0026lt;- c(1:10, Inf)  rescale01 \u0026lt;- function(x) {  rng \u0026lt;- range(x, na.rm = TRUE, finite = TRUE)  (x - rng[1]) / (rng[2] - rng[1]) } rescale01(x) #\u0026gt; [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 #\u0026gt; [8] 0.7777778 0.8888889 1.0000000 Inf 1.4 返回值  函数返回值的位置  函数默认返回最后一步的状态值，但是可以在运行过程中return()来返回目标数值，比如input是空的情况下，可以立刻返回0等等，另外一种情况是：\nf \u0026lt;- function(){  if (x){  # complicated condition  }else{  # simple result  } } 当各种条件不能够满足，应该把简单的结果摆在前面提前返回值，就不需要做那么多的条件判断。\n 可管道操作的函数  最基础的两个管道函数类型是：transformation和side-effect，前者将对象传递给函数的第一个参数，并返回修改后的对象，后者传递的对象不会被转换，该函数对对象执行一个操作，后者不可见的返回第一个参数，这样即使没有输出也可以在管道中使用。举个例子：\nshow_missings \u0026lt;- function(df) {  n \u0026lt;- sum(is.na(df))  cat(\u0026#34;Missing values: \u0026#34;, n, \u0026#34;\\n\u0026#34;, sep = \u0026#34;\u0026#34;)   invisible(df) }  show_missings(mtcars) #\u0026gt; Missing values: 0  x \u0026lt;- show_missings(mtcars) #\u0026gt; Missing values: 0 class(x) #\u0026gt; [1] \u0026#34;data.frame\u0026#34; dim(x) #\u0026gt; [1] 32 11  mtcars %\u0026gt;%  show_missings() %\u0026gt;%  mutate(mpg = ifelse(mpg \u0026lt; 20, NA, mpg)) %\u0026gt;%  show_missings() #\u0026gt; Missing values: 0 #\u0026gt; Missing values: 18 2. 具体细节 2.1 条件执行   条件必须是逻辑值，如果是向量，需要给出warning，如果是NA，需要报错。\n  可以使用||和\u0026amp;\u0026amp;组合多个逻辑表达式，不要在if语句中使用|或\u0026amp;，这些是适用于多个值的向量化操作，如果确实存在逻辑向量的话，可以使用any或者all来将其折叠为单个值。==也是向量化的，容易得到多个输出，要么检查长度为1，使用all或any折叠，要么使用非向量化的identical，identical非常严格，总是返回单个的TRUE或这单个的FALSE，并且不强制类型，因此在比较整数和双精度浮点数时要小心。\n  identical(0L, 0) #\u0026gt; [1] FALSE  x \u0026lt;- sqrt(2) ^ 2 x #\u0026gt; [1] 2 x == 2 #\u0026gt; [1] FALSE x - 2 #\u0026gt; [1] 4.440892e-16 上述可以使用dplyr::near()进行比较，x == NA没有任何用。\n 多个条件时，当条件很多不停的写if的时候，最好重新写，可以使用switch()，能够根据位置或者名称来计算所选的代码（？？？），以及cut()函数来消除if语句的长度，可以用于离散的变量。  2.2 代码风格 多行输写除非很短。\n2.3 函数参数   参数分为两个部分，一个是支持什么样的数据，另外一个是计算的细节。后者通常会有最常使用的默认值。\n  参数的命名\n  参数的命名也很重要，命名较长且应该尽量详细，常用命名如下：\n  x,y,z：向量\n  w：向量的权重\n  df：数据框\n  i,j：行或者列的数字指标\n  n：行的长度或行的个数\n  p：列的个数\n  2.4 检查数值 当写了很多函数的时候，很容易因为忘记函数的使用方法而用无效的输入来调用函数。需要使用提示语句来对输入进行约束。只在关键的几个地方写就可以了，没必要检查每一个，好的举例：\nwt_mean \u0026lt;- function(x, w) {  if (length(x) != length(w)) {  stop(\u0026#34;`x` and `w` must be the same length\u0026#34;, call. = FALSE)  }  sum(w * x) / sum(w) } 另外可以使用内置的stopifnot()来检查每个参数是否为TRUE，如果不是那么生成一个错误的消息。\nwt_mean \u0026lt;- function(x, w, na.rm = FALSE) {  stopifnot(is.logical(na.rm), length(na.rm) == 1)  stopifnot(length(x) == length(w))   if (na.rm) {  miss \u0026lt;- is.na(x) | is.na(w)  x \u0026lt;- x[!miss]  w \u0026lt;- w[!miss]  }  sum(w * x) / sum(w) } wt_mean(1:6, 6:1, na.rm = \u0026#34;foo\u0026#34;) #\u0026gt; Error in wt_mean(1:6, 6:1, na.rm = \u0026#34;foo\u0026#34;): is.logical(na.rm) is not TRUE 2.5 \u0026hellip; R里面的很多函数不限制输入的参数个数，\u0026hellip;可以捕捉任何就算没有匹配的参数，并且把这些参数传输到另外的函数中使用，尤其适用一个函数包装另外一个函数的情况，缺点是不会对拼写错误的参数报错。\ncommas \u0026lt;- function(...) stringr::str_c(..., collapse = \u0026#34;, \u0026#34;) commas(letters[1:10]) #\u0026gt; [1] \u0026#34;a, b, c, d, e, f, g, h, i, j\u0026#34; 想要捕捉\u0026hellip;的数值，可以使用list(\u0026hellip;)\n2.6 lazy evaluation R中的参数是延迟计算的：直到被需要才会计算，如果使用不到那么不会调用，这是R编程的一个重要属性。\n参考资料：  r4ds  ","date":"February 23, 2022","image":null,"permalink":"/post/2022-2-23_function/","title":"R包开发【2】——编写函数"},{"categories":["R"],"contents":"R包开发的各种细节。\n1. 命名   尽量避免同时使用大小写字母\n  使用该包测试命名是否能用（？？？）。\n  library(available)  available(\u0026#34;doofus\u0026#34;) 2. 依赖包 如果只是使用其他的包中的少量函数时，建议在DESCRIPTION文件中的Imports:中标注包的名称，并且在使用函数过程中使用pkg::fun()。如果需要重复使用函数，可以避免繁琐的::可以@importFrom pkg fun，这样可以更快速，因为::增加了5us去使用函数，管道符号也可以使用同样的方法使用@importFrom magrittr %\u0026gt;%。如果重复使用很多其他包中的函数，可以把这个包中所有的函数全部导入@import pkg，不过这样会使得包的代码比较不容易阅读。\nImports部分并不是把function导入到namespace中，它只确保包的安装。无论是否附加包，您都需要以完全相同的方式导入函数。\n在DESCRIPTION中对依赖包进行注释：\nImports:  randomForest,  stats 在函数中使用依赖包中的函数时，需要标注pkg::fun。\n 是在roxygen注释中添加@import pkg。这种方法会在R包的NAMESPACE文件中添加import域，在加载R包时，第三方包的所有函数都会被导入。在使用函数时必须用pkg::fun的方法，以防函数名冲突。第二种方法，是在roxygen注释中添加@importFrom pkg fun。这种方法会在NAMESPACE文件中添加importFrom域，在使用时可以直接使用函数名，就像在.GlobalEnv中创建的函数一样。这里推荐第二种引用方法，因为可以大幅减少函数冲突。\n check之后存在warning：\n\u0026gt; checking dependencies in R code ... WARNING  \u0026#39;::\u0026#39; or \u0026#39;:::\u0026#39; imports not declared from:  ‘cli’ ‘dplyr’ ‘furrr’ ‘future’ ‘purrr’ ‘tibble’ 它说的是:\u0026rsquo;::\u0026lsquo;或\u0026rsquo;:::\u0026rsquo; import没有从‘cli’ ‘dplyr’ ‘furrr’ ‘future’ ‘purrr’ ‘tibble’中声明。这里重要的词是“import”。就像我们在包中导出函数一样，我们需要明确何时在另一个包中使用函数。为此，我们可以使用usethis::use_package()。\nusethis::use_package(\u0026#34;cli\u0026#34;) no visible global function definition for ‘:=’\nmagrittr Consider adding importFrom(\u0026ldquo;stats\u0026rdquo;, \u0026ldquo;end\u0026rdquo;, \u0026ldquo;setNames\u0026rdquo;, \u0026ldquo;start\u0026rdquo;) to your NAMESPACE file.\n为了export对象，需要把@export放在roxygen block中，比如：\n#\u0026#39; @export foo \u0026lt;- function(x, y, z) {  ... } 这将根据对象的类型生成export()、exportMethods()、exportClass()或S3method()。export函数供其他人使用，export的函数必须文档化。\n现在有一种更简单的方法来支持包中的管道。奇妙的包usethis具有use_pipe()功能。你运行该功能一次，它处理一切。这就是use_pipe()文档中描述的usethis函数：\n是否需要在包内部使用magrittr的管道并为包的用户重新导出它：\n将magrittr添加到DESCRIPTION中的“Imports”\n使用必要的roxygen模板创建R / utils-pipe.R\nuse_pipe() 2.1 包中具体函数的依赖 2.2 一个包中所有函数的依赖 3. 如何在R包中内置数据 这部分内容主要参考r-pkg-data。\n目前有三种主要的方法来放置数据：\n  储存二进制数据方便用户使用：将数据存储在data/目录下，这是存储示例数据最好的地方。\n  存储解析数据但不提供给用户使用：将数据存储在R/sysdata.rda中，这是存储函数所需数据最好的地方。【什么时解析数据】\n  存储原始数据：将数据存储在inst/extdata中。\n  这三个选项的一个简单替代方案是将它包含在包的源代码中，可以手工创建，也可以使用dput()将现有的数据集序列化到R代码中。\n3.1 导出数据 最常用的存储数据的目录是data/，该目录下的数据都应该是通过save得到的格式为.RData的数据（可以使用其他的格式，但是RData的格式是最快，最省空间，最直接的【为什么，可以单独写一下RData的存储】），对于较大的数据集，可以对数据进行压缩，默认值是bzip2，但有时gzip或xz可以创建更小的文件。数据可以使用use_data来创建：\nx \u0026lt;- sample(1000) usethis::use_data(x, mtcars) 如果DESCRIPTION中LazyData的设置是true，当载入包时会lazily loaded这种数据，除非我们对其进行使用，否则不会占用空间：\npryr::mem_used() #\u0026gt; 50.2 MB library(nycflights13) pryr::mem_used() #\u0026gt; 58 MB  invisible(flights) pryr::mem_used() #\u0026gt; 98.7 MB 通常在data/中的数据是开发者从其他地方收集的原始数据的清洗版本。建议在包的源版本中包含处理数据的代码，这样更容易更新或复制数据的版本，处理数据的代码放入data-raw/中，不需要在包的捆绑版本中使用它，所以也要将它添加到.rbuildignore中。用一个步骤做到这一切:\nusethis::use_data_raw() 3.2 文档数据 文档数据类似于对函数进行文档化【？】，把数据存储在R中，举例，roxygen2封装ggplot2中的diamond数据，存储为R/data.R：\n#\u0026#39; Prices of 50,000 round cut diamonds. #\u0026#39; #\u0026#39; A dataset containing the prices and other attributes of almost 54,000 #\u0026#39; diamonds. #\u0026#39; #\u0026#39; @format A data frame with 53940 rows and 10 variables: #\u0026#39; \\describe{ #\u0026#39; \\item{price}{price, in US dollars} #\u0026#39; \\item{carat}{weight of the diamond, in carats} #\u0026#39; ... #\u0026#39; } #\u0026#39; @source \\url{http://www.diamondse.info/} \u0026#34;diamonds\u0026#34; 在编写数据文档中有两个另外的标签非常重要：\n  @format给出了数据集的全貌，对于数据框，应该给出每个变量的描述。\n  @source提供了数据的来源细节，通常是一个\\url{}\n  3.3 内部数据 有些函数需要预先处理过的数据，如果把这些数据放在data/中，用户仍然可以使用，这点不太合适。可以将这些数据存储在R/sysdata.rda，可以使用usethis::use_data()，使用参数internal = TRUE来创建这些文件：\nx \u0026lt;- sample(1000) usethis::use_data(x, mtcars, internal = TRUE) R/sysdata.rda的对象不会输出(export)，因此不需要文档化，只有在包中才可以获取。\n如何载入/使用内部数据？\n3.4 原始数据 想要展示载入或解析原始数据的例子时，把原始文件放在inst/extdata中，当安装包时，所有在inst/目录下的文件以及文件夹都被提升到上一级目录中，因此不能够命名为R/或DESCRIPTION，要引用inst/extdata中的文件(无论是否安装)，请使用system.file()。举个例子：readr包使用inst/extdata来存储带分隔符的文件，如下所示:\nsystem.file(\u0026#34;extdata\u0026#34;, \u0026#34;mtcars.csv\u0026#34;, package = \u0026#34;readr\u0026#34;) #\u0026gt; [1] \u0026#34;/home/runner/work/_temp/Library/readr/extdata/mtcars.csv\u0026#34; 注意:默认情况下，如果文件不存在，system.file()不会返回错误——它只返回空字符串:\nsystem.file(\u0026#34;extdata\u0026#34;, \u0026#34;iris.csv\u0026#34;, package = \u0026#34;readr\u0026#34;) #\u0026gt; [1] \u0026#34;\u0026#34; 如果你想在文件不存在的时候有一个错误消息，添加参数mustWork = TRUE:\nsystem.file(\u0026#34;extdata\u0026#34;, \u0026#34;iris.csv\u0026#34;, package = \u0026#34;readr\u0026#34;, mustWork = TRUE) #\u0026gt; Error in system.file(\u0026#34;extdata\u0026#34;, \u0026#34;iris.csv\u0026#34;, package = \u0026#34;readr\u0026#34;, mustWork = TRUE): no file found 3.5 其他的数据   用来测试的数据：可以将小的数据直接放在test目录下，但是请记住，单元测试是为了测试正确性，而不是性能，所以请使用小的数据。\n  数据片段：如果想展示如何处理已载入的数据集，请将该数据放在data/中。如果您想演示如何加载原始数据，请将该数据放在inst/extdata中。\n  3.6 CRAN 笔记 通常，包中数据应该小于1Mb——如果大于1Mb，则需要争取豁免。如果数据在自己的包中，并且不会频繁更新，那么这通常更容易做到。你还应该确保数据已被最佳压缩:\n  使用tools::checkRdaFiles()来确定每个文件最佳的压缩\n  重新使用usethis::use_data()，并设置compress到合适的大小，如果丢失了重新创建文件的代码，可以使用tools::resaveRdaFiles()来重新保存。\n  4. 如何在函数中调用Python或MATLAB？ 5. 如何制作R包的图标（LOGO） 使用这个包能够生成R包六边形LOGO，函数sticker的输出是一个ggplot的对象，filename参数是自定义输出的图标图片名称，使用plot(object)可以直接查看图标。\ninstall.packages(\u0026#34;hexSticker\u0026#34;) library(hexSticker) 实例：\nlibrary(ggplot2)  p \u0026lt;- ggplot(aes(x = mpg, y = wt), data = mtcars) + geom_point() p \u0026lt;- p + theme_void() + theme_transparent()  sticker(p, package=\u0026#34;hexSticker\u0026#34;, p_size=20, s_x=1, s_y=.75, s_width=1.3, s_height=1,  filename=\u0026#34;inst/figures/ggplot2.png\u0026#34;) 参数：\n  p可以是自己生成的图片，也可以是ggplot绘制图的对象\n  p_size：字体大小\n  s_x、s_y：图片在图中的位置\n  6. 如何给包写说明书 参考资料：   r4ds\n  R：在自编写的包中使用magrittr管道运算符\n  ","date":"February 23, 2022","image":null,"permalink":"/post/2022-2-23_rpkg2/","title":"R包开发【3】——R包开发细节"},{"categories":["R"],"contents":"任何可以被自动化的，都应该让它自动化，给自己省时间，也可以方便任何人使用。devtools的目的是使得开发工具变得容易，囊括了多个包来支持开发的各个功能。\n前言：R包开发书籍的基本内容   第二章 示例包\n  第三章 为包开发准备系统\n  第四章 包的基本结构，不同的状态下结构不同（？）\n  第五章 回顾核心的工作流程，介绍核心工具之间的联系：devtools和usethis，Rstudio\n  其余章节 开发细节\n  通过示例入门 1. 在制定路径下新建R包，创建了新的项目 先安装一些包\ninstall.packages(c(\u0026#34;devtools\u0026#34;, \u0026#34;roxygen2\u0026#34;, \u0026#34;testthat\u0026#34;, \u0026#34;knitr\u0026#34;)) library(usethis) create_package(\u0026#34;/Volumes/home /project/Immarker\u0026#34;) 目录下有这些文件\n    .Rbuildignore\n  .Rproj.user\n  DESCRIPTION 文档说明\n  NAMESPACE 列出了外部和内部使用到的函数\n  The R/ directory\n  Immarker.Rproj 不使用Rstudio的话这个可以在创建时，使用参数rstudio = FALSE去除\n  如果使用者经常创建包，那么可以通过usethis.description来设定一些全局变量使得每次新建包时，DESCRIPTION的值是默认的（比如姓名、邮箱以及许可证等等），详细说明见usethis设置，DESCRIPTION的文件格式是DCF(Debian control format)，每行由一个字段名和一个值组成，用冒号分隔。当值跨多行时，需要缩进。\n2. 目前创建的目录是一个R包以及RStudio的项目，现在我们再将其增添为Git仓库。 library(devtools) use_git() 3. 写第一个函数 根据自己的需求写函数，这本书不涉及到函数应该如何写，但是可以参考Functions chapter of R for Data Science and the Functions chapter of Advanced R.\n举个例子：\nstrsplit1 \u0026lt;- function(x, split) {  strsplit(x, split = split)[[1]] } 4. 定义R函数 把R函数保存在R文件中，存放在R目录下。\n\u0026gt; use_r(\u0026#34;strsplit1\u0026#34;) ✓ Setting active project to \u0026#39;/Volumes/home /project/Immarker\u0026#39; • Modify \u0026#39;R/strsplit1.R\u0026#39; • Call `use_test()` to create a matching test file 只在这个文件中写入函数的function，不能包含任何library(devtools)，x，use_git()等其他，R包依赖的函数会在后面详细说明。\n5. 测试函数功能 先使用load_all让函数能够被测试，load_all能够模拟构建、安装和载入我们创建的R包的过程，而且load_all比实际的这个过程迭代速度更快。\nload_all() 测试函数功能\n(x \u0026lt;- \u0026#34;alfa,bravo,charlie,delta\u0026#34;) #\u0026gt; [1] \u0026#34;alfa,bravo,charlie,delta\u0026#34; strsplit1(x, split = \u0026#34;,\u0026#34;) #\u0026gt; [1] \u0026#34;alfa\u0026#34; \u0026#34;bravo\u0026#34; \u0026#34;charlie\u0026#34; \u0026#34;delta\u0026#34; 该函数能够运行，不过它并不在全局的环境中存在。\nexists(\u0026#34;strsplit1\u0026#34;, where = globalenv(), inherits = FALSE) #\u0026gt; [1] FALSE 6. commit该函数 如果使用Git，可以使用自己熟悉的方法把新的R函数commit上去。记得规范化commit的message。 Angular 提交信息规范\n7. 检查 检查所建R包的是否能够正常运行。check的时间一般比较久，但是check是一个好习惯。\ncheck() 处理check输出的结果。\n0 errors ✔ | 1 warning ✖ | 0 notes ✔ 8. 编辑对包的描述DESCRIPTION文件 修改包的作者为自己的名字或ORCID号，\n第八章会详细描述。文件内容看上去类似下面的形式：\nPackage: regexcite Title: Make Regular Expressions More Exciting Version: 0.0.0.9000 Authors@R:  person(\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, , \u0026#34;jane@example.com\u0026#34;, role = c(\u0026#34;aut\u0026#34;, \u0026#34;cre\u0026#34;)) Description: Convenience functions to make some common tasks with string  manipulation and regular expressions a bit easier. License: `use_mit_license()`, `use_gpl3_license()` or friends to pick a  license Encoding: UTF-8 Roxygen: list(markdown = TRUE) RoxygenNote: 7.1.2 9. 设置license 任意选择一个license，默认的似乎是MIT的。\nuse_mit_license() 10. 函数使用文档 在原文件中开发者对函数进行特殊格式的注释，并使用roxygen2包处理函数注释文档man/strsplit1.Rd的建立。关于这个包的具体使用在第十章有描述。使用RStudio可以直接打开函数R文件，每一行的开头为#'，不使用RStudio时，可以自己写comment。\n把光标放在函数内部(必须是括号内部！这样才能确定注释的是哪个函数)，使用快捷键ctrl+alt+shift+R，会在函数的开头自动生成注释文档的格式如下：\n#\u0026#39; Title #\u0026#39; #\u0026#39; @param x #\u0026#39; @param split #\u0026#39; #\u0026#39; @return #\u0026#39; @export #\u0026#39; #\u0026#39; @examples strsplit1 \u0026lt;- function(x, split) {  strsplit(x, split = split)[[1]] } 随后使用document将注释转化为函数的Rd文档，并保存在man文件夹下：\ndocument() 此时可以使用?strsplit1来预览函数的帮助文档。在正式构建和安装包之前，不会正确连接包的文档。这消除了帮助文件之间的链接和包索引的创建等细节。\n另外，document还基于@export更新了NAMESPACE文件，这个文件是只读模式不能够进行更改的，NAMESPACE的文档内容如下：\n# Generated by roxygen2: do not edit by hand  export(strsplit1) export意味着使用者可以通过载入该包来使用这个函数。\n11. 再次check() 12. 安装该包 install() 安装完之后就可以和其他包一样的使用了，可以restart R然后重新载入该包。\n参考资料  rpkg  ","date":"February 22, 2022","image":null,"permalink":"/post/2022-2-22_rpkg/","title":"R包开发【1】——基本流程"},{"categories":["R"],"contents":"基本内容   Excel文件的读入\n  txt和csv文件的读入\n  文件的写出\n  批量读入数据并合并\n  RData和rds的储存形式\n  read.csv和read.csv2的区别\n  Excel文件的读入 在R语言实战实战这本书中表示，读取Excel文件最好的方式，是在Excel中将其导出为一个逗号分隔文件（csv），并使用读取csv的方法读取数据。或者采用xlsx包直接读取数据。xlsx包可以用来对Exceln97/2000/XP/2003/2007文件进行读取、写入和格式转换。还可以使用readxl包来读取Excel的.xls和.xlsx文件。\nread_xlsx(path, sheet = NULL, range = NULL, col_names = TRUE,  col_types = NULL, na = \u0026#34;\u0026#34;, trim_ws = TRUE, skip = 0,  n_max = Inf, guess_max = min(1000, n_max),  progress = readxl_progress(), .name_repair = \u0026#34;unique\u0026#34;) read.xlsx(  xlsxFile,  sheet = 1,  startRow = 1,  colNames = TRUE,  rowNames = FALSE,  detectDates = FALSE,  skipEmptyRows = TRUE,  skipEmptyCols = TRUE,  rows = NULL,  cols = NULL,  check.names = FALSE,  sep.names = \u0026#34;.\u0026#34;,  namedRegion = NULL,  na.strings = \u0026#34;NA\u0026#34;,  fillMergedCells = FALSE ) 参数：\n sheet  Excel表格含有多个表格，读取其中一个表格\n startRow  Excel表格选取部分行进行读取\n  Excel表格\n  .name.repair：是对列名进行更改，默认情况下，确保的列名是非空且特殊的即\u0026quot;unique\u0026quot;，该参数可以输入toupper——将列名变成大写字母，该参数可以输入universal——将列名中空格部分用\u0026rsquo;.\u0026lsquo;代替，也可以指定为自定义函数或其他函数，对列名进行更改。\n  示例： death.xlsx通过Excel打开如下：\n death.xlsx文件\n death.xlsx有两个子表格，且真正需要的部分是A5:F15的部分，在不想修改原始文件的情况下，读入时使用.来代替空格，可通过以下代码进行读取：\n read_excel(  readxl_example(\u0026#34;deaths.xlsx\u0026#34;),  range = \u0026#34;arts!A5:F15\u0026#34;,  .name_repair = \u0026#34;universal\u0026#34;  )   New names: * `Has kids` -\u0026gt; Has.kids * `Date of birth` -\u0026gt; Date.of.birth * `Date of death` -\u0026gt; Date.of.death 写出 library(xlsx) write.xlsx2(dt, file = \u0026#34;~/merge.xlsx\u0026#34; ) 参考资料：  R语言实战（第2版）  ","date":"December 23, 2021","image":null,"permalink":"/post/2021-12-23_basic_r/","title":"R基础——读入和写出数据"},{"categories":["生信"],"contents":"背景介绍 1. 获取绝对拷贝数数值的难点 （1）采样过程中癌细胞混合了未知比例的正常细胞——肿瘤纯度；\n（2）由于染色体数量和结构异常导致的癌细胞的实际DNA含量(倍性)是未知的；\n（3）由于正在进行的亚克隆进化，癌细胞群可能是异质性的。\n理论上，如果知道每个肿瘤细胞中DNA的含量，则可通过测得的相对拷贝数获得绝对拷贝数，或者通过单细胞测序技术解决。\n2. 芯片原始原始数据格式.CEL 对于芯片数据来说，Affymetrix SNP芯片的原始文件为CEL文件，一个CEL文件即是一个个体的全部SNP分型结果。Affymetrix基因芯片是一种生物芯片，它包含一个对一个实验有效的微阵列。为了制造这些芯片，玻璃或硅载玻片上排列有探针，根据它们是否与原始DNA样本互补，探针将表达水平(强度)报告为完全匹配(PM)和不匹配(MM)值。.CEL文件格式有多个版本，使用不同的格式。例如，版本3使用ASCII文本格式，而版本4使用二进制格式。 注意：.CEL 文件需要相应的.CDF 文件，它是存储在.CEL文件中的原始探测级数据的字典。MATLAB有一个名为affyread的内置函数，可以用来读取Windows版本软件中的.CEL 文件。\n3. SNP6.0拷贝数变异检测流程 这里介绍Affymetrix SNP6 Copy Number Inference Pipeline。\n输入：CEL文件\n输出：每个样本的片段化的拷贝数结果：genotype calls；相对的拷贝数数值且经过标准化处理，使得每个样本接近双倍体；拷贝数变异区域\n流程：\n  校准信号强度\n  计算基因型\n  将信号强度转换为拷贝数数值\n  计算拷贝数噪音\n  移除离群的探针减少噪音\n  通过减去一组预先定义的正常样本中的变化，进一步降低噪声\n  将拷贝数片段化\n  计算基因组中片段的数量，并与定义的阈值进行比较，以检查超分割情况\n  工具介绍 1. ABSOLUTE 可以评估癌细胞的纯度和倍性，计算出绝对的拷贝数和突变倍数。\n输入数据：\n可以是HAPSEG，也可以是segmentation文件，前者需要安装HAPSEG包，后者segmentation文件来自芯片CGH或大量平行测序实验的结果，可以包含其他信息，但必须包含的信息有：\u0026ldquo;Chromosome\u0026rdquo;,\u0026ldquo;Start\u0026rdquo;,\u0026ldquo;End\u0026rdquo;,\u0026ldquo;Num_Probes\u0026rdquo;,\u0026ldquo;Segment_Mean\u0026rdquo;，该模式下需要将copy_num_type参数设定为total。\n（1）HAPSEG输入：HAPSEG是一种解释癌症样本中双等位基因标记数据的概率方法。HAPSEG的工作原理是将基因组划分为不同拷贝数的片段，并在每个片段中建模四个不同的基因型。\n（2）segmentation文件：测序的结果。\n输出数据：\n步骤：\n 使用ABSOLUTE分析肿瘤DNA\n ABSOLUTE是如何进行分割片段的？\n2. ACEseq 3. Battenberg 4. CloneHD 5. JaBbA 6. Sclust 直接通过新建R Markdown文件，选择posterdown模板则自动导入模板代码，如下：\n 创建模板\n 参考资料：   CEL是什么格式的文件\n  Dentro, Stefan C., et al. \u0026ldquo;Characterizing genetic intra-tumor heterogeneity across 2,658 human cancer genomes.\u0026rdquo; Cell 184.8 (2021): 2239-2254.\n  介绍了PCAWG中拷贝数数据的来源\n","date":"December 19, 2021","image":null,"permalink":"/post/2021-12-19_cnv_tools2/","title":"检测拷贝数变异的工具及算法【2】-ABSOLUTE"},{"categories":["生信"],"contents":"引言 PCAWG提供的拷贝数变异文件是综合6种不同的拷贝数变异提取工具的结果：\n  ABSOLUTE\n  ACEseq\n  Battenberg\n  CloneHD\n  JaBbA\n  Sclust\n  由于拷贝数结果的不同取决于segmentation的不同，而对大部分基因组的拷贝数状态的分歧来自于是否发生了整个基因组复制的分歧。因此针对6种方法中的5种首先构建了完整的断点数据，针对一致的断点数据使用6种方法得到拷贝数变异结果，解决了倍性结果的不统一后，对6种方法得到的每个segment寻求major allele和minor allele的状态的一致性，最后对每个肿瘤综合6种方法得到纯度结果，对每种方法都给予置信区间和质量星号：克隆性通过（3星），多数投票同意，协议后四舍五入亚克隆拷贝数（2星），调用最好的方法（1星）。3星代表结果非常一致，1星则是结果不那么一致的情况下选一种方法的结果输出。这样来得到最终的完整的拷贝数图谱，包含以下所有列：\nmajor_cn minor_cn position sampleID star total_cn value 工具介绍 1. ABSOLUTE 使用ABSOLUTE算法计算每个样本的纯度、倍性以及绝对DNA拷贝数，在基因组上收集基于片段的覆盖度(来自完整的阅读模板跨度)在基因组上收集，并校正GC含量和匹配偏差。【？】使用PCAWG的正常样本来进行切线归一化(tangent-normalization)处理。基于杂合性位点计算位点特异的拷贝数，使用CBS算法来得到segmentation。采用Nelder-Mead算法搜索可能的纯度和倍性解的空间，并对它们进行排序。对亚克隆拷贝数片段进行Dirichlet过程聚类，以标注相同的亚克隆拷贝数聚类状态。\n2. ACEseq 使用ACEseq计算绝对拷贝数，肿瘤纯度，并估计肿瘤细胞内容，通过结合肿瘤和基因组窗中匹配的正常基因的覆盖率以及相应SNPs的b等位基因频率(BAF)来确定绝对拷贝数。基因组使用PSBCBS包得到segmentation，在分割之前，结构变异断点通过一致的结构变异数据判断，\n利用PSCBS包将基因组分割为平等覆盖和不平衡状态的区域，在分割之前，将共识结构变异集定义的结构变异断点合并成片段边界，片段提交到共识断点估计集，通过共识断点得到的片段使用覆盖度和BAF值注释来估计样本的肿瘤细胞内容和倍性。\n注：配对的双亲特定CBS(配对的PSCBS)算法利用了CBS方法用于将总CN数据分割为来自SNP阵列的2D非阶段数据。该算法依赖于配对测试(肿瘤)和参考(正常)样本杂交到单独的阵列。\n3. Battenberg 使用Battenberg得到绝对拷贝数。针对每个SNP计算BAF和相对logR值，使用GC含量矫正logR值，匹配的正常样本用来获得种系的杂合性SNP，使用分段常数拟合(PCF)对数据进行分段，将结构变异(sv)作为先前建立的中断点，通过对纯度和倍性组合进行网格搜索，拟合克隆拷贝数图谱。\n4. CloneHD 使用CloneHD得到绝对拷贝数。cloneHD使用了隐马尔可夫模型来描述样本的拷贝数状态。cloneHD流程的第一步使用的是filterHD算法，filterHD不寻求解释数据中的亚克隆结构，是一种用于模糊分割的通用算法，是一个通用的一维离散数据概率滤波算法，类似于卡尔曼滤波。它是一个具有泊松或二项发射和跳跃扩散传播子的连续状态空间隐马尔可夫模型。它可以用于无标度平滑、模糊数据分割和数据滤波。\n5. JaBbA JaBbA整合paire-end和read depth信号来推断基因组间隔的拷贝数以及重构junction。在PCAWG共识拷贝数分析中，使用了两轮JaBbA，JbBbA的输入数据是bam文件，junction call set，以及初步分割（可选）和纯度/倍性输入，针对初步分割的结果，进一步使用CBS算法来分割得到低维度的常数拷贝数区域。\n6. Sclust 使用Sclust进行拷贝数分割，计算肿瘤纯度，肿瘤倍性以及位点特异的拷贝数（包含克隆性的和非克隆性的）。输入数据是肿瘤样本和匹配的正常样本的read counts。read counts后续用来计算肿瘤和正常样本的GC含量，接下来Sclust使用SNP数据，计算正常样本的杂合性位点的B位点频率，随后基于read ratio在数据中找到明显的跳跃来进行初次分割。\n共识拷贝数获取步骤 6种拷贝数检测的方法都使用了两步步骤：\n  第一步是把基因组分割为具有恒定拷贝状态的区域\n  第一步是确定每个片段的克隆和亚克隆拷贝数状态\n  6种方法产生的分歧结果主要是以下两个元素：\n  基因组分割的差异\n  是否发生了全基因组重复(WGD)的不确定性\n  共识拷贝数分割结果的分歧 拷贝数识别工具将一个样本的基因组分割成多个具有稳定拷贝数的区域，为了描述这些片段，需要找到片段之间的断点，断点两侧的拷贝数状态发生了变化，一旦建立起断点，不同的工具则确定每个片段内混合的拷贝数状态，包括主等位基因拷贝的数量，次等位基因拷贝的数量，以及处于这种状态的细胞的比例。\n不同的方法得到的断点有差别，有些方法调用的断点比其他方法多一个数量级。，为了解决不同方法考虑的基因组片段不同的问题，建立了共识断点集，所有的方法后续使用共识片段来判断拷贝数状态\n确定共识拷贝数片段断点的方法 创造了共识策略支持真正的断点，潜在的代价是增加假阳性，创建了完整的断点集。来自结构变异的拷贝数断点被用来量化我们的共识策略的“真阳性”和“假阴性”率。拷贝数方法把共识片段作为输入，但允许合并有相同拷贝数的相邻的片段，但是不允许产生额外的断点从而不会产生额外的片段。因为引入虚假断点的成本要小于缺失断点的成本，缺失断点的底层拷贝数状态确实发生了改变。我们为确定共识断点而开发的算法利用了这样一种见解:相邻区段之间的区域表明了一种方法的不确定性，即描述拷贝数状态变化的断点的确切位置。\n 模板\n 参考资料：   CEL是什么格式的文件\n  Dentro, Stefan C., et al. \u0026ldquo;Characterizing genetic intra-tumor heterogeneity across 2,658 human cancer genomes.\u0026rdquo; Cell 184.8 (2021): 2239-2254.\n  ","date":"December 16, 2021","image":null,"permalink":"/post/2021-12-16_cnv_tools/","title":"检测拷贝数变异的工具及算法【1】-PCAWG consensus copy number"},{"categories":["R","可视化"],"contents":"引言 posterdown自动排版，通过调节参数满足个人制作海报的需要。\n1. 介绍 目前posterdown支持3种风格的海报模板：posterdown_html、posterdown_betterland和posterdown_betterport。以posterdown_betterport为例进行阐述。\n 目前支持的三种模板风格\n 2. 创建海报 2.1 创建模板 直接通过新建R Markdown文件，选择posterdown模板则自动导入模板代码，如下：\n 创建模板\n 2.2 添加内容 按照R Markdown语法进行代码块、图片、文字等等的添加，通过点击knit进行渲染，查看在模板基础上更改的内容。点击knit后自动生成包含html在内的如下文件：\n. ├── A\\ Better\\ Reproducible\\ Poster\\ Title.pdf ├── packages.bib ├── poster.Rmd ├── poster.html └── poster_files  ├── figure-html  │ ├── irisfigure-1.png  │ └── myprettycode-1.png  ├── header-attrs-2.11  │ └── header-attrs.js  └── paged-0.15  4 directories, 7 files  图片  可以在该目录下再建立一个Figures文件夹存放Rmd中使用到的图片。\n html文件  默认生成的html文件名前缀和Rmd文件一致，可以通过在Rmd文档开头添加代码自定义生成的html文件名，这里为index.html：\nknit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), \u0026#39;index.html\u0026#39;)) })  海报大小设置  通过横宽两个参数设定，支持cm，in，mm这几个长度单位，在文档开头加入代码：\nposter_height: \u0026#34;90cm\u0026#34; poster_width: \u0026#34;60cm\u0026#34;  字体大小  在文档开头加入代码：\nmain_textsize: \u0026#34;70pt\u0026#34; body_textsize: \u0026#34;30px\u0026#34; 更多参数设置可以参考GitHub上发布的官方指南，另外还有一份更详细的补充参数指南，可具体到该参数支持的长度单位也包含哪些。\n2.3 导出html以及pdf 有多种方法，可以归纳为两种，一种是将Rmd直接转换为PDF，一种是渲染的html文件转换为PDF，能够满足转换的工具都可以尝试使用，不局限于使用R或者其他的方法实现。\n  根据Github上的issue作者的提议，可以使用pagedown包中的函数:pagedown::chrome_print(\u0026quot;myfile.Rmd\u0026quot;)，直接将Rmd导出为PDF，但是该函数并未使用成功，有待后续解决。\n  使用render将Rmd导出为PDF\n  render(\u0026#34;input.Rmd\u0026#34;, \u0026#34;pdf_document\u0026#34;) 该函数导出的PDF并未进行很好的渲染。\n  尝试了使用pandoc对html转换为PDF，但是渲染效果不好。\n  使用Safari浏览器直接将knit的html文件导出为PDF，能够渲染的很好。\n    参考资料：  具体的参数调整官方文档  ","date":"December 15, 2021","image":null,"permalink":"/post/2021-12-15_posterdown/","title":"使用posterdown制作学术海报"},{"categories":["计算机基础"],"contents":"引言 1. 计算机组成 计算机的组成部件可以分为三大类：中央处理单元（CPU）、主存储器和输入/输出子系统。\n中央处理单元 用于数据的运算。在大多数体系结构中，它有3个组成部分：算术逻辑单元（ALU）、控制单元、寄存器组、快速存储定位。\n  算术逻辑单元\n对数据进行逻辑、移位和算术运算。\n  寄存器\n用来存放临时数据的高速独立的存储单元。\n  控制单元\n控制各个子系统的操作。控制是通过从控制单元到其他子系统的信号来进行。\n  主存储器 是存储单元的集合，每一个存储单元都有唯一的标识，称为地址。\n","date":"December 6, 2021","image":null,"permalink":"/post/2021-12-06_consist_of_computer/","title":"计算机组成"},{"categories":["生信","算法","生物统计"],"contents":"引言 潜在语义分析（Latent sematic analysis, LSA）是一种无监督的学习方法。特点是通过矩阵分解来完成，使用的是非概率的话题分析模型，可以通过奇异值分解的方法进行矩阵因子分解，特点是分解的矩阵正交，非负矩阵分解是另一种矩阵的因子分解方法，特点是分解的矩阵非负。\n1. 单词向量空间和话题向量空间 1.1 单词向量空间 给定一个含有 \\( n \\)个文本的集合\\( D=\\left { d_1,d_2,\u0026hellip;,d_n \\right } \\)，在所有文本中出现的\\( m \\)个单词的集合\\( W=\\left { w_1,w_2,\u0026hellip;,w_m \\right } \\)，将单词在文本中出现的数据用一个单词-文本矩阵表示，记作\\( X \\): $$ \\begin{bmatrix} x_{11} \u0026amp;x_{12} \u0026amp;\u0026hellip; \u0026amp;x_{1n} \\ x_{21} \u0026amp;x_{22} \u0026amp;\u0026hellip; \u0026amp;x_{2n} \\ \\vdots \u0026amp;\\vdots \u0026amp; \u0026amp;\\vdots \\ x_{m1} \u0026amp;x_{m2} \u0026amp;\u0026hellip; \u0026amp;x_{mn} \\end{bmatrix} $$\n元素\\( x_{ij} \\)表示单词\\( w_i \\)在文本\\( d_j \\)中出现的频数或权值，由于单词的种类很多，每个文本出现单词的种类通常较少，所以单词-文本矩阵是一个稀疏矩阵。\n权值通常用单词频率-逆文本频率（TF-IDF）表示，定义是： $$ TFIDF_{ij}=\\frac{tf_{ij}}{tf_{·j}}log\\frac{df}{df_i} \\space i=1,2,\u0026hellip;,m; \\space j=1,2,\u0026hellip;,n $$ \\( \\frac{tf_{ij}}{tf_{·j}} \\)表示单词\\( w_i \\)出现在文本\\( d_j \\)中的频数比上文本\\( d_j \\)中出现的所有单词的频数之和，一个单词在一个文本中出现的频数越高，这个单词在文本中的重要度就越高；\\( \\frac{df}{df_i} \\)表示全部文本数比上含有单词\\( w_i \\)的文本数，一个单词在整个文本集合中出现的文本数越少，这个单词就越能代表其所在文本的特点，重要度就越高。TF-IDF是两种重要度的积，表示综合重要度。\n单词向量空间模型直接使用单词-文本矩阵的信息，第\\( j \\)列向量\\( x_j \\)表示文本\\( d_j \\)： $$ x_j=\\begin{bmatrix} x_{1j}\\ x_{2j}\\ \\vdots\\ x_{mj} \\end{bmatrix} $$ 其中$x_{ij}$是单词$w_i$在文本$d_j$的权值，两个单词向量的内积货标准化内积（余弦）表示对应的文本之间的语义相似度，因此文本$d_i$ 与$d_j$ 之间的相似度为: $$ x_i·x_j，\\frac{x_{i}·x_{j}}{\\left | x_{i}\\right |\\left | x_{j}\\right |} $$ $\\cdot $表示向量的内积，$\\left | \\cdot\\right |$表示向量的范数，向量的1-范数即向量元素绝对值之和。两个文本中共同出现的单词越多，其语义就越接近，这个是文本信息处理的一个基本原理。\n单词向量空间的优点是模型简单，计算效率高，局限性是内积相似度未必能够准确表达两个文本的语义相似度，因为自然语言的单词具有一词多义性以及多词一义性，因此基于单词向量的相似度计算存在不精确的问题。\n1.2 话题向量空间 1.2.1 什么是话题向量空间 两个文本的语义相似度可以体现在两者的话题相似度上，话题就是文本所讨论的内容或主题，文本一般含有若干个话题，如果两个文本的话题相似，那么两者的语义应该也相似。话题可以由若干个语义相关的单词表示，则能够解决基于单词的模型存在的问题。话题的个数通常远远小于单词的个数。\n给定一个含有$n$个文本的集合$D=\\left { d_1,d_2,\u0026hellip;,d_n \\right }$，在所有文本中出现的$m$个单词的集合$W=\\left { w_1,w_2,\u0026hellip;,w_m \\right }$，将单词在文本中出现的数据用一个单词-文本矩阵表示，记作$X$，同上单词向量空间中的表示。假设所有的文本共含有$k$个话题，假设每个话题由一个定义在单词集合$W$上的$m$维向量表示，称为话题向量： $$ t_l=\\begin{bmatrix} t_{1l}\\ t_{2l}\\ \\vdots\\ t_{ml} \\end{bmatrix},l=1,2,\u0026hellip;,k $$ 其中$t_{il}$是单词$w_i$在话题$t_l$的权值，权值越大，单词在话题中的重要度就越高，$k$个话题向量张成一个话题向量空间，话题向量空间$T$是单词向量空间$X$的一个字空间。\n单词-话题矩阵： $$ T=\\begin{bmatrix} t_{11} \u0026amp;t_{12} \u0026amp;\u0026hellip; \u0026amp;t_{1k} \\ t_{21} \u0026amp;t_{22} \u0026amp;\u0026hellip; \u0026amp;t_{2k} \\ \\vdots \u0026amp;\\vdots \u0026amp; \u0026amp;\\vdots \\ t_{m1} \u0026amp;t_{m2} \u0026amp;\u0026hellip; \u0026amp;t_{mk} \\end{bmatrix} $$\n1.2.2 文本在话题向量空间的表示 将单词向量投影到话题向量空间$T$中，得到话题向量空间的一个向量$y_j$ $$ y_j=\\begin{bmatrix} y_{1j}\\ y_{2j}\\ \\vdots\\ y_{mj} \\end{bmatrix},j=1,2,\u0026hellip;,n $$ 其中$y_{lj}$是文本$d_j$在话题$t_l$的权值，$l=1,2,\u0026hellip;,k$，权值越大，该话题在该文本中的重要度就越高。\n话题-文本矩阵： $$ Y=\\begin{bmatrix} y_{11} \u0026amp;y_{12} \u0026amp;\u0026hellip; \u0026amp;y_{1n} \\ y_{21} \u0026amp;y_{22} \u0026amp;\u0026hellip; \u0026amp;y_{2n} \\ \\vdots \u0026amp;\\vdots \u0026amp; \u0026amp;\\vdots \\ y_{k1} \u0026amp;y_{k2} \u0026amp;\u0026hellip; \u0026amp;y_{kn} \\end{bmatrix} $$\n1.2.3 从单词向量空间到话题向量空间的线性变换 单词-文本矩阵$X$可以近似的表示为单词-话题矩阵与话题-文本矩阵的乘积形式，这就是潜在语义分析： $$ X\\approx TY $$ 直观上潜在语义分析是将文本在单词向量空间的表示通过线性变换转换为在话题向量空间中的表示：\n原始的单词向量空间中，两个文本的相似度可以由对应的向量的内积表示$x_i·x_j$，经过潜在语义分析，在话题向量空间中，两个文本的相似度可以由对应的向量的内积表示：$y_i·y_j$。\n2. 潜在语义分析 2.1 矩阵奇异值分解算法 奇异值分解（SVD）是一种矩阵因子分解方法，任何一个$m\\times n$矩阵，都可以表示为三个矩阵的乘积形式，分别是$m$阶正交矩阵、由降序排列的非负的对角线元素组成的$m\\times n$矩阵对角矩阵和$n$阶正交矩阵，成为该矩阵的奇异值分解，矩阵的奇异值分解一定存在但不唯一，可以看做是矩阵数据压缩的一种方法。奇异值分解不要求矩阵是方阵。\n 正交矩阵：正交矩阵（Orthogonal Matrix）是指其转置等于其逆的矩阵。\n 奇异值分解是在平方损失（弗罗贝尼乌斯范数）意义下对矩阵的最优近似，紧奇异值分解对应着无损压缩，截断奇异值分解对应着有损压缩。潜在语义分析根据确定的话题个数$k$对单词-文本矩阵$X$进行截断奇异值分解： $$ X\\approx U_k（\\Sigma_k V{k}^{T}） $$ 矩阵的奇异值分解可以看作是将其对应的线性变换分解为旋转变换、缩放变换及旋转变换的组合，得到话题空间$U_k$，以及文本在话题空间的表示$\\Sigma_k V{k}^{T}$。\n2.2 非负矩阵分解 $$ X\\approx WH $$\n2.2.1 损失函数或代价函数 损失函数可以有以下几种：\n  平方损失\n  散度\n  2.2.2 算法 以上两个目标函数：平方损失和散度知识对变量$W$和$H$之一的凸函数，而不是同时对两个变量的凸函数，因此找到全局最优比较困难。Lee提出的基于“乘法更新规则”的优化算法，交替地对$W$和$H$进行更新。\n文献  《统计学习方法》第2版 李航  ","date":"November 26, 2021","image":null,"permalink":"/post/2021-11-26_statistic-lsa/","title":"潜在语义分析（LSA）"},{"categories":["生信","算法","生物统计"],"contents":"引言 1. 马尔可夫模型的基本概念 来对2段氨基酸序列x和y进行残基比对，认为存在3种比对关系的状态：\n M：残基能够比对上但不一定相等 X：序列x的残基比对到1个空位，或x上发生了1次插入 Y：序列y的残基比对到1个空位，或y上发生了1次插入  序列比对就是在上述3个状态中不断转换的过程：\n \\( M(i,j) \\) : \\( x_i \\)比对到\\( y_j \\)时，序列x从1到\\( i \\)和序列\\( y \\)从1到\\( j \\)最好的比对分数 \\( X(i,j) \\) : \\( x_i \\)比对到空位时，序列x从1到\\( i \\)最好的比对分数 \\( Y(i,j) \\) : \\( y_j \\)比对到空位时，序列y从1到\\( j \\)最好的比对分数  转移（从一个状态到另外一个状态）概率：\n$$ a_{kl} = P (X_t=S_l|X_{t-1}=S_k) $$\n$$ a_{lk} = P (X_t=S_k|X_{t-1}=S_l) $$\n转移矩阵：\n \n 设定：\n \\( \\delta \\) ：Gap open（d）的概率 \\( \\epsilon \\) ：Gap extension（e）的概率   马尔可夫链\n 先根据转移概率得到一个转移概率矩阵：\n 转移矩阵\n 假设匹配状态是XMMY：\n 匹配状态\n 计算匹配状态的概率： $$ P(XMMY)=\\alpha_{XM}\\alpha_{MM}\\alpha_{MY} = (1-\\epsilon)(1-2\\delta)\\delta $$\n通过上面的例子，下面介绍马尔可夫模型的一些概念：\n  马尔可夫过程：\n马尔可夫过程是一类随机过程，该过程的“将来”仅依赖“现在”，而不依赖“过去”，把一个总随机过程看作是状态的不断转移，表达式为： $$ x(t+1)=f(x(t)) $$\n  马尔可夫链：\n时间和状态都离散的马尔可夫过程。\n  马尔可夫状态链的状态空间： $$ S = \\begin{Bmatrix} S_1,\u0026amp; S_2,\u0026amp; S_3,\u0026hellip; \\end{Bmatrix}S_i\\in R $$\n  马尔可夫链在时刻\\( t \\)处于状态\\( S_i \\)条件下，在时刻\\( t+1 \\)转移到状态\\( S_j \\)的转移概率是条件概率： $$ P_{ij}(t, t+1)=P\\begin{Bmatrix} x_{t+1}=S_j|x_t=S_i \\end{Bmatrix} $$\n  由于马氏链在时刻 \\( t \\) 从任何一个状态\\( S_i \\)出发，到下一时刻 \\( t+1 \\)，必然转移到\\( S_j \\)，\\( j=1,2,… \\)，诸状\n态中的某一个，所以有：\n $$ \\forall _{i^{\\forall}}\\sum_{j}P_{ij}(t,t+1)=1 $$  当\\( P_{ij}(t,t+1) \\)与\\( t \\)无关时，为齐次马尔可夫链，通常说到的马尔可夫链都是指齐次的。马尔可夫链除了仅依赖于上一次观测值的一阶马尔可夫链，还可以依赖多个连续观测数据，后者称之为高阶马尔可夫链。\n  2. 马尔可夫模型的组成   随机序列变量 $$ X = \\begin{Bmatrix} x_1,\u0026amp; x_2,\u0026amp; x_3,\u0026hellip;,x_t \\end{Bmatrix} $$\n  状态空间 $$ S = \\begin{Bmatrix} S_1,\u0026amp; S_2,\u0026amp; S_3,\u0026hellip; \\end{Bmatrix}\\space\\space S_i\\in R \\space\\space\\space i=1,2,\u0026hellip;,n $$\n  转移概率矩阵 $$ P=\\begin{Bmatrix} P_{ij}=P(x_{t+1}=S_j|x_t=S_i) \\end{Bmatrix} $$\n  初始状态向量 $$ \\prod =\\begin{Bmatrix} \\pi=P(x_0=S_i) \\end{Bmatrix} , \\space\\space\\space i=1,2,\u0026hellip;,n $$\n  3. 隐马尔可夫模型 隐马尔可夫模型是结构最简单的动态贝叶斯网，可以用五元组来描述分别为：\n 状态空间\\( S \\) 状态对应的观测空间\\( X \\) 状态转移矩阵\\( A \\) 每个状态下观测事件的概率矩阵\\( B \\) 初始状态概率分布\\( \\pi \\)  给出这五个参数就能够确定一个隐马尔可夫模型，通常用参数\\( \\lambda=\\begin{Bmatrix}A,B,\\pi\\end{Bmatrix} \\)来指代。\n举一个新的例子：基因预测——给定序列预测编码区\n隐马尔可夫模型在状态的基础上，增加符号的概念，每个状态可以以不同的概率产生可观测到的符号。在例子中，给定的基因组序列为观测到的符号串，编码和非编码为2种隐状态，即编码与否是未知的，需要通过已知的符号来推测。\n 示例：马尔可夫链\n 转移矩阵：\n基因组会同时包含编码和非编码区域，自我转移的箭头表示状态的连续。\n 转移矩阵\n 转移概率矩阵：\n 转移概率矩阵\n $$ a_{kl} = P (X_t=S_l|X_{t-1}=S_k) $$\n生成概率：\n 生成概率\n 这里的状态路径无法进行观测，需要根据符号路径来推测状态，引入生成概率，状态\\( S_k \\)时产生符号\\( b \\)的概率： $$ e_{k}(b) = P (y_i=b|X_{i}=S_k) $$ 生成概率矩阵（在这里有两个）：\n 2个生成概率矩阵\n 训练集：正确标记好了编码和非编码区域的DNA序列。\n根据训练集填好上述转移概率矩阵和生成概率矩阵，来对未知的给定基因组序列反推最可能的基因组状态路径。\nLogarithmic transformation：引入对数计算将乘法变成加法，对转移/生成矩阵概率取\\( log_{10} \\)（在计算机运算中，很容易因为连乘的次数的增加，很容音因为数值过小，出现下溢的问题）\n测试序列：CGAAAAAATCG\n根据训练集得到转移概率矩阵和生成概率矩阵：\n取log10的转移概率矩阵\n取log10的生成概率矩阵\n 动态规划进行序列比对：\n 序列比对\n 假设n状态和c状态默认的分布比例分别是\\( log_{10}(0.8) \\)和\\( log_{10}(0.2) \\)，使用序列比对的方法，找到概率最大的值为起点进行回溯。最终找到的状态链为：NNCCCCCCNNN。\n根据示例我们可以得到一个大概的步骤来对隐马尔可夫模型进行应用：\n 首先要确定具体问题中什么是状态，什么是符号 其次根据状态列出转移概率矩阵，根据状态和符号列出生成概率矩阵 根据训练集填上矩阵的具体数值 根据具体问题使用相应的解决方法(本例子中根据数据来对未知的给定基因组序列反推出最可能的基因组状态路径，迭代使用动态规划进行序列比对)  实际应用中关注隐马尔可夫模型的3个基本问题以及对应的解决方法：\n  估算问题：\n给定模型\\( \\lambda=\\begin{Bmatrix}A,B,\\pi\\end{Bmatrix} \\)，如何有效计算观测序列\\( x=\\begin{Bmatrix} x_1,x_2,\u0026hellip;,x_n\\end{Bmatrix} \\)出现的概率？也就是模型和观测序列之间的匹配程度。\n解决：foreward和backward算法\n  解码问题：\n给定模型\\( \\lambda=\\begin{Bmatrix}A,B,\\pi\\end{Bmatrix} \\)和观测序列\\( x=\\begin{Bmatrix} x_1,x_2,\u0026hellip;,x_n\\end{Bmatrix} \\)，如何找到和观测序列最匹配的状态序列？也就是找到隐藏的模型状态。\n解决：Viterbi算法\n  学习问题或训练问题：\n给定观测序列\\( x=\\begin{Bmatrix} x_1,x_2,\u0026hellip;,x_n\\end{Bmatrix} \\)，如何调整模型参数\\( \\lambda=\\begin{Bmatrix}A,B,\\pi\\end{Bmatrix} \\)，使得该观测序列发生的概率最大？也就是根据训练样本学得最优的参数模型。\n解决：Baum-Welch算法\n  4. 隐马尔可夫模型判断CNV 以PennCNV为例介绍HMM在实际判断CNV中的应用。PennCNV是一个免费的检测SNP分型阵列芯片的拷贝数变异的工具，目前可以处理Illumina和Affymetrix array的数据来得到信号强度，若使用其他类型的SNP芯片数据和寡核苷酸芯片需要预先处理文件的格式。PennCNV使用隐马尔可夫模型整合多个来源的信息来对单个样本推断CNV。它与基于分割的算法不同，除了单独考虑信号强度外，还考虑了SNP等位基因比例分布等因素。\n PennCNV通过基因型来检测拷贝数变异的算法流程（Wang K et al. 2007）\n 在PennCNV中使用的是一阶HMM，隐状态是人为设定的离散值1，2，3，4，5，6，各自对应的总拷贝数数值和CNV基因型如上表所示。注意这里最大的拷贝数状态设定为拷贝数为4，因为4个和4个以上的拷贝无法进行区分。符号是信号强度值的两种形式：BAF和LRR。结合BAF和LRR可以判断不同的拷贝数以及区分出拷贝中性的LOH（文章中没有具体说怎么结合BAF和LRR数据的，但是通过公示我的理解是BAF和LRR同时发生的联合概率作为HMM的生成概率）。\n 隐状态，拷贝数数值及其对应的描述（Wang K et al. 2007）\n 下面介绍一些定义：\n  原始的信号强度数据需要经过“5步标准化”（http://icom.illumina.com/iom/software.ilmn），每个SNP的X值和Y值分别代表实验得到经过标准化的等位基因A和B的信号强度值，\\( R \\)为总信号强度： $$ R = X + Y $$\n   \\( \\theta \\)为相对的等位信号强度比率： $$ \\theta = \\frac{arctan(Y/X)}{\\pi/2} $$ 该计算分母为\\( \\pi/2 \\)可以将数值\\( \\theta \\)压缩在-1～1之间。\n  B Allele Frequency （BAF）通常指的是标准化的等位基因B和A的相对信号强度：\n BAF公式（Wang K et al. 2007）\n 0代表只检测到了A这个allele对应的荧光信号，分型结果为AA；1代表只检测到了B这个allele对应的荧光信号，分型结果为BB；0.5代表A和B这两个allele的荧光信号强度相等，分型结果为AB。\\( \\theta_{AA} \\),\\( \\theta_{AB} \\),\\( \\theta_{BB} \\)都是根据正常样本得到的值，荧光信号存在一定程度的扰动，因此BAF的取值是在0-1范围波动的值。\n  \\( R_{observed} \\)是观测值，\\( R_{expected} \\)是通过算法拟合得到的数值代表了正常样本中的信号强度，每个SNP的\\( log \\) R 比率（LRR）：\n  $$ LRR={log_{2}}(R_{observed}/R_{expected}) $$\n​\tLRR = 0\t拷贝数为2；LRR \u0026gt; 0 拷贝数增加；LRR \u0026lt; 0 拷贝数减少\n介绍完定义之后，来确定转移概率和生成概率以及对应矩阵。\n  LRR的生成概率：\n是混合的均匀和正态分布模型， $$ P(r|z)=\\pi_r+(1-\\pi_r)\\phi(r;\\mu_{r,z},S_{r,z}) $$ \\( (\\phi·；·) \\)是均值为\\( \\mu_{r,z} \\)，标准差为\\( S_{r,z} \\)的正态分布，均匀分布用于对于芯片中随机的信号波动和可能的基因组错误注释和错误assmbly的建模。\n  BAF的生成概率：\n对于每一个隐状态（除了状态1），有不同可能的基因型也就有不同模式的BAF。\n BAF生成概率（Wang K et al. 2007）\n   对于染色体X进行特殊处理：\n将染色体X中所有的SNP的LRR值减去1个常数，使得针对女性，平均的LRR值不是0，针对男性，LRR不是单拷贝删除的LRR的参考值，因为正常的男性染色体X的状态就是state2不应该是state3。\n  隐状态的转移概率：\n即两个相邻SNP发生拷贝数状态改变的概率，一般来讲，临近的SNPs拷贝数状态不太可能发生变化，但是较远的SNPs的拷贝数状态更容易发生改变。因此需要找到转移概率和距离的关系，那么转移概率为：\n $$ p(z_i=l|z_{i-1}=j)=\\left\\{\\begin{matrix} 1-\\sum_{k=2}^{b}p_{j,k-1}(1-e^{d_i/D}),if \\space l=j \\\\ p_{j,l-1}(1-e^{d_i/D}),if \\space l\\neq j \\end{matrix}\\right. $$ \\( D \\)是一个常数，当状态为4时设为100Mb，其他状态为100kb，p是未知参数，用Baum-Welch算法进行估计，得到最优的参数后，再使用Viterbi算法进行隐状态的判断。在PennCNV的分析中，排除了所有包含小于等于\u00032个SNP的CNV，因为这些CNV中可能存在较高的假阳性比例。\n  介绍算法之前先定义几个概念：\n \\( O = O_1,O_2,\u0026hellip;,O_t \\)\t输出的观察序列符号 \\( P(O|\\lambda) \\) 给定模型参数时，输出符号序列\\( O \\)的概率 \\( a_{ij} \\)\t从状态\\( S_i \\)到状态\\( S_j \\)的转移概率 \\( b_j(O_t) \\)\t在状态\\( S_j \\)时，输出\\( O_t \\)的概率 \\( a_t(j) \\)\t输出部分符号序号\\( O = O_1,O_2,\u0026hellip;,O_t \\)达到状态\\( S_j \\)时的前向概率  前向算法：\n 前向算法\n  前向算法步骤\n Vertibi算法\n介绍完定义，明确完我们的五元组，我们再明确一下我们的问题是解码问题，解码问题对应的解法是Viterbi算法，介绍一下该算法具体过程：\n Vertibi算法步骤\n 参考   Wang, Kai, et al. \u0026ldquo;PennCNV: an integrated hidden Markov model designed for high-resolution copy number variation detection in whole-genome SNP genotyping data.\u0026rdquo; Genome research 17.11 (2007): 1665-1674.\n  图：Kai Wang et al. Genome Res. 2007;17:1665-1674\n  Circular Binary Segmentation from Jeremy Teibelbaum\u0026amp;rsquo;s blog\n  CKVkit：https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004873\n  Ming-Lian\u0026amp;rsquo;s blog\n  隐马尔可夫模型：https://www.cnblogs.com/skyme/p/4651331.html\n  coursera生物信息学：导论与方法第四周\n  动态贝叶斯网络\n  ","date":"November 19, 2021","image":null,"permalink":"/post/2021-11-19_hmm/","title":"隐马尔可夫模型（HMM）"},{"categories":["R"],"contents":"引言 使用do.call批量读入文件并合并。\n1. 基本处理步骤 source_dir \u0026lt;- \u0026#34;文件所在路径\u0026#34; file \u0026lt;- list.files(  path = source_dir,  pattern = \u0026#34;*.txt\u0026#34;, # 目标文件夹下需要的文件的格式  all.files = F,  full.names = F,  recursive = F,  include.dirs = F ) allfile \u0026lt;- lapply(file, function(x){read.csv2(paste0(source_dir, x), sep = \u0026#34;\\t\u0026#34;)}) combine \u0026lt;- do.call(rbind, allfile) 2. 涉及到的需求和参数设定   文件名作为新列补充进数据框\n  对文件名进行更改\n  参考    ","date":"November 17, 2021","image":null,"permalink":"/post/2021-11-17_docall/","title":"R：批量读入文件并合并"},{"categories":["生信","R"],"contents":"引言 一个提供批量下载Synapser数据的R包。\n1. 登陆 synLogin(\u0026#34;baomihai@sina.com\u0026#34;,\u0026#34;******\u0026#34;) Welcome, baomihai@sina.com!NULL 参考  biostars-How to install gdc-client in Ubnutu  ","date":"July 24, 2021","image":null,"permalink":"/post/2021-8-31_synapser/","title":"synapser"},{"categories":["R"],"contents":"引言 stringr包是建立在stringi上的，stringi包使用ICU C库提供准确、快速的常见字符串操作，stringr提供了最重要和最常用的字符串处理函数。\nstringr stringr包中所有的函数都以str_开头，第一个参数为字符串向量。对应的在base函数中也有功能一致的函数，记得对比两者之间的异同。\n1. 找到自己需要的数据 tcga_mut \u0026lt;- read.csv2(\u0026#34;/home/tzy/projects/CNX-method/data/TCGA/mc3.v0.2.8.PUBLIC.nonsilentGene.xena\u0026#34;,sep = \u0026#34;\\t\u0026#34;)  saveRDS(tcga_mut, file = \u0026#34;/home/tzy/projects/CNX-method/data/TCGA/tcga_mut.rds\u0026#34;) save(tcga_mut, file = \u0026#34;/home/tzy/projects/CNX-method/data/TCGA/tcga_mut.RData\u0026#34;) 参考  biostars-How to install gdc-client in Ubnutu  ","date":"July 24, 2021","image":null,"permalink":"/post/2021-7-24_stringr_stringi/","title":"处理字符串的两个R包:stringi和stringr"},{"categories":["生信"],"contents":"引言 之前一直用别的方法下载数据，这次使用了gdc-client命令行去下载GDC上TCGA driver gene mutation的一批数据。\n步骤 1. 找到自己需要的数据 这是我本次要下载的数据\n点击数据下载地址发现出现如下界面，其中id就是使用gdc-client下载的文件对应的id\n对于Open access data，使用这两种方法下载\n下载了MAC的Client版本\n2. 安装 解压下载的文件，如果双击会发现出现erro：\n且常规的对~/.bash_profile文件添加环境变量也不可以，正确做法是：\n./gdc-client #(to verify that program works) cp -pi ./gdc-client /usr/local/bin #(if this does not work) sudo cp -pi ./gdc-client /usr/local/bin 在任何路径都可以打开。\n3. 使用  下载单个文件  gdc-client download id  下载多个文件  将含有多个文件的id和名字等信息的页面存储为txt，download加上-m，进行批量下载。\ngdc-client download -m ./PanCan-Driver_Open_GDC-Manifest.txt 参考  biostars-How to install gdc-client in Ubnutu  ","date":"July 20, 2021","image":null,"permalink":"/post/2021-7-20_gdc_client/","title":"gdc-client"},{"categories":["R"],"contents":"引言 rds比RData省空间，为什么？\n步骤 1. 找到自己需要的数据 tcga_mut \u0026lt;- read.csv2(\u0026#34;/home/tzy/projects/CNX-method/data/TCGA/mc3.v0.2.8.PUBLIC.nonsilentGene.xena\u0026#34;,sep = \u0026#34;\\t\u0026#34;)  saveRDS(tcga_mut, file = \u0026#34;/home/tzy/projects/CNX-method/data/TCGA/tcga_mut.rds\u0026#34;) save(tcga_mut, file = \u0026#34;/home/tzy/projects/CNX-method/data/TCGA/tcga_mut.RData\u0026#34;) 参考  biostars-How to install gdc-client in Ubnutu  ","date":"July 20, 2021","image":null,"permalink":"/post/2021-7-20_rdata/","title":"R储存数据"},{"categories":["技术"],"contents":"步骤 1. 配置Leancloud 这部分详细参考：hugo博客添加评论系统Valine\n2. 更改 comments.html 文件 将整体内容替换成如下代码：\n\u0026lt;!-- valine change from origin code--\u0026gt;  {{- if .Site.Params.valine.enable -}}  \u0026lt;!-- id 将作为查询条件 --\u0026gt;   \u0026lt;div id=\u0026#34;vcomments\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;  \u0026lt;script src=\u0026#34;//cdn1.lncld.net/static/js/3.0.4/av-min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;  \u0026lt;script src=\u0026#39;//unpkg.com/valine/dist/Valine.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt;   \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt;  new Valine({  el: \u0026#39;#vcomments\u0026#39; ,  appId: \u0026#39;{{ .Site.Params.valine.appId }}\u0026#39;,  appKey: \u0026#39;{{ .Site.Params.valine.appKey }}\u0026#39;,  notify: \u0026#39;{{ .Site.Params.valine.notify }}\u0026#39;,  verify: \u0026#39;{{ .Site.Params.valine.verify }}\u0026#39;,  avatar:\u0026#39;{{ .Site.Params.valine.avatar }}\u0026#39;,  placeholder: \u0026#39;{{ .Site.Params.valine.placeholder }}\u0026#39;,  visitor: \u0026#39;{{ .Site.Params.valine.visitor }}\u0026#39;  });  \u0026lt;/script\u0026gt;  {{- end -}} 3. 引入评论 layouts/_default/single.html 中引入评论，本主题已有配置：\n {{ if not ( eq .Params.comments false) }}  {{ .Render \u0026#34;comments\u0026#34; }}  {{ end }} 参考   Hugo评论插件集成之Valine\n  hugo博客添加评论系统Valine\n  关于该主题已有的配置：给Hugo个人博客添加Valine评论系统\n  Valine配置项\n  ","date":"June 8, 2021","image":null,"permalink":"/post/2021-6-8_comment/","title":"hugo主题增加valine评论功能_test"},{"categories":["技术"],"contents":"引言 根据hugo-future-imperfect-slim主题中issue提到的TOC更改版本，改进后进行配置应用。\n步骤 1. 更改 config.html 文件 在[params] 内容下加入以下参数：\n toc = true # 默认显示toc  tocWords = 400 #超过400字显示toc 2. 更改 main.scss 文件 添加TableOfContents：\n#TableOfContents {  border: $secondary-border;   ul {  list-style-type: none;  padding-inline-start: 1.5em;  } } /* ==========================================================================  Add-Ons  ========================================================================== */  /* reCaptcha */ 3. 更改 single.html 文件 在content内容中添加{{ .TableOfContents }}\n \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt;  {{ .Render \u0026#34;featured\u0026#34; }}  {{ .TableOfContents }} #此处添加一行  {{ .Content }}  \u0026lt;/div\u0026gt; 更新——2022.11.14 由于我更换的新的博客主题：logbook主题，因此对于TOC的配置也有一些变化，新的配置过程主要参考的是Vincent Liu的教程\n 在layouts/partials目录下新建toc.html，toc.html内容如下  \u0026lt;!-- toc.html --\u0026gt; \u0026lt;!-- ignore empty links with + --\u0026gt; {{ $headers := findRE \u0026#34;\u0026lt;h[1-4].*?\u0026gt;(.|\\n])+?\u0026lt;/h[1-4]\u0026gt;\u0026#34; .Content }} \u0026lt;!-- at least one header to link to --\u0026gt; {{ if ge (len $headers) 1 }} {{ $h1_n := len (findRE \u0026#34;(.|\\n])+?\u0026#34; .Content) }} {{ $re := (cond (eq $h1_n 0) \u0026#34;\u0026lt;h[2-4]\u0026#34; \u0026#34;\u0026lt;h[1-4]\u0026#34;) }} {{ $renum := (cond (eq $h1_n 0) \u0026#34;[2-4]\u0026#34; \u0026#34;[1-4]\u0026#34;) }}  \u0026lt;!--Scrollspy--\u0026gt; \u0026lt;div class=\u0026#34;toc\u0026#34;\u0026gt;   \u0026lt;div class=\u0026#34;page-header\u0026#34;\u0026gt;\u0026lt;strong\u0026gt;- CATALOG -\u0026lt;/strong\u0026gt;\u0026lt;/div\u0026gt;   \u0026lt;div id=\u0026#34;page-scrollspy\u0026#34; class=\u0026#34;toc-nav\u0026#34;\u0026gt;   {{ range $headers }}  {{ $header := . }}  {{ range first 1 (findRE $re $header 1) }}  {{ range findRE $renum . 1 }}  {{ $next_heading := (cond (eq $h1_n 0) (sub (int .) 1 ) (int . ) ) }}  {{ range seq $next_heading }}  \u0026lt;ul class=\u0026#34;nav\u0026#34;\u0026gt;  {{end}}  {{ $anchorId := (replaceRE \u0026#34;.* id=\\\u0026#34;(.*?)\\\u0026#34;.*\u0026#34; \u0026#34;$1\u0026#34; $header ) }}  \u0026lt;li class=\u0026#34;nav-item\u0026#34;\u0026gt;  \u0026lt;a class=\u0026#34;nav-link text-left\u0026#34; href=\u0026#34;#{{ $anchorId}}\u0026#34;\u0026gt;  {{ $header | plainify | htmlUnescape }}  \u0026lt;/a\u0026gt;  \u0026lt;/li\u0026gt;  \u0026lt;!-- close list --\u0026gt;  {{ range seq $next_heading }}  \u0026lt;/ul\u0026gt;  {{ end }}  {{ end }}  {{ end }}  {{ end }}   \u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt; \u0026lt;!--Scrollspy--\u0026gt;  {{ end }} 新建toc样式，在自己主题的css样式文件中加入以下内容，logbook主题是更改themes/logbook/assets/scss/templates/_main.scss文件  /* toc style */ .toc {  position: fixed;  top: 50%;  left: 2%;  width: 20%;  transform: translateY(-50%);  background-color: #f6f6f6;  /*border: solid 1px #c9c9c9;*/  border-radius: 5px;  padding-bottom: 1rem; }  .toc .page-header {  margin-top: 1rem;  margin-bottom: 1rem; }  .toc-nav ul {  overflow:hidden;  white-space:nowrap;  line-height: 1rem; }  /* ignore h1 header */ .toc-nav ul ul ul {  margin-left: 2rem; }  .toc-nav .nav-link {  text-overflow:ellipsis;  overflow:hidden;  color: #333; }  .toc-nav a.nav-link:hover {  background-color: #f6f6f6;  color: #d78a64;  /*color: var(--accent);*/  /*border-left: 2px solid #ce8460;*/  border-left: solid 2px var(--accent); }    /* Media Queries */ @media (max-width: 1080px) {  main {  max-width: 100%;  }  .toc {  display: none;  } } .toc-nav a.nav-link:hover是鼠标滑过时toc标题产生颜色变化。\nmedia是设置小屏幕不显示toc\n在layouts/_default/single.html中加入以下内容，放在了 \u0026lt;/article\u0026gt; 和comment之间。  {{ if .Site.Params.toc | default true }} {{ partial \u0026#34;toc\u0026#34; . }} {{ end }} 在themes/logbook/assets/js/script.js中添加  /* scroll to the anchor and scroll spy */ var navbarHeight = 55; var scrollSpeed = 200; $(\u0026#34;#page-scrollspy a.nav-link\u0026#34;).on(\u0026#39;click\u0026#39;, function () {  /* decode chinese hash */  var target = decodeURI(this.hash.replace(/^#/, \u0026#39;\u0026#39;));  $(\u0026#39;html,body\u0026#39;).animate({scrollTop: $(\u0026#34;:header[id=\u0026#39;\u0026#34; + target + \u0026#34;\u0026#39;]\u0026#34;).offset().top - navbarHeight}, scrollSpeed);  return false; }); 参考   图片来源\n  hugo官方文档\n  github issue\n  可以尝试的其他方法：HuGo中文章加入目录\n  Hugo博客侧边导航栏\n  Hugo添加文章目录toc\n  Blog养成记(13) 增加一个TOC侧边栏\n  ","date":"June 8, 2021","image":null,"permalink":"/post/2021-6-8_toc/","title":"hugo增加TOC"},{"categories":["技术"],"contents":"引言 分为功能改进、美观改进。\n1. 博客功能改进   个人博客归档（已完成）\n  date的时间更换成自动化填充，每次手动填写很麻烦\n  搜索中文优化，目前速度较慢\n  更改归档页面时间0001\n  每篇博文的分享页面超链接到对应网址\n      删除不需要的语言支持，只保留英文和中文\n  增加评论功能Valine（已完成）\n  参考：https://www.smslit.top/2018/07/08/hugo-valine/\n    分类更加细化，最好能有层层分类\n  ABOUT页面填充自己的个人信息\n  翻页添加页码\n  博文添加目录（已完成）\n  代码块配色更改，增加浅色代码块阴影\n  2. 博客整体美观   每个博文的图片选取大小合适的\n  标题的英文或中文的字距过大，比如下图的 引言 和 博客功能改进\n    ","date":"June 8, 2021","image":null,"permalink":"/post/2021-6-8_optimization_of_the_blog/","title":"博客优化计划"},{"categories":["技术"],"contents":"引言 hugo没有自带的归档设置，需要手动添加。\n操作步骤   在taozy_blog/layouts/_default/目录下创建 archives.html 文件\n  将taozy_blog/layouts/_default/目录下的 single.html 内容复制进 archives.html 文件（single.html的格式就是每篇博文的格式，也可以采用主题的contact.html的格式或者about.html的格式）\n  找到archives.html 文件中的{{ .Content }} 替换为下面的内容:\n  {{ range (.Site.RegularPages.GroupByDate \u0026#34;2006\u0026#34;) }}  \u0026lt;h3\u0026gt;{{ .Key }}\u0026lt;/h3\u0026gt;   \u0026lt;ul class=\u0026#34;archive-list\u0026#34;\u0026gt;  {{ range (where .Pages \u0026#34;Type\u0026#34; \u0026#34;blog\u0026#34;) }}  \u0026lt;li\u0026gt;  {{ .PublishDate.Format \u0026#34;2006-01-02\u0026#34; }}  -\u0026gt;  \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;  \u0026lt;/li\u0026gt;  {{ end }}  \u0026lt;/ul\u0026gt; {{ end }} 对上述代码进行解读:\n 归档目录  Pages \u0026ldquo;Type\u0026rdquo; \u0026ldquo;blog\u0026quot;即归档目录设置为content/blog/下的内容，如果去掉blog，引号内留空，就会自动归档根目录下的文件，也就是content目录的文件。\n 可选归档时间  .Site.RegularPages.GroupByDate \u0026ldquo;2006\u0026rdquo;：按年归档 .Site.RegularPages.GroupByDate \u0026ldquo;2006-01\u0026rdquo;：按年月归档\n在config.toml文件[menu]中仿照其他添加以下代码，使得主页上栏显示该分类：   [[menu.main]]  name = \u0026#34;Archives\u0026#34;  identifier = \u0026#34;archives\u0026#34;  url = \u0026#34;/archives/\u0026#34;  pre = \u0026#34;\u0026lt;i class=\u0026#39;fa fa-newspaper\u0026#39;\u0026gt;\u0026lt;/i\u0026gt;\u0026#34;  weight = 6 如下：\n完成归档页面的建立。\n 更新——2022.4.2 由于我更换的新的博客主题：logbook主题，因此对于归档页面的配置也有一些变化，新的配置过程如下\n  同上\n  同上\n  在theme/config/_default/menus.en.toml中添加\n  [[main]] name = \u0026#34;Archives\u0026#34; url = \u0026#34;archives\u0026#34; weight = 5 #主页上面的按钮排序为第5 用来替换content的代码{{- if ne .Key \u0026quot;0001\u0026quot; }}来去除莫名出现的年份：0001；另外注意{{ range (where .Pages \u0026quot;type\u0026quot; \u0026quot;post\u0026quot;) }}中的type需要更改为Section（因为存放博文的目录和之前博客主题不同）。  {{ range (.Site.RegularPages.GroupByDate \u0026#34;2006\u0026#34;) }}  {{- if ne .Key \u0026#34;0001\u0026#34; }}  \u0026lt;h3\u0026gt;{{ .Key }}\u0026lt;/h3\u0026gt;   \u0026lt;ul class=\u0026#34;archive-list\u0026#34;\u0026gt;  {{ range (where .Pages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34;) }}  \u0026lt;li\u0026gt;  {{ .PublishDate.Format \u0026#34;2006-01-02\u0026#34; }}  -\u0026gt;  \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;  \u0026lt;/li\u0026gt;  {{ end }}  \u0026lt;/ul\u0026gt; {{ end }} 在博文目录下新建一个archives.md文件，文件内容如下：  --- title: \u0026#34;归档\u0026#34; layout: \u0026#34;archives\u0026#34; description: \u0026#34;历史文章按照年归档.\u0026#34; draft: false --- 如果想要单独的年份归档页面（我这里只有2021和2022），可以如下操作\n 在taozy_blog/layouts/_default/目录下创建 archives1.html 文件和archives2.html 文件，其中{{ .Content }}替换代码改为如下：   {{ range (.Site.RegularPages.GroupByDate \u0026#34;2006\u0026#34;) }}  {{ if ne .Key \u0026#34;0001\u0026#34; }}   \u0026lt;ul class=\u0026#34;archive-list\u0026#34;\u0026gt;  {{ if eq .Key \u0026#34;2021\u0026#34; }} #2022的需要修改为2022  \u0026lt;h3\u0026gt;{{ .Key }}\u0026lt;/h3\u0026gt;  {{ range (where .Pages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34;) }}  \u0026lt;li\u0026gt;  {{ .PublishDate.Format \u0026#34;2006-01-02\u0026#34; }}  -  \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;  \u0026lt;/li\u0026gt;  {{ end }}  {{ end }}  \u0026lt;/ul\u0026gt; {{ end }}  在theme/config/_default/menus.en.toml中添加  [[main]] name = \u0026#34;Archives\u0026#34; weight = 5 hasChildren = true   [[main]]  parent = \u0026#34;Archives\u0026#34;  name = \u0026#34;2022\u0026#34;  url = \u0026#34;2022/\u0026#34;  weight = 1   [[main]]  parent = \u0026#34;Archives\u0026#34;  name = \u0026#34;2021\u0026#34;  url = \u0026#34;2021/\u0026#34;  weight = 2  在放置博客的目录下新建两个博文2021.md和2022.md，2021.md的内容如下  --- title: \u0026#34;归档\u0026#34; layout: \u0026#34;archives1\u0026#34; description: \u0026#34;历史文章按照年月归档.\u0026#34; draft: false --- 完成。\n参考：   配置主要参考：为hugo添加归档页面\n  我为什么要从 Hexo 更换到 Hugo\n  解读部分主要参考：Hugo添加归档页面\n  Hugo博客时间轴中文化\n  hugo获取某个归档中文章个数\n  ","date":"June 7, 2021","image":null,"permalink":"/post/2021-6-7_%E9%85%8D%E7%BD%AE%E5%BD%92%E6%A1%A3/","title":"hugo博客配置归档页面"},{"categories":["BUG"],"contents":"引言 为了在集群上跑1000个模拟样本的SigprofilerExtractor工具，在集群上自己新建的环境里安装，解决安装bug，并成功使用。\n解决bug思路   发现依赖torch1.5.1版本\n  通过pip install安装失败\n  使用whl安装发现没有对应python3.9的版本\n  根据版本推测1.5.1不能在python3.9安装\n  重建新环境，安装python3.8，进而安装1.5.1\n  成功\n  具体实施 使用sigprofilerextractor发现报错如下：\nERROR: Could not find a version that satisfies the requirement torch==1.5.1 (from sigprofilerextractor) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.1, 1.8.0, 1.8.1) ERROR: No matching distribution found for torch==1.5.1 需要安装torch1.5.1版本，直接conda安装和pip安装都失败了 另外提示中表明可以通过source使用whl安装，网址如下：\nCannot install torch just with pip, try again with source from https://download.pytorch.org/whl/torch_stable.html Looking in links: https://download.pytorch.org/whl/torch_stable.html 选择匹配版本，发现报错，其中网址中36指的是python3.6版本\nERROR: Could not install requirement torch==1.5.1+cpu from https://download.pytorch.org/whl/cpu/torch-1.5.1%2Bcpu-cp39-cp39-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: https://download.pytorch.org/whl/cpu/torch-1.5.1%2Bcpu-cp39-cp39-linux_x86_64.whl for URL https://download.pytorch.org/whl/cpu/torch-1.5.1%2Bcpu-cp39-cp39-linux_x86_64.whl torch1.5.1没有支持3.9的版本，但是我的python是3.9的\n(R4) [taozy@hpc-login-gpu01 ~]$ python Python 3.9.1 | packaged by conda-forge | (default, Dec 21 2020, 22:08:58) [GCC 9.3.0] on linux Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. 创建一个新环境去跑sigprofilerextractor：sigminer环境，这个python是3.8的 先在这个环境里安装sigminer，使用conda安的时候老是出现以下报错\nSolving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. 有可能是镜像的问题，换源 还没换源莫名其妙好了\nCollecting package metadata (repodata.json): done Solving environment: done proceed yes之后继续下载 载入sigminer时有问题\n\u0026gt; library(sigminer) Error: package or namespace load failed for ‘sigminer’ in dyn.load(file, DLLpath = DLLpath, ...):  unable to load shared object \u0026#39;/slst/home/taozy/miniconda3/envs/sigminer/lib/R/library/data.table/libs/datatable.so\u0026#39;:  /slst/home/taozy/miniconda3/envs/sigminer/lib/R/library/data.table/libs/datatable.so: symbol GOMP_loop_nonmonotonic_dynamic_next, version GOMP_4.5 not defined in file libgomp.so.1 with link time reference In addition: Warning message: package ‘sigminer’ was built under R version 4.0.5 \u0026gt; install.packages(\u0026#34;data.table\u0026#34;) 是库的问题，重新安装data.table，解决 在python3.8下安装发现问题解决，torch1.5.1成功安装 运行时发现自动调用python3.9，通过py_path强制设定3.8版本\nsigprofiler_extract(simulate.tally_X,  output = \u0026#34;PCAWG_1000\u0026#34;,  range = 2:30,  nrun = 100,  init_method = \u0026#34;random\u0026#34;,  is_exome = FALSE,  use_conda = FALSE,  py_path = \u0026#34;~/miniconda3/envs/sigminer/bin/python\u0026#34; ) 运行报错python3.8没有sigprofilerExtractor\npython: /slst/home/taozy/miniconda3/envs/sigminer/bin/python libpython: /public/slst/home/taozy/miniconda3/envs/sigminer/lib/libpython3.8.so pythonhome: /slst/home/taozy/miniconda3/envs/sigminer:/slst/home/taozy/miniconda3/envs/sigminer version: 3.8.2 | packaged by conda-forge | (default, Mar 5 2020, 17:11:00) [GCC 7.3.0] numpy: /slst/home/taozy/miniconda3/envs/sigminer/lib/python3.8/site-packages/numpy numpy_version: 1.20.3  NOTE: Python version was forced by use_python function Python module SigProfilerExtractor not found, try installing it... WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;NewConnectionError(\u0026#39;\u0026lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b8645858070\u0026gt;: Failed to establish a new connection: [Errno -2] Name or service not known\u0026#39;)\u0026#39;: /simple/sigprofilerextractor/ WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;NewConnectionError(\u0026#39;\u0026lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b8645858c40\u0026gt;: Failed to establish a new connection: [Errno -2] Name or service not known\u0026#39;)\u0026#39;: /simple/sigprofilerextractor/ WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;NewConnectionError(\u0026#39;\u0026lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b86458584f0\u0026gt;: Failed to establish a new connection: [Errno -2] Name or service not known\u0026#39;)\u0026#39;: /simple/sigprofilerextractor/ WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;NewConnectionError(\u0026#39;\u0026lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b8645858ac0\u0026gt;: Failed to establish a new connection: [Errno -2] Name or service not known\u0026#39;)\u0026#39;: /simple/sigprofilerextractor/ WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;NewConnectionError(\u0026#39;\u0026lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b864586a550\u0026gt;: Failed to establish a new connection: [Errno -2] Name or service not known\u0026#39;)\u0026#39;: /simple/sigprofilerextractor/ ERROR: Could not find a version that satisfies the requirement SigProfilerExtractor==1.1.0 (from versions: none) ERROR: No matching distribution found for SigProfilerExtractor==1.1.0 Error: Error installing package(s): \u0026#39;SigProfilerExtractor==1.1.0\u0026#39; Execution halted 指定该版本的工具进行安装\npip install SigProfilerExtractor==1.1.0 安装成功\nInstalling collected packages: joblib, threadpoolctl, scipy, scikit-learn, pillow, pyparsing, six, cycler, kiwisolver, python-dateutil, matplotlib, nimfa, pytz, pandas, seaborn, sigProfilerPlotting, patsy, statsmodels, SigProfilerMatrixGenerator, reportlab, psutil, PyPDF2, xlrd, SigProfilerExtractor Successfully installed PyPDF2-1.26.0 SigProfilerExtractor-1.1.0 SigProfilerMatrixGenerator-1.1.30 cycler-0.10.0 joblib-1.0.1 kiwisolver-1.3.1 matplotlib-3.4.2 nimfa-1.4.0 pandas-1.2.4 patsy-0.5.1 pillow-8.2.0 psutil-5.8.0 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2021.1 reportlab-3.5.67 scikit-learn-0.24.2 scipy-1.6.3 seaborn-0.11.1 sigProfilerPlotting-1.1.15 six-1.16.0 statsmodels-0.12.2 threadpoolctl-2.1.0 xlrd-1.2.0 终于跑上了\n/opt/gridview//pbs/dispatcher/mom_priv/jobs/2439514.node1.SC: line 16: out_dir: command not found sigminer version 2.0.1 - Star me at https://github.com/ShixiangWang/sigminer - Run hello() to see usage and citation. Warning message: package ‘sigminer’ was built under R version 4.0.5 Loading required namespace: reticulate Python environment configuration. ==================== python: /slst/home/taozy/miniconda3/envs/sigminer/bin/python libpython: /public/slst/home/taozy/miniconda3/envs/sigminer/lib/libpython3.8.so pythonhome: /slst/home/taozy/miniconda3/envs/sigminer:/slst/home/taozy/miniconda3/envs/sigminer version: 3.8.2 | packaged by conda-forge | (default, Mar 5 2020, 17:11:00) [GCC 7.3.0] numpy: /slst/home/taozy/miniconda3/envs/sigminer/lib/python3.8/site-packages/numpy numpy_version: 1.20.3  NOTE: Python version was forced by use_python function  ************** Reported Current Memory Use: 0.41 GB *****************  Extracting signature 2 for mutation type 176 The matrix normalizig cutoff is 17600 参考：  how can i install torch  ","date":"June 4, 2021","image":null,"permalink":"/post/2021-6-4_torch1.5.1_error/","title":"ERROR: Could not find a version that satisfies the requirement torch==1.5.1"},{"categories":["技术"],"contents":"引言 小雨毕业填各种表格，发现数字很长的时候会直接用0替代超长位数的数字，检索之后，给她解决了这个小问题。\n操作方法 设置数字格式，在自定义中输入@\n 重新输入可以发现已经可以了\n  参考资料：  Excel数字长度超13位尾号变0或E\u0026#43;，教你这招只用一个0就能搞定  ","date":"May 27, 2021","image":null,"permalink":"/post/2021-5-27_excel_number/","title":"Excel数字长度超13位尾号变0"},{"categories":["计算机基础"],"contents":"引言 看了一下廖雪峰的官方网站中对多线程和多进程的讲解，写的真是又简洁又明了，结合今天workshop中zk提到的并行计算，简单汇总写个学习笔记。\n操作系统可以同时执行多任务，比如同时运行浏览器、QQ和word，CPU执行代码是按照顺序一条条的执行。\n操作系统执行多任务是让CPU对多个任务轮流进行交替执行，比如让浏览器执行0.1秒，让word执行0.1秒。\n不管是单核还是多核的CPU，都可以同时运行多个任务，单核CPU执行任务交替进行，多核CPU在任务数量多于CPU的核数时，也是交替执行任务。\n一、进程 计算机中一个任务为一个进程，浏览器是一个进程，word也是一个进程。部分进程内部需要同时执行多个子任务，比如使用word一边打字一边检查拼写，一边后台打印，子任务就是线程。\n操作系统调度的最小任务单位是线程。由于一个应用程序可以有，多个进程，也可以有多个线程，实现任务的方法包括：\n多进程模式（每个进程只有一个线程）：\n  多线程模式（一个进程有多个线程）：\n  多进程＋多线程模式（复杂度最高）：\n  二、线程 线程包含在进程内，多任务既可以多进程来实现，也可以单进程内的多线程实现，也可以混合多进程和多线程。\n和多线程相比，多进程的缺点在于：\n 创建进程比创建线程开销大，尤其是在Windows系统上； 进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。  多进程的优点在于：\n多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。\n多线程编程的特点在于：\n 经常需要读写共享数据，并且需要同步，比如播放电影时一个线程播视频，一个线程播音频，两个线程需要协调运行保持音画同步，因此多线程编程的复杂度高，调试更困难。  三、串行，并发与并行   串行 多个任务，执行时一个执行完再执行另一个。\n  并发 多个线程在单个核心运行，同一时间一个线程运行，系统不停切换线程，看起来像同时运行，实际上是线程不停切换。\n  并行 每个线程分配给独立的核心，线程同时运行。\n  四、CPU与核心  物理核  物理核数量=cpu数(机子上装的cpu的数量)*每个cpu的核心数\n 虚拟核  所谓的4核8线程，4核指的是物理核心。通过超线程技术，用一个物理核模拟两个虚拟核，每个核两个线程，总数为8线程。在操作系统看来是8个核，但是实际上是4个物理核。 通过超线程技术可以实现单个物理核实现线程级别的并行计算，但是比不上性能两个物理核。\n 单核cpu和多核cpu  都是一个cpu，不同的是每个cpu上的核心数，多核cpu是多个单核cpu的替代方案，多核cpu减小了体积，同时也减少了功耗，一个核心只能同时执行一个线程。\n参考资料：   廖雪峰的官方网站-多线程\n  认识cpu、核与线程\n  ","date":"May 24, 2021","image":null,"permalink":"/post/2021-5-24_threads_and_processes/","title":"线程和进程"},{"categories":["生物统计"],"contents":"介绍  p值的含义    假设存在药物A和药物B，想知道两种药物的区别？\n维基百科定义：p值是假设检验中假设零假设为真时观测到至少与实际观测样本相同极端的样本的概率（似乎很拗口）。\np值是介于0-1之间的数字，量化我们相信两种药物不同的信心，p值越接近0，越相信两者不同。当p的阈值为0.05意味着，假设两种药物之间没有差异，执行多次且相同的实验，那么只有5%的实验会得出错误决定，简单来说，p值是对意外的测量。\np值能够帮助确定两种药物是否不同，但是不能告诉我们有什么不同，不管差异是都大还是小，都可以使用较小的p值，即较小的p值不代表差异是大还是小，只是代表意外的结果概率更小 。\n 阈值0.05的由来  不出于逻辑或统计原因，只是科学惯例。\n 术语：假阳性  指的是没有差异时却获得小的p值的情况。\n 术语：假设检验（Hypothesis testing）  试图确定这些药物是否相同的想法。\n 术语：零假设（Null Hypothesis）  零假设是药物相同，p值帮助我们决定是否拒绝零假设。\n统计显著性检验分支 分为以下两个主要的：\n  R.A.Fisher\n  Neyman and Pearson\n  控制假设检验的两类错误很重要：\n 第一类错误  无效说成有效（取伪）。\n第二类错误。  有效判成无效（弃真）。\n这两种错误不能同时消除，但是可以给出一种规范的决策过程来确保第一类错误的可能性只在预先确定的比率下发生（奈曼和皮尔逊），这个比率为显著性水平α（false positive rate），可以根据经验和期望基础设置合适的α，举例：\n建立10%的第一类错误率，设置α = 0.1，当希望决策更加保守，可以将α设置成0.01或更小。确定α后，可以考察哪个 检验过程 的第二类错误的比率更低。\n该体系下，定义一个原假设，即“无效”的假设，再定义一个备择假设，“效应大于0”，构建一个检验去比较这两个假设，假设使用p值，如果p\u0026lt;α，拒绝原假设（费希尔的检验过程把注意力放在揭示任何一个特定的试验证据的强度），p值的大小只用来是否“拒绝原假设”。\n误区 误区1：一次试验的第一类错误率为3.2%\n注意，仅仅通过一次试验不能得到第一类错误率，这是由检验过程决定的，不是一次试验的结果得到的，一个检验过程得到的是一个长期的第一类错误率，不能对应到每一次试验得到的真实p值和对应的第一类错误率。\n误区2：p值越小，差异越大\np值仅仅反应我们相信我们存在差异的信心，p值越小，越有信心拒绝零假设，不管差异是大还是小。\n误区3：P值就是假阳性率\n拒绝原假设犯错属于一类错误，错误的概率就是我们的α，p值只是我们根据一次抽样结果计算出来的值。P \u0026lt; α）表达的是在一次抽样中出现当前结果及更极端结果的可能性比我们认为的在一次抽样中不可能发生的小概率事件的概率更小。\n置信区间 一个置信区间包含一个点估计，和该估计的不确定性。举例：\n如果想检验这个效应量是否显著区别于0，可以构建一个95%的置信区间来检验这个区间是否包含0。\n参考：   《StatQuest》\n  《统计会犯错：如何避免数据分析中的统计陷阱》\n  统计知识|谈谈P值和α水平\n  ","date":"April 10, 2021","image":null,"permalink":"/post/2021-4-07_p%E5%80%BC/","title":"p值"},{"categories":["生信"],"contents":"前言 最近需要对TCGA和PCAWG的表达数据进行免疫浸润水平分析，使用了R包immunedeconv,其中TCGA已经有文献的supplement给出了不同免疫浸润工具进行分析的结果，PCAWG需要自己手动分析，其中CIBERSORT在immunedeconv包中运行需要两个文件：LM22.txt，CIBERSORT.R，需要在官网：https://cibersortx.stanford.edu/ 进行申请。\n上传数据  mixture file  官网上会列出上传数据的要求，我上传了PCAWG的基因表达counts矩阵，数据的第一列是genesymbol的名称，而且第一列的列名需要加上：“GeneSymbol”，数据的第一行是样本的名称。\n注意上传数据的时候，需要勾选自己上传的数据属于哪种类型，否则在分析数据中无法选择到自己上传的数据。\n进行分析 选择cell fraction的custom，得到免疫浸润细胞比例结果，选择custom分析自己的数据。\n勾选LM22作为参考matrix，选择自己上传的matrix作为mixture matrix，需要比较样本间时记住勾选abs。\n运行过程中，可以看到报错信息和输出信息。\n","date":"April 6, 2021","image":null,"permalink":"/post/2021-4-06-cibersort/","title":"使用CIBERSORTx网页版分析免疫浸润"},{"categories":["生物统计"],"contents":"引言  线性回归  假设数据包含 尺寸 和 重量 两组，根据这两组数据用 最小二乘法 拟合一条线后，我们可以做如下的事情：\n  计算r平方来确定两个变量是否相关\n  计算p值确定R平方是否具有统计显著性\n  用于预测，如果一个新鼠标有某重量，可以根据这个线来预测其大小\n  多元回归  假设用 体重 和 血容量 来预测大小，拟合曲线可做上述三个同样的事情，还可以用离散型数值来预测大小。\n比较模型  进行正态回归，使用权重来预测大小。\n逻辑回归 S型函数定义：\n  S 型函数会产生以下曲线图：\n y\u0026rsquo; 是逻辑回归模型针对特定样本的输出。 z 是 b + w1x1 + w2x2 + … wNxN w 的值是该模型学习的权重，b 是偏差。 x 的值是特定样本的特征值。  线性回归的损失函数是平方损失。逻辑回归的损失函数是对数损失函数。\n  逻辑回归预测事物是对还是错，而不是连续的事物，通常用于分类，根据该目的拟合了“S”形曲线，可以给出概率。\n假设这里用体重来预测肥胖，或者基因型和体重来预测肥胖，即不仅可以处理体重和年龄等连续数值，还可以处理离散数值，可以测试每个变量是否对预测肥胖有用。\n 最大似然法拟合曲线  本质就是不停的计算，选择具有最大似然的曲线。\n最大似然 最大似然的目标是找到使分布适合数据的最佳方法。不同类型的数据存在不同类型的分布，让分布适合数据可以让数据的使用更轻松，更通用，使它适用于相同类型的每个实验，假设称重了一批老鼠如下：\n在这种情况下，认为老鼠体重可能呈正态分布，正态分布意味着：\n  老鼠的重量 接近均值 或平均值。\n  期望测量值围绕平均值相对对称。（虽然看起来不是完全对称，但是也没有非常明显的偏向）\n  正态分布存在多种大小和形状，如下：\n当确定形状之后，要找出居中的位置。\n假设选任何一个旧的正态分布，下图分布表示测量的大多数值应该接近平均值，看看它对数据的拟合程度（黑色虚线处是分布平均值）：\n发现测量的大多数值和分布的均值相差很大（右边多数的测量值的似然很低），如果将正态分布移到其均值和平均体重相同的位置：\n发现红色圈出部分的似然相对高，继续移动正态分布，这些测量值的似然再次下降：\n可以在分布中心的位置，绘制观察到数据的似然，从左侧开始，计算观察数据的似然，将分布向右移动并重新计算似然：\n当尝试了所有可能的位置，就可以将正态分布置于需要的位置，即最大化观察我们所测量值的似然的位置，以上都是平均值的最大似然估计，这里讨论的是分布的均值，不是数据的均值吗，接下来找出标准差的最大似然估计：\n日常对话中，概率和似然（likelihood）可能指的是同一个概念，但是在统计学中，似然（likelihood）指的是：尝试针对给定的一组观察到的测量值，找到分布的最佳均值或标准差，这就是找到适合数据分布的过程。\n参考：   《StatQuest》\n  《统计会犯错：如何避免数据分析中的统计陷阱》\n  逻辑回归(Logistic Regression)：计算概率|机器学习速成课程\n  ","date":"April 6, 2021","image":null,"permalink":"/post/2021-4-11_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","title":"逻辑回归"},{"categories":["技术"],"contents":"前言 最近写文章的时候，发现修改时调整参考文献的引用，是一件非常麻烦的事情，于是找到了一种简便的方法进行调整，发现了用Latex写论文原来这么好用（据涛哥和翔哥说，word也可以自动调整引用文献顺序，而我一直不知道\u0026hellip;）。\n主要内容  LaTex基本语法 模板的使用 文献的引用 排版的调整 特殊符号的引用 图片表格的插入  使用的工具  Oneleaf —— Online LaTeX Editor（推荐）  优点：不用本地搭建环境；在线编辑并渲染；部分投稿期刊直接提供链接模板。\n MacTeX—— For mac  mac的我还没有安过，仅列出来供参考。\n TexWorks —— For windows（我觉得很好用）。  除了上面列出来的，还有很多其他的，根据自己的喜好使用吧。\n基本语法  整体框架  注意到一个部分有开始也有结束，中间就是这个部分的内容。\n\\documentclass{article}  \\begin{document} First document. This is a simple example, with no extra parameters or packages included. \\end{document}  preamble  在LaTex中，所有在document内容之前的都称为preamble,在preamble中可以定义整个文档的格式、使用的语言、你需要使用到的宏包、以及其他的元素。举例：\n\\documentclass[12pt, letterpaper]{article} %设置 \\usepackage[utf8]{inputenc} %加载了名叫inputenc的宏包，设置使用utf-8来编码。 在[]方括号中都是一些参数的选择，documentclass中设置了字体大小为12pt（默认的字体大小是10pt），纸张大小为信纸，其他的设置可以看\u0026lt;font color=blue\u0026gt;Oneleaf的文档说明\u0026lt;/font\u0026gt;。\n这些内容也属于preamble：\n标题\n\\title{First document} 作者\n\\author{cat} 日期\n\\date{March 2021} 谢言\n\\thanks{funded by the Overleaf team}  document内容  字体的简单格式可以通过以下代码实现：\nSome of the \\textbf{greatest} %字体加粗 discoveries in \\underline{science} %加入下划线 were made by \\textbf{\\textit{accident}}. %斜体   还有更多的字体变换可以更改，具体参考\u0026lt;font color=blue\u0026gt;字体格式\u0026lt;/font\u0026gt;，来个简单的应用示例感受一下它的实用性。\nOneleaf 示例 1. 模板的使用 进入 \u0026lt;font color=blue\u0026gt;Oneleaf模板\u0026lt;/font\u0026gt;，选择合适的模板 这其中包括\u0026lt;font color=blue\u0026gt;国科大的毕业论文模板\u0026lt;/font\u0026gt;、\u0026lt;font color=blue\u0026gt;开题报告模板\u0026lt;/font\u0026gt;，以及各种杂志期刊、简历、信件、海报、报告、作业和写书的模板等等（部分期刊杂志会提供Oneleaf的模板）。\n 以论文写作为例 随便打开一个\u0026lt;font color=blue\u0026gt;模板\u0026lt;/font\u0026gt;，最右边是实时的渲染结果，中间是可以编辑的部分，左边是模板的目录下文件。\n  左边的目录下存储了4个文件：\n 以.bib结尾的文件  通常用来存储引用文献的信息，其中引用文献需要用特定的格式——BibTex存储，可以通过谷歌学术、百度学术或者其它方式进行导出（谷歌学术的导出功能经常会崩溃），如何使用Bibtex格式进行文献引用请跳转 \u0026lt;font color=blue\u0026gt;点击跳转\u0026lt;/font\u0026gt;。\n 以.bst结尾的文件  这个文件通常由期刊或杂志提供，设置了参考文献出现的文章结尾的方式，比如：设置排序方式，设置作者名称是缩写还是全称，标题的大小写等等，一般不需要自行维护，而且可以根据自己的需求来在.tex中重新调整。\n 以.tex结尾的文件  这个是进行文档内容编辑的文件，可以在这个文件中加载宏包，进行内容以及格式的更改。\n 以.cls结尾的文件  这个文件通常是类文件，通过文档最前面的\\documentclass导入，这里的\\documentclass[options]{class}是用来指定文档类型的，可以通过options参数来定制文档类的属性，不同的选项之间需要用逗号隔开，比如这里的\\documentclass[final,3p]{CSP}，其中final指的不在页面的边缘标记一个黑色框，这个3p对它的解释是：\n formats the article to the look and feel of the final format of model 3+ journals.\n 我没明白这个3+model是什么意思，但是通过调试，发现这个数字越大，页面距就越大。\n另外别的模板中还存在这些文件：\n .bbl文件  这是编译之后形成的文件，这里直接就显示了编译后的形式，可以直接下载PDF文件。\n .sty文件  这是包文件，通常使用\\usepackage导入。\n2. 内容的编辑以及参考文献的引用 内容的编辑 载入宏包的方法是在文档开始前\\begin{document}，写入\\usepackage{package}\n这里介绍几个常用的宏包：\n  数学公式 - amsmath\n  插图 - graphicx\n  颜色 - xcolor\n  表格 - array\n  中文 - ctex, xecjk\n  西文 otf 字体 - fontspec\n  英文下划线 - geometry\n  特别的，当要使用英文下划线-时，比如写入sigminer包中的函数名read_maf时，并不能直接识别下划线，需要载入该包来处理，或者给每处下划线改成\\_也可以不用载入包来识别。\n 超链接 - hyperref  语法：\\usepackage[options]{hyperref}\n示例：加入超链接，同时将文献也超链接到reference中，并且设置超链接的颜色，常用的设为蓝色或者黑色。\n\\usepackage[backref, colorlinks,linkcolor=blue]{hyperref}   \n文献的引用  保存引用文献信息(google)  如下图所示，谷歌学术中可以直接得到BibTex这种引用格式，将文献信息存储在.txt中，然后更改后缀为.bib即可。\n  点击 BibTex 后会弹出下图所示页面，复制内容至.bib文件内即可。\n   在tex文件中对引用格式进行设置  在文档结束位置\\end{document}前，增加对参考文献格式以及引用的设置，比如：\n\\bibliographystyle{bibft}\\it \\bibliography{bibfile} bibft是模板自带并自己命名的格式文件，这是由.cls文件定义的，bibfile就是制作好的bibtex文件，\n注意这里的文献引用格式有很多种，除了模板中定义的格式，可以通过参数的调整将格式更改为自己想要的，比如常用的，在方括号中标注数字，并且根据文献引用的先后顺序对reference排序：\n\\begin{document} \\bibliographystyle{unsrt}  \\bibliographystyle{unsrt} %根据引用顺序自动排序 \\bibliography{bibfile.bib} %引用文献的文件 \\end{document} 我认为最方便的地方就是这里，能够根据文献引用的顺序对reference进行自动排序。\n 保存引用文献信息(zotero)  在zotero中设置导出为BibTex，在zotero中通过command+shift+c就可以直接复制出该文献的BibTex引用格式。\n   在tex文件中进行引用  通常在正文中有很常见的几种文献引用格式： 温哥华格式（上标形式） 哈佛格式（直接显示作者和发表年份） IEEE 格式（方括号内标注引用顺序）\n在文中一般使用\\cite{}进行引用，括号中的内容就是BibTex中的第一个参数，这个是可以自定义的，通常都是作者的姓或者名+发表年份+论文题目的首个单词。\n@inproceedings{song2013hierarchical,  title={Hierarchical representation using NMF},  author={Song, Hyun Ah and Lee, Soo-Young},  booktitle={International conference on neural information processing},  pages={466--473},  year={2013},  organization={Springer} } 另外还有宏包natbib，通过不同形式的cite比如：\n\\citet：\n\\citet{jon90} ## Jones et al. (1990) \\citep：\n\\citep{jon90}\t## (Jones et al., 1990) \\citeyear：\n\\citeyearpar{jon90} ## (1990) \u0026hellip;\u0026hellip;\n该宏包的使用方式如下。\n\\usepackage[option]{natbib}  \\bibliographystyle{natbib} \\bibliography{bibfile} 3. 表格的制作 分为以下两个部分：\n 手动输入表格（适合小型表格） 其他工具进行表格转换   Excel中的表格  可以在Excel中使用插件：Excel2Latex，该插件能够将Excel表格转化为LaTex的表格形式。\nWord中的表格  可以使用pandoc直接转换为.tex格式，不过转换之后不是完美的，可能需要手动调整一下。\n示例：\npandoc test.docx -o test.tex 其他文件形式的表格  比如在R当中得到的表格，可以使用stargazer包把结果输出为LaTex格式，或者xtable包。\n以xtable包为例：\n\u0026gt; install.packages(\u0026#34;xtable\u0026#34;) #安装xtable \u0026gt; library(xtable) #载入 \u0026gt; data(iris) # 示例数据 \u0026gt; xtable(head(iris),digits=3,caption=\u0026#34;Head of Iris Data\u0026#34;) #将iris数据前6行 # 保留三位小数 #标题设为\u0026#34;Head of Iris Data\u0026#34;，导出为LaTex格式 % latex table generated in R 4.0.2 by xtable 1.8-4 package % Sat Mar 6 20:04:47 2021 \\begin{table}[ht] \\centering \\begin{tabular}{rrrrrl}  \\hline  \u0026amp; Sepal.Length \u0026amp; Sepal.Width \u0026amp; Petal.Length \u0026amp; Petal.Width \u0026amp; Species \\\\  \\hline 1 \u0026amp; 5.100 \u0026amp; 3.500 \u0026amp; 1.400 \u0026amp; 0.200 \u0026amp; setosa \\\\  2 \u0026amp; 4.900 \u0026amp; 3.000 \u0026amp; 1.400 \u0026amp; 0.200 \u0026amp; setosa \\\\  3 \u0026amp; 4.700 \u0026amp; 3.200 \u0026amp; 1.300 \u0026amp; 0.200 \u0026amp; setosa \\\\  4 \u0026amp; 4.600 \u0026amp; 3.100 \u0026amp; 1.500 \u0026amp; 0.200 \u0026amp; setosa \\\\  5 \u0026amp; 5.000 \u0026amp; 3.600 \u0026amp; 1.400 \u0026amp; 0.200 \u0026amp; setosa \\\\  6 \u0026amp; 5.400 \u0026amp; 3.900 \u0026amp; 1.700 \u0026amp; 0.400 \u0026amp; setosa \\\\  \\hline \\end{tabular} \\caption{Head of Iris Data} \\end{table} 表格的介绍 表格的基本格式和要素如下（2行2列表格）：\n\\documentclass{article} \\usepackage{float}%提供float浮动环境 \\usepackage{makecell} %%用来基线 \\begin{table}[h] \\centering %%表居中 \\caption{table} %%表格标题  \\begin{tabular}{|c|c|} %%{cc} 表示各列元素对齐方式，left-l,right-r,center-c，两个c表示两列，｜表示增加垂直方向基线 \\hline %%\\hline 在此行下面画一横线 a \u0026amp; b \\\\\\hline c \u0026amp; d\\\\ \\hline \\end{tabular} \\end{table}  \\end{document}   当表格太大或者太小的时候，有非常多的解决办法，可以通过调整字体的长或宽，也可以直接整体调整表格的大小，本质都是通过在tabular类外，套上调整表格的参数设置。\n通过调整字体的宽度（mm是百分比，60mm就是60%）：\n\\resizebox{\\textwidth}{60mm}{} 通过调整表格的大小：\n\\usepackage{graphicx} \\begin{table}  \\caption{表格标题} \\scalebox{0.9}{ %缩小至原来的90% \\begin{tabular} …… \\end{tabular}} \\end{table} 文献中常用的三线表可以通过以下Latex实现：\n\\documentclass{article} \\usepackage{float}%提供float浮动环境 \\usepackage{booktabs} %%提供命令\\toprule、\\midrule、\\bottomrule \\usepackage{makecell} %%用来基线 \\usepackage{geometry} \\usepackage{amsmath} %\\geometry{papersize={40cm,80cm}} \\geometry{left=1cm,right=1cm,top=3cm,bottom=1cm} \\begin{document}  %经典三线表 \\begin{table}[H] %%H为当前位置 \\caption{\\textbf{test title}}%标题 \\centering%把表居中 \\begin{tabular}{ccc}%四个c代表该表一共四列，内容全部居中 \\toprule[1.5pt]%第一道横线 year \u0026amp; month \u0026amp; day \\\\ \\midrule%第二道横线 2021 \u0026amp; 3 \u0026amp; 5 \\\\  \\bottomrule[1.5pt]%第三道横线 \\end{tabular} \\end{table}  \\end{document}   4. 图片的引入 LaTeX插入图片时，支持格式有各种：png, pdf, jpg, eps等等。\n 准备图片  将图片全部保存在目录下的同一个文件夹下，方便查找，注意图片的命名尽量避免中文，特殊字符等等(这里就只用了一个文件，我就直接放在目录下了)。\n 图片基本语法  必须加载graphicx等包来支持图片的导入。\n\\documentclass{article} \\usepackage{graphicx} \\graphicspath{ {images/} }  \\begin{document} The universe is immense and it seems to be homogeneous, in a large scale, everywhere we look at.  \\includegraphics{universe}  There\u0026#39;s a picture of a galaxy above \\end{document}  给图片进行排版（排版有很多种，现在展示的是两个图片并排）    \\usepackage{graphicx} %%插入图片的宏包 \\usepackage{float} %%设置图片浮动位置的宏包 \\usepackage{subfigure} %%插入多图时用子图显示的宏包 \\begin{figure} %%旋转子系统姿态角  \\centering  \\subfigure{  \\label{fig:subfig:a} %% label for first subfigure  \\includegraphics[width=4cm,height=4cm]{taoziyu.jpg}}  \\hspace{1in}  \\subfigure{  \\label{fig:subfig:b} % label for second subfigure  \\includegraphics[width=4cm,height=4cm]{taoziyu.jpg}}  \\caption{The same cute cat (a) cute cat1. (b) cute cat2.}  %% caption用于图表的标题  \\label{fig:attitude} %% label for entire figure \\end{figure} 参考资料  \u0026lt;font color=blue\u0026gt;Oneleaf官方文档\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;BibTex的使用方法\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;latex documentclass 及相关布局\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;document class\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;LaTex - 从出门到掉到坑\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;LaTex插入图形，表格\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;LaTeX排版札记\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;用R语言快速生成Latex表格\u0026lt;/font\u0026gt; \u0026lt;font color=blue\u0026gt;LaTeX高效写作系列：word表格转LaTeX\u0026lt;/font\u0026gt;  题外之言 该markdown文档中新使用了以下几个有趣的功能：\n 自定义锚实现页内跳转  在跳转目的地附近加上\u0026lt;span id=\u0026quot;jump\u0026quot;\u0026gt;目的地\u0026lt;/span\u0026gt;，在需要点击跳转的地方加上[点击跳转](#jump)\n  更改超链接颜色，更改字体颜色\n  多图片并列\n ","date":"March 5, 2021","image":null,"permalink":"/post/2021-3-05-latex_blog/","title":"使用Latex写论文"},{"categories":["技术"],"contents":"前言 最近师兄和涛哥都分享了这个流程：如何使用了R包blogdown搭建个人博客，但是我和轩哥在使用的过程中发现一个小问题，有一些主题似乎并不能很好的被blogdown安装的hugo来应用，几番求解无果，暂时将这个问题搁置，因为太喜欢这个theme不想放弃，所以找了一种不用blogdown的方法去搭建，终于取得成果。\n搭建流程  下载Hugo  Mac直接使用brew安装即可\n其他的系统可以直接参考官方文档：hugo官方文档\nbrew install hugo 部分主题依赖extended version（比如我使用的这个主题），所以检查一下Hugo版本\nhugo version Hugo Static Site Generator v0.80.0/extended darwin/amd64 BuildDate: unknown  使用Hugo建博客  #新的博客的名字就叫blog hugo new site blog ls archetypes\tcontent\tlayouts\tresources\tthemes config.toml\tdata\tstatic  设置博客主题  从Hugo的官网找到自己喜欢的主题：Hugo themes\n我使用了这个主题：hugo-future-imperfect-slim\n下面开始设置博客主题\ncd blog/themes #blog就是刚才新创建的博客目录，themes是它的子目录  git clone https://github.com/pacollins/hugo-future-imperfect-slim.git #点击主题页的download，进入GitHub主页，找到code⬇当中的网址进行clone #注意主题要下在theme目录下  cp themes/hugo-future-imperfect-slim/exampleSite/config.toml . #使用模板自带的配置文件替换blog目录下的配置文件 有的模板没有exampleSite目录，或者是config.toml文件名为其他的名字，不管怎样，用模板目录下的config文件替换blog目录下的config文件即可。\n像轩哥用的主题 hyde，根据其在github上的安装提示，对blog目录下的config文件首行增加theme = \u0026quot;hyde\u0026quot; 即可，无需拷贝模板的config文件。\n以及主题 npq-hugo，它的模板config文件不在exampleSite目录下，而是themes/npq-hugo/example-config.toml。\n 回到blog目录下创建新博文  cd .. hugo new posts/test.md #会在content/post文件夹下创建test.md的文件，可以直接对其进行修改 开始可能不会修改，可以根据主题目录中的范例进行模仿，我选择主题的范例文件储存在：\nblog/themes/hugo-future-imperfect-slim/exampleSite/content/blog\n 进行预览  hugo server -D Start building sites …   | EN | FR | PL | PT | DE | ES | ZH-CN | ZH-TW | JA | NL -------------------+----+----+----+----+----+----+-------+-------+----+-----  Pages | 11 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8  Paginator pages | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Non-page files | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Static files | 27 | 27 | 27 | 27 | 27 | 27 | 27 | 27 | 27 | 27  Processed images | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Aliases | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  Sitemaps | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  Cleaned | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Built in 108 ms Watching for changes in /Volumes/home /github/taozy_blog/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Volumes/home /github/taozy_blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 在浏览器中打开terminal中显示的网址进行预览： http://localhost:1313/\n你可以使用可以编辑md的工具进行内容修改，我习惯用Rstudio打开它进行更改，而且很棒的是，在编辑的过程中，预览网址上的内容也会实时的变更，很方便进行修改。\n 配置Github repository  我配置了两个repos，一个用于托管我blog目录下所有的文件，一个用来托管public文件夹用于博客的显示。\n 第一个repos随意命名  上传blog目录下的文件\n第二个repos必须命名为：your_git_name.github.io，才能够被当作个人的主页  上传public目录下的文件（注意不是public这个文件夹，而是该文件夹下的所有文件），目前还没有生成public文件，不要着急，继续下面的操作。\n在blog目录下执行下面的命令，把theme改成你自己的theme名称，baseUrl换成你自己的github名，执行完会在blog目录下生成一个public目录，将这个目录下的内容上传到your_git_name.github.io仓库中\nhugo --theme=hugo-future-imperfect-slim --baseUrl=\u0026#34;https://taoziyu97.github.io\u0026#34; Start building sites …   | EN | FR | PL | PT | DE | ES | ZH-CN | ZH-TW | JA | NL -------------------+----+----+----+----+----+----+-------+-------+----+-----  Pages | 11 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8  Paginator pages | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Non-page files | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Static files | 21 | 21 | 21 | 21 | 21 | 21 | 21 | 21 | 21 | 21  Processed images | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Aliases | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  Sitemaps | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  Cleaned | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  Total in 209 ms  总结一下hugo-future-imperfect-slim这个主题的缺点\n 对于内容没有很好的汇总界面 只能一页一页的翻页，不能指定去哪一页 搜索内容的时候不支持中文    设置仓库的GitHub Pages  在仓库主页找到Setting，在Options中对Github Pages进行设置，如果你没有购买域名，那么你就可以在这步直接使用github提供的网址，作为你的博客网页，如果你有自己的域名（比如我从阿里云买了.cn的域名，我就将Custom domain设置为自己的域名）\n 解析域名方法1  打开域名解析，进行解析设置，添加两条记录，主机记录分别为@和www，记录类型为CNAME，记录值都指向github的网址，your_git_name.github.io\n现在打开个人主页试一试效果吧！\n 解析域名方法2（推荐，相对方法1速度更快一些）  Netlify 是一个提供静态资源网络托管的综合平台。参考这个链接内容部署：Netlify部署静态网页\n将Netlify随机生成的域名，在阿里云中进行解析设置。\n  Netlify中解析成功。\n  可以打开啦，访问速度比方法1快一些，另外，我的图是通过图床阅览的，方法2中拜托了github.io域名的限制，相当于把github当作一个储存的空间。\n","date":"January 21, 2021","image":null,"permalink":"/post/2021-1-11-build_blog/","title":"Hugo+Github+阿里云域名搭建个人博客（附Netlify部署方法）"},{"categories":["photography"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6  Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\n Link I\u0026amp;rsquo;m an inline-style link\nI\u0026amp;rsquo;m an inline-style link with title\nI\u0026amp;rsquo;m a reference-style link\nI\u0026amp;rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n Paragraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\n Ordered List  List item List item List item List item List item   Unordered List  List item List item List item List item List item   Notice This is a simple note.\n This is a simple tip.\n This is a simple info.\n  Tab  This is first tab  this is second tab  this is third tab     Collapse collapse 1   This is a simple collapse  collapse 2   This is a simple collapse  collapse 3   This is a simple collapse   Code and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s  Blockquote  This is a blockquote example.\n  Inline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\n Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags.   Tables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3     Image  Gallery               Youtube video   ","date":"January 1, 2021","image":null,"permalink":"/post/elements/","title":"Elements That You Can Use To Create A New Post On This Template."},{"categories":null,"contents":"H1 Heading H2 Heading H3 Heading H4 Heading H5 Heading H6 Heading  Paragraph Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf. Could someone survive inside a transporter buffer for 75 years? Fate. It protects fools, little children, and ships.\n Emphasis :  Did you come here for something in particular or just general Did you come here for something in particular Did you come here Did you come here for something in particular Did you come here for something in particular  Did you come here for something in particular URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).   Ordered list  you appeared for an instant to be in two places at once. We have a saboteur aboard. you appeared for an instant to be in two places at once.   Unordered list  Quisque sem ipsum, placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo Blandit vestibulum libero. Morbi sollicitudin viverra justo Placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo   Code and Syntax Highlighting : var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s); var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s);  Buttons Button  Quote  “Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once.”\n  Notice : This is a simple note.\n This is a simple tip.\n This is a simple info.\n This is a simple warning.\n  Tab :  Title goes here Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf.  Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.  Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo     Table :    # First Last Handle     1 Row:1 Cell:1 Row:1 Cell:2 Row:1 Cell:3   2 Row:2 Cell:1 Row:2 Cell:2 Row:2 Cell:3   3 Row:3 Cell:1 Row:3 Cell:2 Row:3 Cell:3     Collapse : collapse 1    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur    collapse 2    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur    collapse 3    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur     Image  Gallery               Youtube :   ","date":"January 1, 1","image":null,"permalink":"/elements/","title":"Elements"},{"categories":null,"contents":"上海科技大学在读博士生。\n研究方向：癌症基因组学。\n  Find your passion and then aim to be the best on the planet at what you do by having a ferocious hunger for learning.\n-Unknown    My Skills \u0026amp; Experiences:  R 机器学习 生物信息  ","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/about/","title":"Hi, I Am Ziyu Tao"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/full/","title":"Homepage Full"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/full-left/","title":"Homepage Full Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/full-right/","title":"Homepage Full Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/grid/","title":"Homepage Grid"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/grid-left/","title":"Homepage Grid Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/grid-right/","title":"Homepage Grid Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/list/","title":"Homepage List"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/list-left/","title":"Homepage List Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://blog-1310600458.cos.ap-shanghai.myqcloud.com/20220408140841.png\" alt=\"\" class=\"img-fluid\" height=\"\" width=\"650\"\u003e\n","permalink":"/homepage/list-right/","title":"Homepage List Right"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin sit amet vulputate augue. Duis auctor lacus id vehicula gravida. Nam suscipit vitae purus et laoreet. Donec nisi dolor, consequat vel pretium id, auctor in dui. Nam iaculis, neque ac ullamcorper. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin sit amet vulputate augue. Duis auctor lacus id vehicula gravida. Nam suscipit vitae purus et laoreet.\nDonec nisi dolor, consequat vel pretium id, auctor in dui. Nam iaculis, neque ac ullamcorper.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin sit amet vulputate augue.\n","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/author/john-doe/","title":"John Doe"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/404/","title":"No Search Found"},{"categories":null,"contents":"Responsibility of Contributors Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\npretium, aliquam sit. Praesent elementum magna amet, tincidunt eros, nibh in leo. Malesuada purus, lacus, at aliquam suspendisse tempus. Quis tempus amet, velit nascetur sollicitudin. At sollicitudin eget amet in. Eu velit nascetur sollicitudin erhdfvssfvrgss eget viverra nec elementum. Lacus, facilisis tristique lectus in.\nGathering of Personal Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\nProtection of Personal- Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus.\nMolestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat\nPrivacy Policy Changes  Sll the Themefisher items are designed to be with the latest , We check all comments that threaten or harm the reputation of any person or organization personal information including, but limited to, email addresses, telephone numbers Any Update come in The technology Customer will get automatic Notification.  ","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/privacy-policy/","title":"Our Privacy Policy"},{"categories":null,"contents":"Responsibility of Contributors Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\npretium, aliquam sit. Praesent elementum magna amet, tincidunt eros, nibh in leo. Malesuada purus, lacus, at aliquam suspendisse tempus. Quis tempus amet, velit nascetur sollicitudin. At sollicitudin eget amet in. Eu velit nascetur sollicitudin erhdfvssfvrgss eget viverra nec elementum. Lacus, facilisis tristique lectus in.\nGathering of Personal Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\nProtection of Personal- Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus.\nMolestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat\nPrivacy Policy Changes  Sll the Themefisher items are designed to be with the latest , We check all comments that threaten or harm the reputation of any person or organization personal information including, but limited to, email addresses, telephone numbers Any Update come in The technology Customer will get automatic Notification.  ","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/terms-conditions/","title":"Our Terms And Conditions"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/search/","title":"Search Results"},{"categories":null,"contents":"Ask Us Anything Or just Say Hi, Rather than just filling out a form, Sleeknote also offers help to the user with links directing them to find additional information or take popular actions.\n","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/contact/","title":"Talk To Me Anytime :)"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/2021/","title":"归档"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/2022/","title":"归档"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/2023/","title":"归档"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/2024/","title":"归档"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_h2_box.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hud5ac54a8ad9b9f4abf6483df88269ec8_11176_650x0_resize_q90_box.jpg'\"\u003e\n \n \n \n\n","permalink":"/archives/","title":"归档"}]